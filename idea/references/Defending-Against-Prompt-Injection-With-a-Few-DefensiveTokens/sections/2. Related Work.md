2. Related Work
Prompt injection attacks could be divided into optimization-free attacks and optimization-based attacks. Optimization-free attacks~\cite{bib.bib20 bib.bib41} use heuristic prompts to enhance the injection. Optimization-based attacks~\cite{bib.bib19 bib.bib27} are significantly stronger but they generally require white-box access to the model weights prompt template and defense details for computationally-heavy optimization. The threat of prompt injection has been realized in industry-level products e.g. Google Bard ~\cite{bib.bib32} Slack AI ~\cite{bib.bib29} and Anthropic’s ~\cite{bib.bib33} and OpenAI’s ~\cite{bib.bib31} web agents.
 Prompt injection defenses could be divided into detection-based defenses and prevention-based ones. Detection-based defenses aim to identify prompt injection attempts before their execution and reject potentially malicious queries at test time~\cite{bib.bib18 bib.bib12 bib.bib21}. We focus on prevention-based defenses that maintain functionality even when under attack. Existing prevention-based defenses secure the LLM at test time or training time. In the test time defensive prompts could be added before~\cite{bib.bib40} in the middle~\cite{bib.bib35 bib.bib46 bib.bib36} or at the end ~\cite{bib.bib42} of LLM input. The recently proposed system-level defense ~\cite{bib.bib8} uses insights from system security to build a secure LLM system by design hoping to have some guaranteed properties. In contrast to the above training-time defenses use optimization to more effectively defend against prompt injections. Jatmo ~\cite{bib.bib28} fine-tunes a base LLM on only one task without supplying any task instruction so the defended LLM has no instruction (injection) -following ability. StruQ ~\cite{bib.bib4} SecAlign ~\cite{bib.bib5 bib.bib6} and ISE ~\cite{bib.bib43} fine-tune a supervised-fine-tuned LLM in the presence of injections and ask it to behave securely. Instruction hierarchy~\cite{bib.bib38} defines a multi-layer security policy where the higher-priority instruction should always be obeyed and is implemented in frontier LLM such as gpt-4o ~\cite{bib.bib25} and gemini-2.5-flash ~\cite{bib.bib37}. DefensiveToken differs from all above using optimization for effective defense but is as flexible for developers as prompting. As detection-based defenses are designed to refuse answering (and thus lose utility) when there is an attack and the only existing system-level defense ~\cite{bib.bib8} is only applicable to agentic use cases with reported utility drop we omit those baselines and focus on prompting-based ones in comparing with test-time defense baselines.
 Parameter-efficient fine-tuning adapts large pre-trained models to new tasks by updating only a small subset of parameters ~\cite{bib.bib44}. Among them soft prompt optimization ~\cite{bib.bib44 bib.bib17 bib.bib15} insert trainable continuous vectors into the model. Especially prompt-tuning ~\cite{bib.bib15} inserts a single prefix at the input level which could be implemented without touching the existing LLM infrastructure. The developer may pass a soft token (or its embeddings) to a deployed LLM. Unlike continuous soft prompt tuning hard prompt optimization focuses on generating or refining discrete prompts to enhance LLM system performance ~\cite{bib.bib48 bib.bib30 bib.bib47 bib.bib13}. Prompt tuning requires white-box access to calculate gradients while prompt optimization generally only needs black-box interaction as the optimization uses LLM judge as feedback. Recent works have used prompt tuning ~\cite{bib.bib51} or prompt optimization ~\cite{bib.bib52 bib.bib24} to mitigate jailbreaks ~\cite{bib.bib39} where the user is malicious against the system. In comparison our focus is mitigating prompt injection which is a different problem where the user and system are benign and the environment is malicious.


## Section References
[bib.bib20] Liu et al. (2024a) Yupei Liu Yuqi Jia Runpeng Geng Jinyuan Jia and Neil Zhenqiang Gong. 2024a. Formalizing and benchmarking prompt injection attacks and defenses. In USENIX Security Symposium. 1831–1847. https://www.usenix.org/conference/usenixsecurity24/presentation/liu-yupei
[bib.bib41] Willison (2022) Simon Willison. 2022. Prompt Injection Attacks against GPT-3. https://simonwillison.net/2022/Sep/12/prompt-injection/
[bib.bib19] Liu et al. (2024b) Xiaogeng Liu Zhiyuan Yu Yizhe Zhang Ning Zhang and Chaowei Xiao. 2024b. Automatic and Universal Prompt Injection Attacks against Large Language Models. https://arxiv.org/abs/2403.04957
[bib.bib27] Pasquini et al. (2024) Dario Pasquini Martin Strohmeier and Carmela Troncoso. 2024. Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks. In Workshop on Artificial Intelligence and Security (AISec). 89–100. doi:10.1145/3689932.3694764 [https://doi.org/10.1145/3689932.3694764]
[bib.bib32] Rehberger (2023) Johann Rehberger. 2023. Hacking Google Bard - From Prompt Injection to Data Exfiltration. https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration
[bib.bib29] PromptArmor (2024) PromptArmor. 2024. Data Exfiltration from Slack AI via indirect prompt injection. https://promptarmor.substack.com/p/data-exfiltration-from-slack-ai-via
[bib.bib33] Rehberger (2024) Johann Rehberger. 2024. ZombAIs: From Prompt Injection to C2 with Claude Computer Use. https://embracethered.com/blog/posts/2024/claude-computer-use-c2-the-zombais-are-coming.
[bib.bib31] Red (2025) Embrace The Red. 2025. ChatGPT Operator: Prompt Injection Exploits & Defenses. https://embracethered.com/blog/posts/2025/chatgpt-operator-prompt-injection-exploits
[bib.bib18] Lin et al. (2025) Huawei Lin Yingjie Lao Tong Geng Tan Yu and Weijie Zhao. 2025. UniGuardian: A Unified Defense for Detecting Prompt Injection Backdoor Attacks and Adversarial Attacks in Large Language Models. https://arxiv.org/abs/2502.13141
[bib.bib12] Hung et al. (2025) Kuo-Han Hung Ching-Yun Ko Ambrish Rawat I-Hsin Chung Winston H. Hsu and Pin-Yu Chen. 2025. Attention Tracker: Detecting Prompt Injection Attacks in LLMs. In Findings of the Association for Computational Linguistics (NAACL). 2309–2322. https://aclanthology.org/2025.findings-naacl.123/
[bib.bib21] Liu et al. (2025) Yupei Liu Yuqi Jia Jinyuan Jia Dawn Song and Neil Zhanqiang Gong. 2025. DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks . In IEEE Symposium on Security and Privacy (SP). IEEE Computer Society 2190–2208. doi:10.1109/SP61157.2025.00250 [https://doi.org/10.1109/SP61157.2025.00250]
[bib.bib40] Wei et al. (2024) Zeming Wei Yifei Wang and Yisen Wang. 2024. Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. In International Conference on Machine Learning (ICML). https://arxiv.org/abs/2310.06387
[bib.bib35] Schulhoff (2024a) Sander Schulhoff. 2024a. Instruction Defense. https://learnprompting.org/docs/prompt_hacking/defensive_measures/instruction
[bib.bib46] Yi et al. (2025) Jingwei Yi Yueqi Xie Bin Zhu Emre Kiciman Guangzhong Sun Xing Xie and Fangzhao Wu. 2025. Benchmarking and Defending against Indirect Prompt Injection Attacks on Large Language Models. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1 (KDD). 1809–1820. doi:10.1145/3690624.3709179 [https://doi.org/10.1145/3690624.3709179]
[bib.bib36] Schulhoff (2024b) Sander Schulhoff. 2024b. Sandwich Defense. https://learnprompting.org/docs/prompt_hacking/defensive_measures/sandwich_defense
[bib.bib42] Wu et al. (2025a) Tong Wu Chong Xiang Jiachen T. Wang and Prateek Mittal. 2025a. Effectively Controlling Reasoning Models through Thinking Intervention. arXiv:2503.24370 https://arxiv.org/abs/2503.24370
[bib.bib8] Debenedetti et al. (2025) Edoardo Debenedetti Ilia Shumailov Tianqi Fan Jamie Hayes Nicholas Carlini Daniel Fabian Christoph Kern Chongyang Shi Andreas Terzis and Florian Tramèr. 2025. Defeating prompt injections by design. arXiv preprint arXiv:2503.18813 (2025). https://arxiv.org/abs/2503.18813
[bib.bib28] Piet et al. (2023) Julien Piet Maha Alrashed Chawin Sitawarin Sizhe Chen Zeming Wei Elizabeth Sun Basel Alomair and David Wagner. 2023. Jatmo: Prompt Injection Defense by Task-Specific Finetuning. In European Symposium on Research in Computer Security (ESORICS). 105–124. doi:10.1007/978-3-031-70879-4_6 [https://doi.org/10.1007/978-3-031-70879-4_6]
[bib.bib4] Chen et al. (2025a) Sizhe Chen Julien Piet Chawin Sitawarin and David Wagner. 2025a. StruQ: Defending against prompt injection with structured queries. In USENIX Security Symposium. https://arxiv.org/abs/2402.06363
[bib.bib5] Chen et al. (2025b) Sizhe Chen Arman Zharmagambetov Saeed Mahloujifar Kamalika Chaudhuri David Wagner and Chuan Guo. 2025b. SecAlign: Defending Against Prompt Injection with Preference Optimization. In The ACM Conference on Computer and Communications Security (CCS). https://arxiv.org/abs/2410.05451
[bib.bib6] Chen et al. (2025c) Sizhe Chen Arman Zharmagambetov David Wagner and Chuan Guo. 2025c. Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks. arXiv:2507.02735 (2025). https://arxiv.org/abs/2507.02735
[bib.bib43] Wu et al. (2025b) Tong Wu Shujian Zhang Kaiqiang Song Silei Xu Sanqiang Zhao Ravi Agrawal Sathish Reddy Indurthi Chong Xiang Prateek Mittal and Wenxuan Zhou. 2025b. Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy. In International Conference on Learning Representations (ICLR). https://arxiv.org/abs/2410.09102
[bib.bib38] Wallace et al. (2024) Eric Wallace Kai Xiao Reimar Leike Lilian Weng Johannes Heidecke and Alex Beutel. 2024. The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions. arXiv:2404.13208 https://arxiv.org/abs/2404.13208
[bib.bib25] OpenAI (2024) OpenAI. 2024. Safety evaluations hub. https://openai.com/safety/evaluations-hub.
[bib.bib37] Shi et al. (2025) Chongyang Shi Sharon Lin Shuang Song Jamie Hayes Ilia Shumailov Itay Yona Juliette Pluto Aneesh Pappu Christopher A Choquette-Choo Milad Nasr et al. 2025. Lessons from Defending Gemini Against Indirect Prompt Injections. arXiv preprint arXiv:2505.14534 (2025). https://arxiv.org/abs/2505.14534
[bib.bib44] Xu et al. (2023) Lingling Xu Haoran Xie Si-Zhao Joe Qin Xiaohui Tao and Fu Lee Wang. 2023. Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment. https://arxiv.org/abs/2312.12148
[bib.bib17] Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Association for Computational Linguistics (ACL). 4582–4597. doi:10.18653/v1/2021.acl-long.353 [https://doi.org/10.18653/v1/2021.acl-long.353]
[bib.bib15] Lester et al. (2021) Brian Lester Rami Al-Rfou and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Empirical Methods in Natural Language Processing (EMNLP) Marie-Francine Moens Xuanjing Huang Lucia Specia and Scott Wen-tau Yih (Eds.). 3045–3059. doi:10.18653/v1/2021.emnlp-main.243 [https://doi.org/10.18653/v1/2021.emnlp-main.243]
[bib.bib48] Yuksekgonul et al. (2025) Mert Yuksekgonul Federico Bianchi Joseph Boen Sheng Liu Pan Lu Zhi Huang Carlos Guestrin and James Zou. 2025. Optimizing Generative AI by Backpropagating Language Model Feedback. In Nature Vol. 639. 609–616. doi:10.1038/s41586-025-08661-4 [https://doi.org/10.1038/s41586-025-08661-4]
[bib.bib30] Pryzant et al. (2023) Reid Pryzant Dan Iter Jerry Li Yin Lee Chenguang Zhu and Michael Zeng. 2023. Automatic Prompt Optimization with “Gradient Descent” and Beam Search. In Empirical Methods in Natural Language Processing (EMNLP). 7957–7968. doi:10.18653/v1/2023.emnlp-main.494 [https://doi.org/10.18653/v1/2023.emnlp-main.494]
[bib.bib47] Yin and Wang (2025) Li Yin and Zhangyang Wang. 2025. LLM-AutoDiff: Auto-Differentiate Any LLM Workflow. https://ui.adsabs.harvard.edu/abs/2025arXiv250116673Y/abstract
[bib.bib13] Khattab et al. (2024) Omar Khattab Arnav Singhvi Paridhi Maheshwari Zhiyuan Zhang Keshav Santhanam Sri Vardhamanan Saiful Haq Ashutosh Sharma Thomas T. Joshi Hanna Moazam Heather Miller Matei Zaharia and Christopher Potts. 2024. DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. https://openreview.net/pdf?id=sY5N0zY5Od
[bib.bib51] Zheng et al. (2024) Chujie Zheng Fan Yin Hao Zhou Fandong Meng Jie Zhou Kai-Wei Chang Minlie Huang and Nanyun Peng. 2024. On Prompt-Driven Safeguarding for Large Language Models. In International Conference on Machine Learning (ICML). 61593–61613. https://arxiv.org/abs/2401.18018
[bib.bib52] Zhou et al. (2024) Andy Zhou Bo Li and Haohan Wang. 2024. Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks. In Annual Conference on Neural Information Processing Systems (NeurIPS). https://arxiv.org/abs/2401.17263
[bib.bib24] Mo et al. (2024) Yichuan Mo Yuji Wang Zeming Wei and Yisen Wang. 2024. Fight back against jailbreaking via prompt adversarial tuning. In Annual Conference on Neural Information Processing Systems (NeurIPS). https://proceedings.neurips.cc/paper_files/paper/2024/file/759ca99a82e2a9137c6bef4811c8d378-Paper-Conference.pdf
[bib.bib39] Wei et al. (2023) Alexander Wei Nika Haghtalab and Jacob Steinhardt. 2023. Jailbroken: How Does LLM Safety Training Fail?. In Neural Information Processing Systems (NeurIPS). https://proceedings.neurips.cc/paper_files/paper/2023/hash/fd6613131889a4b656206c50a8bd7790-Abstract-Conference.html