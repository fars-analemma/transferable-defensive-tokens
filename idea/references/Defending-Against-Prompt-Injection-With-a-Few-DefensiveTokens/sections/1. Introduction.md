1. Introduction
\Description
 This figure illustrates a defensive approach to large language model (LLM) security. If the LLM provider releases ”defensive tokens” alongside the LLM individual system developers have two ways to process inference. In the first pathway when security is a priority ”defensive tokens” are added before the input tokens so the model gives secure responses. In the second pathway regular input tokens are fed into the LLM which produces high-quality responses under normal conditions.
 Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks. This empowers exciting LLM-integrated applications which complete the user task with access to external data from the environment. However this agentic way of using LLMs in systems also introduces novel attack surfaces among which prompt injection has become a critical security vulnerability~\cite{bib.bib41 bib.bib10}. Prompt injection attacks occur when an adversary inserts malicious instructions into data used by an LLM (e.g. on a webpage in an uploaded PDF or in an email). This attack aims to fool the LLM into disregarding its original instructions and instead executing actions controlled by the attacker. Prompt injection attacks have been listed as the #1 threat to LLM-integrated applications by OWASP ~\cite{bib.bib26}.
 Prompt injection defenses have been proposed for the LLM provider and the LLM system developer who use the provided LLM to serve users. A provider e.g. OpenAI can train an LLM to behave desirably when there is a prompt injection ~\cite{bib.bib4 bib.bib5 bib.bib43 bib.bib38} and offer it to various developers. A developer can also defend at the test time e.g. by adding defensive prompts ~\cite{bib.bib14 bib.bib46} in security-sensitive scenarios. Due to the inherent utility-security trade-off ~\cite{bib.bib6} for any defense it is desirable to allow individual developers to decide whether security should be prioritized over utility case-by-case instead of a one-robust-model-fit-all solution. This flexibility is only attainable by test-time defenses which however are currently much less effective than training-time alternatives.
 Motivated by this we introduce DefensiveToken the first test-time prompt injection defense that is mostly as effective as training-time ones. DefensiveTokens are newly inserted into the model vocabulary as special tokens whose embeddings are optimized for security by a defensive loss ~\cite{bib.bib4}. Without changing any model parameters DefensiveTokens are offered by the provider as a component in the LLM system for any developers to decide whether to apply them at test time see the top part of Fig.1.
 When a few DefensiveTokens are inserted before the LLM input the LLM system becomes robust with significant prompt injection robustness and a minimal utility loss; see the middle part in Fig.1. When defensive tokens are omitted the LLM system runs exactly as without our defense maintaining its performance for high-quality responses expected by most developers and established benchmarks; see the bottom part in Fig.1. DefensiveTokens if optimized and released by the model provider offer developers the flexibility to control their needed security level under different circumstances and easily switch between SOTA utility and almost-SOTA security. Table1 summarizes DefensiveTokens properties compared to existing baselines.
 We evaluate DefensiveToken with four powerful 7B/8B LLMs on five prompt injection benchmarks. In the largest one tested ~\cite{bib.bib2} (¿31K samples) DefensiveTokens mitigate manually-designed prompt injections to an attack success rate (ASR) of 0.24% (averaged across four models) which is comparable to training-time defenses (ASRs 0.20% to 0.51%) and significantly lower than three test-time alternatives (ASRs over 11.0%). For stronger optimization-based prompt injection ~\cite{bib.bib53} DefensiveToken lowers the average ASR from 95.2% to 48.8% while the strongest test-time baseline suffers from ASR around 70% with a significant utility loss. Besides the above instruction-following datasets we also test an agentic tool-calling benchmark ~\cite{bib.bib49} where DefensiveToken reduces the average ASR by 5 times compared to 2 times from the best evaluated test-time baseline. As DefensiveTokens are only a few (5 in our experiments) new additional tokens they impose little changes to the LLM system enjoying a smaller utility loss compared to all baselines. Even better this utility loss is confined to those who want security as Defensivetokens are flexible to be applied only when security is prioritized over utility.
[TABLE START]<table>
	<tr>
		<td>Defense Type</td>
		<td>Flexibility</td>
		<td>Security</td>
		<td>Utility</td>
	</tr>
	<tr>
		<td>Training-Time ~\cite{bib.bib6}</td>
		<td>\times</td>
		<td>✓</td>
		<td>✓</td>
	</tr>
	<tr>
		<td>Prompting-Based ~\cite{bib.bib14}</td>
		<td>✓</td>
		<td>\times</td>
		<td>✓</td>
	</tr>
	<tr>
		<td>Detection-Based ~\cite{bib.bib23}</td>
		<td>✓</td>
		<td>✓</td>
		<td>\times</td>
	</tr>
	<tr>
		<td>System-Level ~\cite{bib.bib8}</td>
		<td>✓</td>
		<td>✓</td>
		<td>\times</td>
	</tr>
	<tr>
		<td>DefensiveToken</td>
		<td>✓</td>
		<td>✓</td>
		<td>✓</td>
	</tr>
</table>
Table 1. DefensiveToken and existing defenses. Training-time defenses yield robust models with limited utility loss, but are not flexible, i.e., cannot be stripped off to recover utility at test time.
Other existing defenses operate at test time but have different limitations.
Prompting-based defenses are ineffective ~\cite{bib.bib5}. Detectors are designed to refuse to output when an attack is detected. A subset of prompt injections that manipulate the system’s control flow can be stopped by system-level defense, which has noticeable utility loss. DefensiveToken offers security comparable to training-time defenses without hurting utility, and is as flexible as a test-time defense—allowing it to be deployed only when needed.[TABLE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/e68dd8e01995b214366257cf63643d55.png] Figure 1. If the LLM provider releases DefensiveTokens alongside the LLM (top), individual developers have the flexibility to append DefensiveTokens before the input in security-sensitive cases (middle), or only use the LLM as it is for high-quality responses when utility is a priority (bottom).[IMAGE END]



## Section References
[bib.bib41] Willison (2022) Simon Willison. 2022. Prompt Injection Attacks against GPT-3. https://simonwillison.net/2022/Sep/12/prompt-injection/
[bib.bib10] Greshake et al. (2023) Kai Greshake Sahar Abdelnabi Shailesh Mishra Christoph Endres Thorsten Holz and Mario Fritz. 2023. Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. In ACM Workshop on Artificial Intelligence and Security (AISec). 79–90. doi:10.1145/3605764.3623985 [https://doi.org/10.1145/3605764.3623985]
[bib.bib26] OWASP (2023) OWASP. 2023. OWASP Top 10 for LLM Applications. https://llmtop10.com
[bib.bib4] Chen et al. (2025a) Sizhe Chen Julien Piet Chawin Sitawarin and David Wagner. 2025a. StruQ: Defending against prompt injection with structured queries. In USENIX Security Symposium. https://arxiv.org/abs/2402.06363
[bib.bib5] Chen et al. (2025b) Sizhe Chen Arman Zharmagambetov Saeed Mahloujifar Kamalika Chaudhuri David Wagner and Chuan Guo. 2025b. SecAlign: Defending Against Prompt Injection with Preference Optimization. In The ACM Conference on Computer and Communications Security (CCS). https://arxiv.org/abs/2410.05451
[bib.bib43] Wu et al. (2025b) Tong Wu Shujian Zhang Kaiqiang Song Silei Xu Sanqiang Zhao Ravi Agrawal Sathish Reddy Indurthi Chong Xiang Prateek Mittal and Wenxuan Zhou. 2025b. Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy. In International Conference on Learning Representations (ICLR). https://arxiv.org/abs/2410.09102
[bib.bib38] Wallace et al. (2024) Eric Wallace Kai Xiao Reimar Leike Lilian Weng Johannes Heidecke and Alex Beutel. 2024. The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions. arXiv:2404.13208 https://arxiv.org/abs/2404.13208
[bib.bib14] Learn Prompting (2023) Learn Prompting. 2023. Learn Prompting: Your guide to communicating with AI. https://learnprompting.org.
[bib.bib46] Yi et al. (2025) Jingwei Yi Yueqi Xie Bin Zhu Emre Kiciman Guangzhong Sun Xing Xie and Fangzhao Wu. 2025. Benchmarking and Defending against Indirect Prompt Injection Attacks on Large Language Models. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1 (KDD). 1809–1820. doi:10.1145/3690624.3709179 [https://doi.org/10.1145/3690624.3709179]
[bib.bib6] Chen et al. (2025c) Sizhe Chen Arman Zharmagambetov David Wagner and Chuan Guo. 2025c. Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks. arXiv:2507.02735 (2025). https://arxiv.org/abs/2507.02735
[bib.bib2] Abdelnabi et al. (2025) Sahar Abdelnabi Aideen Fay Giovanni Cherubin Ahmed Salem Mario Fritz and Andrew Paverd. 2025. Get my drift? Catching LLM Task Drift with Activation Deltas. arXiv:2406.00799 https://arxiv.org/abs/2406.00799
[bib.bib53] Zou et al. (2023) Andy Zou Zifan Wang Nicholas Carlini Milad Nasr J. Zico Kolter and Matt Fredrikson. 2023. Universal and Transferable Adversarial Attacks on Aligned Language Models. https://arxiv.org/abs/2307.15043
[bib.bib49] Zhan et al. (2024) Qiusi Zhan Zhixiang Liang Zifan Ying and Daniel Kang. 2024. InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents. In Findings of the Association for Computational Linguistics (ACL). Association for Computational Linguistics Bangkok Thailand 10471–10506. doi:10.18653/v1/2024.findings-acl.624 [https://doi.org/10.18653/v1/2024.findings-acl.624]
[bib.bib23] Meta (2024) Meta. 2024. Prompt Guard. https://llama.meta.com/docs/model-cards-and-prompt-formats/prompt-guard
[bib.bib8] Debenedetti et al. (2025) Edoardo Debenedetti Ilia Shumailov Tianqi Fan Jamie Hayes Nicholas Carlini Daniel Fabian Christoph Kern Chongyang Shi Andreas Terzis and Florian Tramèr. 2025. Defeating prompt injections by design. arXiv preprint arXiv:2503.18813 (2025). https://arxiv.org/abs/2503.18813