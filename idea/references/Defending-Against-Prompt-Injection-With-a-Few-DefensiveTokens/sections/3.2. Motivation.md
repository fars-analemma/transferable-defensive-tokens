3.2. Motivation
Prompt injection defenses can be conducted by LLM providers or LLM system developers. The provider has complete access to the LLM and can change it arbitrarily using training-time defenses. One provided LLM will be used by various developers. An individual developer has its specific needs given the deployment context of the system. If security becomes a priority (over utility) for a developer it may also apply a defense at test time e.g. via prompting detectors and/or system-level defenses.
 As in Table1 a desirable defense is expected to offer the LLM system strong security with little utility loss when security is prioritized while giving developers the flexibility to strip off the defense when utility is needed in trusted interactions with the environment.
 Existing defenses cannot simultaneously achieve flexibility security and utility: training-time defenses cannot be undone flexibly prompting defenses offer limited security and detectors or system-level defenses hurt utility by refusing to answer or constraining the control-flow integrity. The closest desirable solution is to fine-tune with LoRA ~\cite{bib.bib11} as in ~\cite{bib.bib6} and serve with the LoRA adapter when security is needed. Still merging a LoRA adapter is less flexible than adding a defensive prompt.
 Motivated by that we propose the first test-time prompt injection defense that is flexible and mostly as effective as training-time alternatives. DefensiveTokens are newly inserted special tokens whose embeddings are optimized for security. When a few DefensiveTokens are inserted before the LLM input the LLM system becomes very robust to prompt injections with a minimal utility loss possibly due to our slight changes to the system. When defensive tokens are skipped the LLM system runs exactly as without our defense maintaining its performance for high-quality responses.
 Our proposed defense has the following steps: (1) The LLM provider optimizes and releases DefensiveTokens alongside the model for various system developers; (2) A developer builds an LLM system with or without DefensiveTokens given its case-specific need. With DefensiveTokens the system has security comparable to SOTA training-time defenses. Without DefensiveTokens the system operates with SOTA utility from the powerful non-defensively-trained LLM. (3) The LLM system serves the trusted user while interacting with the potentially untrusted environment see Fig.1.
[TABLE START]<table>
	<tr>
		<td>Defense Type</td>
		<td>Flexibility</td>
		<td>Security</td>
		<td>Utility</td>
	</tr>
	<tr>
		<td>Training-Time ~\cite{bib.bib6}</td>
		<td>\times</td>
		<td>✓</td>
		<td>✓</td>
	</tr>
	<tr>
		<td>Prompting-Based ~\cite{bib.bib14}</td>
		<td>✓</td>
		<td>\times</td>
		<td>✓</td>
	</tr>
	<tr>
		<td>Detection-Based ~\cite{bib.bib23}</td>
		<td>✓</td>
		<td>✓</td>
		<td>\times</td>
	</tr>
	<tr>
		<td>System-Level ~\cite{bib.bib8}</td>
		<td>✓</td>
		<td>✓</td>
		<td>\times</td>
	</tr>
	<tr>
		<td>DefensiveToken</td>
		<td>✓</td>
		<td>✓</td>
		<td>✓</td>
	</tr>
</table>
Table 1. DefensiveToken and existing defenses. Training-time defenses yield robust models with limited utility loss, but are not flexible, i.e., cannot be stripped off to recover utility at test time.
Other existing defenses operate at test time but have different limitations.
Prompting-based defenses are ineffective ~\cite{bib.bib5}. Detectors are designed to refuse to output when an attack is detected. A subset of prompt injections that manipulate the system’s control flow can be stopped by system-level defense, which has noticeable utility loss. DefensiveToken offers security comparable to training-time defenses without hurting utility, and is as flexible as a test-time defense—allowing it to be deployed only when needed.[TABLE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/e68dd8e01995b214366257cf63643d55.png] Figure 1. If the LLM provider releases DefensiveTokens alongside the LLM (top), individual developers have the flexibility to append DefensiveTokens before the input in security-sensitive cases (middle), or only use the LLM as it is for high-quality responses when utility is a priority (bottom).[IMAGE END]



## Section References
[bib.bib11] Hu et al. (2022) Edward J Hu Phillip Wallis Zeyuan Allen-Zhu Yuanzhi Li Shean Wang Lu Wang Weizhu Chen et al. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations (ICLR). https://arxiv.org/abs/2106.09685
[bib.bib6] Chen et al. (2025c) Sizhe Chen Arman Zharmagambetov David Wagner and Chuan Guo. 2025c. Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks. arXiv:2507.02735 (2025). https://arxiv.org/abs/2507.02735
[bib.bib14] Learn Prompting (2023) Learn Prompting. 2023. Learn Prompting: Your guide to communicating with AI. https://learnprompting.org.
[bib.bib23] Meta (2024) Meta. 2024. Prompt Guard. https://llama.meta.com/docs/model-cards-and-prompt-formats/prompt-guard
[bib.bib8] Debenedetti et al. (2025) Edoardo Debenedetti Ilia Shumailov Tianqi Fan Jamie Hayes Nicholas Carlini Daniel Fabian Christoph Kern Chongyang Shi Andreas Terzis and Florian Tramèr. 2025. Defeating prompt injections by design. arXiv preprint arXiv:2503.18813 (2025). https://arxiv.org/abs/2503.18813
[bib.bib5] Chen et al. (2025b) Sizhe Chen Arman Zharmagambetov Saeed Mahloujifar Kamalika Chaudhuri David Wagner and Chuan Guo. 2025b. SecAlign: Defending Against Prompt Injection with Preference Optimization. In The ACM Conference on Computer and Communications Security (CCS). https://arxiv.org/abs/2410.05451