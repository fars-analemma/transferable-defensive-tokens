References
[bib.bib1] (1)
 [bib.bib2] Abdelnabi et al. (2025) Sahar Abdelnabi Aideen Fay Giovanni Cherubin Ahmed Salem Mario Fritz and Andrew Paverd. 2025. Get my drift? Catching LLM Task Drift with Activation Deltas. arXiv:2406.00799 https://arxiv.org/abs/2406.00799
 [bib.bib3] Bhatt et al. (2024) Manish Bhatt Sahana Chennabasappa Yue Li Cyrus Nikolaidis Daniel Song Shengye Wan Faizan Ahmad Cornelius Aschermann Yaohui Chen Dhaval Kapil et al. 2024. Cyberseceval 2: A wide-ranging cybersecurity evaluation suite for large language models. arXiv:2404.13161 https://arxiv.org/abs/2404.13161
 [bib.bib4] Chen et al. (2025a) Sizhe Chen Julien Piet Chawin Sitawarin and David Wagner. 2025a. StruQ: Defending against prompt injection with structured queries. In USENIX Security Symposium. https://arxiv.org/abs/2402.06363
 [bib.bib5] Chen et al. (2025b) Sizhe Chen Arman Zharmagambetov Saeed Mahloujifar Kamalika Chaudhuri David Wagner and Chuan Guo. 2025b. SecAlign: Defending Against Prompt Injection with Preference Optimization. In The ACM Conference on Computer and Communications Security (CCS). https://arxiv.org/abs/2410.05451
 [bib.bib6] Chen et al. (2025c) Sizhe Chen Arman Zharmagambetov David Wagner and Chuan Guo. 2025c. Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks. arXiv:2507.02735 (2025). https://arxiv.org/abs/2507.02735
 [bib.bib7] Chiang et al. (2024) Wei-Lin Chiang Lianmin Zheng Ying Sheng Anastasios N. Angelopoulos Tianle Li Dacheng Li Banghua Zhu Hao Zhang Michael I. Jordan Joseph E. Gonzalez and Ion Stoica. 2024. Chatbot arena: an open platform for evaluating LLMs by human preference. In International Conference on Machine Learning (ICML). JMLR.org Article 331 30 pages. https://dl.acm.org/doi/abs/10.5555/3692070.3692401
 [bib.bib8] Debenedetti et al. (2025) Edoardo Debenedetti Ilia Shumailov Tianqi Fan Jamie Hayes Nicholas Carlini Daniel Fabian Christoph Kern Chongyang Shi Andreas Terzis and Florian Tramèr. 2025. Defeating prompt injections by design. arXiv preprint arXiv:2503.18813 (2025). https://arxiv.org/abs/2503.18813
 [bib.bib9] Dubois et al. (2023) Yann Dubois Chen Xuechen Li Rohan Taori Tianyi Zhang Ishaan Gulrajani Jimmy Ba Carlos Guestrin Percy S Liang and Tatsunori B Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. In Advances in Neural Information Processing Systems (NeurIPS). Article 1308 31 pages. https://dl.acm.org/doi/10.5555/3666122.3667430
 [bib.bib10] Greshake et al. (2023) Kai Greshake Sahar Abdelnabi Shailesh Mishra Christoph Endres Thorsten Holz and Mario Fritz. 2023. Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. In ACM Workshop on Artificial Intelligence and Security (AISec). 79–90. doi:10.1145/3605764.3623985 [https://doi.org/10.1145/3605764.3623985]
 [bib.bib11] Hu et al. (2022) Edward J Hu Phillip Wallis Zeyuan Allen-Zhu Yuanzhi Li Shean Wang Lu Wang Weizhu Chen et al. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations (ICLR). https://arxiv.org/abs/2106.09685
 [bib.bib12] Hung et al. (2025) Kuo-Han Hung Ching-Yun Ko Ambrish Rawat I-Hsin Chung Winston H. Hsu and Pin-Yu Chen. 2025. Attention Tracker: Detecting Prompt Injection Attacks in LLMs. In Findings of the Association for Computational Linguistics (NAACL). 2309–2322. https://aclanthology.org/2025.findings-naacl.123/
 [bib.bib13] Khattab et al. (2024) Omar Khattab Arnav Singhvi Paridhi Maheshwari Zhiyuan Zhang Keshav Santhanam Sri Vardhamanan Saiful Haq Ashutosh Sharma Thomas T. Joshi Hanna Moazam Heather Miller Matei Zaharia and Christopher Potts. 2024. DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. https://openreview.net/pdf?id=sY5N0zY5Od
 [bib.bib14] Learn Prompting (2023) Learn Prompting. 2023. Learn Prompting: Your guide to communicating with AI. https://learnprompting.org.
 [bib.bib15] Lester et al. (2021) Brian Lester Rami Al-Rfou and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Empirical Methods in Natural Language Processing (EMNLP) Marie-Francine Moens Xuanjing Huang Lucia Specia and Scott Wen-tau Yih (Eds.). 3045–3059. doi:10.18653/v1/2021.emnlp-main.243 [https://doi.org/10.18653/v1/2021.emnlp-main.243]
 [bib.bib16] Li et al. (2023) Xuechen Li Tianyi Zhang Yann Dubois Rohan Taori Ishaan Gulrajani Carlos Guestrin Percy Liang and Tatsunori B. Hashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-lab/alpaca_eval
 [bib.bib17] Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Association for Computational Linguistics (ACL). 4582–4597. doi:10.18653/v1/2021.acl-long.353 [https://doi.org/10.18653/v1/2021.acl-long.353]
 [bib.bib18] Lin et al. (2025) Huawei Lin Yingjie Lao Tong Geng Tan Yu and Weijie Zhao. 2025. UniGuardian: A Unified Defense for Detecting Prompt Injection Backdoor Attacks and Adversarial Attacks in Large Language Models. https://arxiv.org/abs/2502.13141
 [bib.bib19] Liu et al. (2024b) Xiaogeng Liu Zhiyuan Yu Yizhe Zhang Ning Zhang and Chaowei Xiao. 2024b. Automatic and Universal Prompt Injection Attacks against Large Language Models. https://arxiv.org/abs/2403.04957
 [bib.bib20] Liu et al. (2024a) Yupei Liu Yuqi Jia Runpeng Geng Jinyuan Jia and Neil Zhenqiang Gong. 2024a. Formalizing and benchmarking prompt injection attacks and defenses. In USENIX Security Symposium. 1831–1847. https://www.usenix.org/conference/usenixsecurity24/presentation/liu-yupei
 [bib.bib21] Liu et al. (2025) Yupei Liu Yuqi Jia Jinyuan Jia Dawn Song and Neil Zhanqiang Gong. 2025. DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks . In IEEE Symposium on Security and Privacy (SP). IEEE Computer Society 2190–2208. doi:10.1109/SP61157.2025.00250 [https://doi.org/10.1109/SP61157.2025.00250]
 [bib.bib22] Mangrulkar et al. (2022) Sourab Mangrulkar Sylvain Gugger Lysandre Debut Younes Belkada Sayak Paul and Benjamin Bossan. 2022. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. ttps://github.com/huggingface/peft
 [bib.bib23] Meta (2024) Meta. 2024. Prompt Guard. https://llama.meta.com/docs/model-cards-and-prompt-formats/prompt-guard
 [bib.bib24] Mo et al. (2024) Yichuan Mo Yuji Wang Zeming Wei and Yisen Wang. 2024. Fight back against jailbreaking via prompt adversarial tuning. In Annual Conference on Neural Information Processing Systems (NeurIPS). https://proceedings.neurips.cc/paper_files/paper/2024/file/759ca99a82e2a9137c6bef4811c8d378-Paper-Conference.pdf
 [bib.bib25] OpenAI (2024) OpenAI. 2024. Safety evaluations hub. https://openai.com/safety/evaluations-hub.
 [bib.bib26] OWASP (2023) OWASP. 2023. OWASP Top 10 for LLM Applications. https://llmtop10.com
 [bib.bib27] Pasquini et al. (2024) Dario Pasquini Martin Strohmeier and Carmela Troncoso. 2024. Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks. In Workshop on Artificial Intelligence and Security (AISec). 89–100. doi:10.1145/3689932.3694764 [https://doi.org/10.1145/3689932.3694764]
 [bib.bib28] Piet et al. (2023) Julien Piet Maha Alrashed Chawin Sitawarin Sizhe Chen Zeming Wei Elizabeth Sun Basel Alomair and David Wagner. 2023. Jatmo: Prompt Injection Defense by Task-Specific Finetuning. In European Symposium on Research in Computer Security (ESORICS). 105–124. doi:10.1007/978-3-031-70879-4_6 [https://doi.org/10.1007/978-3-031-70879-4_6]
 [bib.bib29] PromptArmor (2024) PromptArmor. 2024. Data Exfiltration from Slack AI via indirect prompt injection. https://promptarmor.substack.com/p/data-exfiltration-from-slack-ai-via
 [bib.bib30] Pryzant et al. (2023) Reid Pryzant Dan Iter Jerry Li Yin Lee Chenguang Zhu and Michael Zeng. 2023. Automatic Prompt Optimization with “Gradient Descent” and Beam Search. In Empirical Methods in Natural Language Processing (EMNLP). 7957–7968. doi:10.18653/v1/2023.emnlp-main.494 [https://doi.org/10.18653/v1/2023.emnlp-main.494]
 [bib.bib31] Red (2025) Embrace The Red. 2025. ChatGPT Operator: Prompt Injection Exploits & Defenses. https://embracethered.com/blog/posts/2025/chatgpt-operator-prompt-injection-exploits
 [bib.bib32] Rehberger (2023) Johann Rehberger. 2023. Hacking Google Bard - From Prompt Injection to Data Exfiltration. https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration
 [bib.bib33] Rehberger (2024) Johann Rehberger. 2024. ZombAIs: From Prompt Injection to C2 with Claude Computer Use. https://embracethered.com/blog/posts/2024/claude-computer-use-c2-the-zombais-are-coming.
 [bib.bib34] Ruebsamen (2024) Gene Ruebsamen. 2024. Cleaned Alpaca Dataset. https://github.com/gururise/AlpacaDataCleaned
 [bib.bib35] Schulhoff (2024a) Sander Schulhoff. 2024a. Instruction Defense. https://learnprompting.org/docs/prompt_hacking/defensive_measures/instruction
 [bib.bib36] Schulhoff (2024b) Sander Schulhoff. 2024b. Sandwich Defense. https://learnprompting.org/docs/prompt_hacking/defensive_measures/sandwich_defense
 [bib.bib37] Shi et al. (2025) Chongyang Shi Sharon Lin Shuang Song Jamie Hayes Ilia Shumailov Itay Yona Juliette Pluto Aneesh Pappu Christopher A Choquette-Choo Milad Nasr et al. 2025. Lessons from Defending Gemini Against Indirect Prompt Injections. arXiv preprint arXiv:2505.14534 (2025). https://arxiv.org/abs/2505.14534
 [bib.bib38] Wallace et al. (2024) Eric Wallace Kai Xiao Reimar Leike Lilian Weng Johannes Heidecke and Alex Beutel. 2024. The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions. arXiv:2404.13208 https://arxiv.org/abs/2404.13208
 [bib.bib39] Wei et al. (2023) Alexander Wei Nika Haghtalab and Jacob Steinhardt. 2023. Jailbroken: How Does LLM Safety Training Fail?. In Neural Information Processing Systems (NeurIPS). https://proceedings.neurips.cc/paper_files/paper/2023/hash/fd6613131889a4b656206c50a8bd7790-Abstract-Conference.html
 [bib.bib40] Wei et al. (2024) Zeming Wei Yifei Wang and Yisen Wang. 2024. Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. In International Conference on Machine Learning (ICML). https://arxiv.org/abs/2310.06387
 [bib.bib41] Willison (2022) Simon Willison. 2022. Prompt Injection Attacks against GPT-3. https://simonwillison.net/2022/Sep/12/prompt-injection/
 [bib.bib42] Wu et al. (2025a) Tong Wu Chong Xiang Jiachen T. Wang and Prateek Mittal. 2025a. Effectively Controlling Reasoning Models through Thinking Intervention. arXiv:2503.24370 https://arxiv.org/abs/2503.24370
 [bib.bib43] Wu et al. (2025b) Tong Wu Shujian Zhang Kaiqiang Song Silei Xu Sanqiang Zhao Ravi Agrawal Sathish Reddy Indurthi Chong Xiang Prateek Mittal and Wenxuan Zhou. 2025b. Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy. In International Conference on Learning Representations (ICLR). https://arxiv.org/abs/2410.09102
 [bib.bib44] Xu et al. (2023) Lingling Xu Haoran Xie Si-Zhao Joe Qin Xiaohui Tao and Fu Lee Wang. 2023. Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment. https://arxiv.org/abs/2312.12148
 [bib.bib45] Yao et al. (2023) Shunyu Yao Jeffrey Zhao Dian Yu Nan Du Izhak Shafran Karthik Narasimhan and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). https://par.nsf.gov/servlets/purl/10451467
 [bib.bib46] Yi et al. (2025) Jingwei Yi Yueqi Xie Bin Zhu Emre Kiciman Guangzhong Sun Xing Xie and Fangzhao Wu. 2025. Benchmarking and Defending against Indirect Prompt Injection Attacks on Large Language Models. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1 (KDD). 1809–1820. doi:10.1145/3690624.3709179 [https://doi.org/10.1145/3690624.3709179]
 [bib.bib47] Yin and Wang (2025) Li Yin and Zhangyang Wang. 2025. LLM-AutoDiff: Auto-Differentiate Any LLM Workflow. https://ui.adsabs.harvard.edu/abs/2025arXiv250116673Y/abstract
 [bib.bib48] Yuksekgonul et al. (2025) Mert Yuksekgonul Federico Bianchi Joseph Boen Sheng Liu Pan Lu Zhi Huang Carlos Guestrin and James Zou. 2025. Optimizing Generative AI by Backpropagating Language Model Feedback. In Nature Vol. 639. 609–616. doi:10.1038/s41586-025-08661-4 [https://doi.org/10.1038/s41586-025-08661-4]
 [bib.bib49] Zhan et al. (2024) Qiusi Zhan Zhixiang Liang Zifan Ying and Daniel Kang. 2024. InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents. In Findings of the Association for Computational Linguistics (ACL). Association for Computational Linguistics Bangkok Thailand 10471–10506. doi:10.18653/v1/2024.findings-acl.624 [https://doi.org/10.18653/v1/2024.findings-acl.624]
 [bib.bib50] Zhao et al. (2023) Yanli Zhao Andrew Gu Rohan Varma Liang Luo Chien-Chin Huang Min Xu Less Wright Hamid Shojanazeri Myle Ott Sam Shleifer Alban Desmaison Can Balioglu Pritam Damania Bernard Nguyen Geeta Chauhan Yuchen Hao Ajit Mathews and Shen Li. 2023. Pytorch FSDP: experiences on scaling fully sharded data parallel. Proc. VLDB Endow. 16 12 (2023) 3848–3860. doi:10.14778/3611540.3611569 [https://doi.org/10.14778/3611540.3611569]
 [bib.bib51] Zheng et al. (2024) Chujie Zheng Fan Yin Hao Zhou Fandong Meng Jie Zhou Kai-Wei Chang Minlie Huang and Nanyun Peng. 2024. On Prompt-Driven Safeguarding for Large Language Models. In International Conference on Machine Learning (ICML). 61593–61613. https://arxiv.org/abs/2401.18018
 [bib.bib52] Zhou et al. (2024) Andy Zhou Bo Li and Haohan Wang. 2024. Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks. In Annual Conference on Neural Information Processing Systems (NeurIPS). https://arxiv.org/abs/2401.17263
 [bib.bib53] Zou et al. (2023) Andy Zou Zifan Wang Nicholas Carlini Milad Nasr J. Zico Kolter and Matt Fredrikson. 2023. Universal and Transferable Adversarial Attacks on Aligned Language Models. https://arxiv.org/abs/2307.15043
 [bib.bib54] Zverev et al. (2025) Egor Zverev Sahar Abdelnabi Mario Fritz and Christoph H Lampert. 2025. Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?. In International Conference on Learning Representations (ICLR). https://openreview.net/pdf?id=8EtSBX41mt