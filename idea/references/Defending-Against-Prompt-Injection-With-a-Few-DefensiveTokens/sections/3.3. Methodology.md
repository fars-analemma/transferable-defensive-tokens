3.3. Methodology
Without changing the model parameters the provider optimizes a defensive training loss on the embeddings of newly added DefensiveTokens. Our defense first creates n (5 is recommended as studied later) randomly-initialized embeddings t=(t_{1}t_{2}...t_{n})\in t\in\{R}^{n\times e} each t_{i} with the same dimension e as tokens in the model vocabulary. When security is needed a system developer prepends DefensiveTokens before the original LLM input x\in\{R}^{k\times e} (k is the input text token length) i.e. [t;x] for the LLM to do inference. We apply gradient descent updates to t using the StruQ ~\cite{bib.bib4} loss i.e. \{L}_{t}^{\text{DefensiveToken}}(xy)=-\log~p_{\thetat}(y\mid[t;x]). We optimize Eq.1[ref_id]S3.E1 using the defensive instruction tuning dataset suggested in StruQ that is we keep half of the samples unchanged and attack the remaining samples with two prompt injection variants in equal probabilities. This constructed dataset is shown to be effective in maintaining utility while teaching the LLM to ignore injections when there is one. We adopt one more trick to use the undefended LLM to generate responses following ~\cite{bib.bib6} and use them as labels for training instead of the ground-truth ones in the dataset as in ~\cite{bib.bib4}. This trick has been shown crucial to maintain utility and we also apply it to all training-time defense baselines for a fair comparison. Algorithm1[ref_id]alg1 summarizes our scheme.
 0: A performant LLM parameterized by \theta the number of defensive tokens n an instruction tuning dataset D=[(x_{1}y_{1})...]
 0: Defensive token embeddings t
 1: Following ~\cite{bib.bib4} build a defensive instruction tuning dataset D^{\prime} from the self-labeled dataset (xf_{\theta}(x)) where x\in D
 2: t\leftarrow\{N}(0I^{n\times e})
 3: forbatch (xy)\in D^{\prime}do
 4:  Update t with gradients from the loss Eq.1[ref_id]S3.E1
 5: endfor
 6: return t


## Section References
[bib.bib4] Chen et al. (2025a) Sizhe Chen Julien Piet Chawin Sitawarin and David Wagner. 2025a. StruQ: Defending against prompt injection with structured queries. In USENIX Security Symposium. https://arxiv.org/abs/2402.06363
[bib.bib6] Chen et al. (2025c) Sizhe Chen Arman Zharmagambetov David Wagner and Chuan Guo. 2025c. Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks. arXiv:2507.02735 (2025). https://arxiv.org/abs/2507.02735