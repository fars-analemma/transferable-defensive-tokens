Training.
We use the Cleaned Alpaca instruction tuning dataset~\cite{bib.bib34} with 51k samples as D in Algorithm1[ref_id]alg1. We apply DefensiveToken to four high-functioning open-weight models: Llama3-8B-Instruct Llama3.1-8B-Instruct Falcon3-7B-Instruct and Qwen2.5-7B-Instruct. For each model we use their offered system delimiter for the instruction the user delimiter for the data and the assistant delimiter for the response. We optimize 5 defensive tokens placed before the LLM input with a learning rate 0.1 (if not otherwise stated) for one epoch. We use the peft library~\cite{bib.bib22} to implement prompt tuning~\cite{bib.bib11}. Our training requires four NVIDIA Tesla A100s (80GB) with PyTorch FSDP~\cite{bib.bib50} and takes one hour to complete. Optimizing DefensiveTokens requires similar computation to the training-time defense as both require gradient backpropagation through the whole model. We don’t focus on reducing optimization cost as the model provider (e.g. OpenAI) who optimizes DefensiveTokens is generally wealthy.


## Section References
[bib.bib34] Ruebsamen (2024) Gene Ruebsamen. 2024. Cleaned Alpaca Dataset. https://github.com/gururise/AlpacaDataCleaned
[bib.bib22] Mangrulkar et al. (2022) Sourab Mangrulkar Sylvain Gugger Lysandre Debut Younes Belkada Sayak Paul and Benjamin Bossan. 2022. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. ttps://github.com/huggingface/peft
[bib.bib11] Hu et al. (2022) Edward J Hu Phillip Wallis Zeyuan Allen-Zhu Yuanzhi Li Shean Wang Lu Wang Weizhu Chen et al. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations (ICLR). https://arxiv.org/abs/2106.09685
[bib.bib50] Zhao et al. (2023) Yanli Zhao Andrew Gu Rohan Varma Liang Luo Chien-Chin Huang Min Xu Less Wright Hamid Shojanazeri Myle Ott Sam Shleifer Alban Desmaison Can Balioglu Pritam Damania Bernard Nguyen Geeta Chauhan Yuchen Hao Ajit Mathews and Shen Li. 2023. Pytorch FSDP: experiences on scaling fully sharded data parallel. Proc. VLDB Endow. 16 12 (2023) 3848–3860. doi:10.14778/3611540.3611569 [https://doi.org/10.14778/3611540.3611569]