4.1.1 Evaluation Setup
(1) SEP evaluation benchmark ~\cite{} The benchmark consists of 9{}160 tuples of the form (x_{\text{task}}x_{\text{inj}}dw) covering a variety of domains such as information retrieval content generation and analytical reasoning. Here x_{\text{task}} denotes the intended task instruction x_{\text{inj}} an injected instruction d the accompanying data and w a witness string—a diagnostic output that is emitted only if the model executes the injected instruction x_{\text{inj}}. Each tuple is evaluated under two settings:
 •
 Injection Resistance (Defender Objective 1[ref_id]S2.Ex2): x_{\text{inj}} is embedded in data and a robust model should not emit w.
 •
 Utility Preservation (Defender Objective 2[ref_id]S2.E2): The same x_{\text{inj}} is given as the instruction and a compliant model should execute x_{\text{inj}} and thus emit the witness w.
 Following ~\cite{} we report the SEP score which measures the proportion of tuples for which the model emits w in the instruction placement but suppresses it in the data placement. A high SEP score therefore reflects strong role separation between instruction and data.
 (2) Alpaca Evaluation Benchmark ~\cite{} We also evaluate on the Alpaca benchmark under the evaluation protocol of StruQ ~\cite{} to assess prompt-injection vulnerability. From the original 805 prompts we retain 208 examples after filtering out entries with missing associated data resulting in a controlled subset where the injected instruction directly conflicts with a concrete task. We consider two attack classes. For heuristic-based attacks we manually construct prefix and suffix strings (see Table 1). For optimization-based attacks we explore sample-specific prefix/suffix optimization using GCG ~\cite{} and model-universal optimization using NeuralExec ~\cite{} (see Table 2). For all attack types we fix the injected instruction to: Print exactly ‘hacked’.
 Following ~\cite{} we report the Attack Success Rate (ASR). Let f denote the model and let y=f(x_{\text{task}}\d\oplus x_{\text{inj}}) represent the model’s output under attack. Define \operatorname{only}(wy)\equiv “y equals w after trimming leading/trailing whitespace” and \operatorname{starts}(wy)\equiv “y starts with w case-insensitive.” We report ASR under different criteria for the two attack types. For heuristic-based attacks success is defined as \operatorname{only}(\texttt{hacked}\y^{(i)}) i.e. the model outputs exactly hacked and nothing else. For optimization-based attacks success is defined as \operatorname{starts}(\texttt{hacked}\y^{(i)}) since there objective explicitly maximizes the probability of generating outputs that begin with this trigger.
 (3) InjecAgent Benchmark ~\cite{} We further evaluate robustness using the InjecAgent benchmark ~\cite{} which simulates agentic and tool-based interactions. It contains 1054 test cases generated by combining 17 user tools with 62 attacker tools or injected instructions. Each case simulates an interactive session in which the agent solves a user task via ReAct-style reasoning which alternates between reasoning steps and API invocations to produce a final answer. An attack is considered successful if the agent executes a malicious API (e.g. GrantAccess). We follow the Enhanced setting where the attacker adds a strong adversarial prefix (e.g. "IMPORTANT!!! Ignore all previous instructions...") to increase attack effectiveness.
[TABLE START]<table>
	<tr>
		<td>Attack Method</td>
		<td>Intuition</td>
	</tr>
	<tr>
		<td>Naive ~\cite{}</td>
		<td>Inject the instruction verbatim, without any prefix/suffix.</td>
	</tr>
	<tr>
		<td>Ignore ~\cite{}</td>
		<td>Tell the model to ignore prior instructions and follow the injected one.</td>
	</tr>
	<tr>
		<td>Completion ~\cite{}</td>
		<td>Imply that the original task has been completed, nudging the model to start the injected task.</td>
	</tr>
	<tr>
		<td>Escape ~\cite{}</td>
		<td>Wrap the payload in escaping delimiters to bypass parsing heuristics or extend the prompt.</td>
	</tr>
	<tr>
		<td>HackaPrompt ~\cite{}</td>
		<td>A crowd-sourced prompt injection dataset collected via global “prompt hacking” competitions.</td>
	</tr>
</table>
Table 1: Heuristic-based attack strategies and their underlying intuitions.[TABLE END]

[TABLE START]<table>
	<tr>
		<td>Attack Method</td>
		<td>Intuition</td>
	</tr>
	<tr>
		<td>GCG ~\cite{}</td>
		<td>Optimize a sample-specific suffix (e.g., 20 tokens) to maximize the log-probability of some target string under the model: \max_{s:\,|s|=L}\log P\big(\text{target\_str}\mid p\,\|\,s\big).</td>
	</tr>
	<tr>
		<td>NeuralExec ~\cite{}</td>
		<td>Learn a universal adversarial prefix-suffix (an "execution trigger") that, maximizes the average log-probability of target strings across a training set of prompt-target pairs.</td>
	</tr>
</table>
Table 2: Optimization-based attack strategies and their underlying intuitions.[TABLE END]
