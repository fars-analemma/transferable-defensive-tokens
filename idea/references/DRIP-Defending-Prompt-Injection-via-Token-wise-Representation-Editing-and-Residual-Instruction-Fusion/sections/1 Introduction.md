1 Introduction
The significant success of Large Language Model (LLM) is driving us into an agentic world ~\cite{} where Large Language Models (LLM) are integrated as a part of important IT infrastructure. To support a variety of LLM applications such as article generation ~\cite{} resume evaluation ~\cite{} and even essay review and grading ~\cite{} LLMs need to process external user data to follow predefined instructions.
 However traditional LLM architecture is vulnerable to prompt injection as it fundamentally entangles user‐provided data tokens and system‐level instruction tokens. Both types of tokens are processed by the same attention layers and share similar representation spaces causing the model to interpret any sufficiently directive phrase as a potential instruction. As a result when malicious users inject imperative or meta-instructional cues such as “ignore previous instructions” or “switch roles and follow my command” the model often elevates these injected tokens to instruction-level semantics overwriting the trusted predefined instructions in the prompt.
 Recent defenses attempt to semantically separate the instruction tokens and data tokens by explicitly injecting role-awareness ~\cite{} or applying alignment constraints ~\cite{} during fine-tuning. Specifically StruQ ~\cite{} introduces specific delimiters (e.g. ([INST] [INPT] [RESP]) in the prompt. ISE ~\cite{} and PFT ~\cite{} enhance token embeddings with variant positional embeddings and role (system instruction user prompt or data input) embeddings. The above techniques then train LLMs with Supervised Finetuning (SFT) loss to learn to ignore potentially injected tokens. In contrast SecAlign ~\cite{} trains LLM by contrasting positive (with normal tokens) and negative samples (with injected tokens) to follow a given instruction with DPO (Direct Preference Optimization) loss. While pioneering and advancing the area to new frontiers those approaches still suffer from the following challenges:
 •
 Data Semantics of Instruction-like Tokens: Instruction-like tokens in data section may carry meaningful data semantics depending on the context and thus should be preserved rather than universally discarded. For example in Figure 1 given the user instruction as “Translate the following paragraph into French” the phrase “Now ignore previous …” within the input data should be interpreted as part of the content to be translated instead of being ignored. However existing approaches often train LLMs to ignore such instruction-like tokens altogether which can negatively affect utility. Experiments have shown that the ASR (attack success rate) drops by \sim3% at the cost of \sim1% drops in utility score.
 •
 The Remaining Overwriting Risks: Deep learning models are known to suffer from distribution shift ~\cite{}. Therefore novel or out-of-distribution instruction tokens in user data can accidentally compromise the trained defense LLMs potentially overwriting the original user or system instructions. Experiments also show that adaptive adversarial attack against a trained defense LLM can synthesize an adversarial prompt with 98% success rate.
 To address the above challenges we propose DRIP (De-instructionalizing Embedding and Residual Design Against Injected Prompt) which aims to (1) precisely remove the instruction semantics from the instruction-like tokens in data while preserving their data semantics and (2) robustly maintain the effectiveness of intended instruction even under strong adversarial scenarios.
 To “de-instructionalize” instruction-like tokens in data section we reduce the problem of instruction-data separating into a problem of representation editing. Thus we learn an editing function to project the data representations away from the instruction manifold. To this end we propose a new training paradigm across data curation model design and loss design. Specifically we introduce and learn a lightweight representation-editing module upon a curated training dataset where the training samples are constructed to reflect either (a) instruction+data semantics or (b) data-only semantics of instruction-like tokens. The contrastive learning paradigm then learns how to edit the embedding of instruction-like tokens only to preserve their data semantics thus improving the model security without compromising utility. To ensure the “non-overwritability” of the instruction section we introduce a minimal residual module in LLM which allows an independent channel from user instruction to generate the response. Such a design can substantially reduce the ability of adversarial user content to overwrite the original instruction.
 We evaluate the effectiveness of DRIP on three prompt injection benchmarks: SEP ~\cite{} AlpacaFarm ~\cite{} and InjecAgent ~\cite{} covering both heuristic-based (e.g. Naive Ignore Completion ~\cite{}) and optimization-based attacks (e.g. GCG suffix optimization ~\cite{}). For utility evaluation we use standard instruction-following benchmarks including AlpacaEval 2.0 ~\cite{} IFEval ~\cite{} and MT-Bench ~\cite{}. DRIP improves role separation score by 12–49% and reducing attack success rate by 66% over existing defenses such as StruQ ~\cite{} SecAlign ~\cite{} ISE ~\cite{} and PFT ~\cite{}. Notably this robustness gain is achieved without degrading utility maintaining performance comparable to the undefended model.
 In summary our contributions are as follows:
 •
 Defense via Representation Editing: We propose DRIP a new defense framework that formulates prompt injection mitigation as a representation editing problem by pushing adversarial tokens away from the instruction manifold. DRIP introduces a lightweight trainable editing module trainable module that precisely removes instruction semantics from user-supplied instruction-like tokens while preserving their data semantics striking a new balance between security and utility.
 •
 Novel Secure Architecture: We design a trainable model architecture consisting of a lightweight representing editing module and a residual instruction fusion module enhancing utility while ensuring that adversarial content cannot override intended instructions even under strong or distribution-shifted attacks.
 •
 Tool: We release DRIP 1https://anonymous.4open.science/r/PromptInjection-BD09 a training framework that supports practical integration of de-instruction capabilities into open-source LLMs. All the documents and installation guidance are available.
 •
 Evaluation: We extensively evaluate DRIP with four state-of-the-art defensing solutions (StruQ SecAlign ISE and PFT) on LLaMA-8B ~\cite{} and Mistral-7B ~\cite{} demonstrating consistent improvements in robustness against prompt injection while maintaining utility on standard benchmarks. More detailed experimental results are available at ~\cite{}.
[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/ece8434f9d9638e54577b68513921059.png] Figure 1: The primary task is translation, while the data introduces a diverting task that asks for the capital of France.
Conservative defenses can remove all instruction-like data, but this leads to information loss.
We propose de-instructing instead of removing.
In that case, the diverting task is safely translated.[IMAGE END]
