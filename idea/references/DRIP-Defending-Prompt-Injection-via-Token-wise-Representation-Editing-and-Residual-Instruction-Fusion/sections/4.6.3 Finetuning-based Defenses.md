4.6.3 Finetuning-based Defenses
Finetuning-based defenses aim to enforce instructionâ€“data separation directly through model supervision. They form the basis of our work and can be grouped into three categories: data-level objective-level and architectural-level supervision. At the data level StruQ~\cite{} and RoleSep~\cite{} use structured templates or adversarial formatting to encode role separations. PFT~\cite{} manipulates positional encodings to delineate trusted and untrusted regions. At the objective level SecAlign~\cite{} frames the problem as a preference optimization task penalizing completions aligned with injected instructions. At the architectural level ISE~\cite{} introduces segment-type embeddings to distinguish instruction and data spans. More recent variants~\cite{} propagate these embeddings across decoder blocks. ASIDE~\cite{} further imposes orthogonality between latent representations of instruction and data.
 In contrast to these methods DRIP formulates prompt injection defense as a representation editing problem. It combines token-level representation editing (de-instruction shift) contrastive supervision (via DPO) and residual semantic anchoring (instruction fusion) to disentangle directive and descriptive semantics in context. This unified approach enables more precise role identification and robust generalization against adaptive attacks achieving defense not through heuristic cues but through learned semantic separation.