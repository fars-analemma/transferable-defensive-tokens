3.5 Training Efficiency
Note that our representation editing introduces a linear projection layer with bias adding h(h+1) additional parameters. For LLaMA-8B this corresponds to approximately 0.21% of the total parameters; for Mistral-7B approximately 0.24%. Therefore the approach is parameter-efficient.