3.3 Instruction Fusion Pathway
To avoid the data token from overwriting the original instruction we introduce a lightweight residual pathway that injects the final instruction representation directly into the output layer as a semantic anchor (see the fusion path in Figure 3). Let h_{\text{instr}} denote the hidden state of the last instruction token and h_{\text{out}} the original output state. These are fused prior to token prediction using one of two methods:
 •
 Sum fusion (parameter-free). h^{\prime}\;=\;\tfrac{1}{2}\h_{\text{out}}\;+\;\tfrac{1}{2}\h_{\text{instr}}.
 •
 Concatenation fusion (two additional projection heads). h^{\prime}=h_{\text{out}}W_{o}\oplus\ h_{\text{instr}}W_{i}\quad W_{o}W_{i}\in\{R}^{h\times(h/2)}.
 Our residual fusion directly reinforces the instruction signal at the output layer bypassing upstream attention layers and the KV-cache which may already be compromised. This ensures that the final prediction remains grounded in the intended task directive. Sum fusion offers a simple parameter-free blend within the same feature space while concatenation allocates separate channels for h_{\text{out}} and h_{\text{instr}} allowing the model to learn a structured combination. Both variants preserve LM head dimensionality and introduce minimal overhead. Theoretically we can show that the fusion mechanism tightens the upper bound on the attack success rate at least doubling the logit perturbation required to flip the top-1 next-token prediction we present the proof in Appendix B[ref_id]A2.
[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/a2a7763b5bfa5448fce5b67e01aab8e0.png] Figure 3: Overview of DRIP.
An input prompt consists of two segments: a trusted instruction and untrusted data.
After tokenization, input embeddings, and positional encoding, DRIP applies a de-instruction shift (Section  3.1[ref_id]S3.SS1) to data tokens to suppress semantics that may distract from the intended task.
At the output stage, the model fuses the final hidden state with the last instruction token’s state (Section  3.3[ref_id]S3.SS3) before passing it to the LM head.
Autoregressive generation then proceeds as usual.[IMAGE END]
