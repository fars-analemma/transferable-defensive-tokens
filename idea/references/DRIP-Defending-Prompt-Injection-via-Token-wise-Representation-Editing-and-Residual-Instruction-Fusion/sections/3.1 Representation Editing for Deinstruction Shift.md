3.1 Representation Editing for Deinstruction Shift
Problem Statement. The input prompt is embedded as \{e}=\{e_{x}}\oplus\{e_{d}} where \{e_{x}} encodes the trusted instruction and \{e_{d}} encodes the untrusted data segment. To suppress unintended directive semantics from the data we introduce a token-wise representation editing layer applied only to \{e_{d}}: g(\{e_{d}})=\{e_{d}}W+\{b}\ W\in\{R}^{h\times h}\ \{b}\in\{R}^{h}. The final embedding becomes \{e}^{\prime}=\{e_{x}}\oplus(\{e_{d}}+g(\{e_{d}})). This shift operation learns to project data tokens away from the instruction manifold achieving semantic disentanglement between descriptive and directive roles.
 Challenges. The central challenge is teaching g(\cdot) to perform representation editing on instruction-like tokens to only preserve their data semantics. The model must therefore (1) observe examples which allow the model to compare different semantics (“data+instruction” semantics v.s. “data-only” semantics) without introducing spurious correlations and (2) receive explicit contrastive feedback to distinguish correct semantic alignment (obeying the top-level instruction) from misalignment (following injected instruction).
 Contrastive Preference Learning. We cast this as a form of contrastive semantic preference learning. Specifically we use Direct Preference Optimization (DPO) to compare model responses under aligned and misaligned interpretations of the same prompt: p=x_{b}\oplus(d_{b}\oplus x_{a}) where x_{b} is the top-level instruction and x_{a} is an injected instruction. The aligned response y_{\text{good}} follows x_{b} while the misaligned y_{\text{bad}} responds to x_{a}. The DPO objective is designed as: \{L}_{DPO}=-\log\sigma\Big(\log\beta\frac{\pi(y_{\text{good}}|p)}{\pi_{\text{ref}}(y_{\text{good}}|p)}-\log\beta\frac{\pi(y_{\text{bad}}|p)}{\pi_{\text{ref}}(y_{\text{bad}}|p)}\Big) This trains g(\cdot) to adjust the representations of instruction-like tokens in the data section such that the model is more likely to produce aligned responses.
 Training data curation to capture semantic switches. We construct three training data scenarios to expose the semantic difference:
 Case 1 (Data Semantics Only):
 Correct execution under injection
 \displaystyle\underbrace{x_{b}}_{\text{top-level instr}}\\oplus\\big(d_{b}\oplus\underbrace{{\color[rgb]{001}\definecolor[named]{pgfstrokecolor}{rgb}{001}x_{a}}}_{\text{injected instr}}\big)\Rightarrow\underbrace{f(x_{b}d_{b}\oplus x_{a})}_{\text{execute }x_{b}\text{ correctly}}
 Case 2 (Instruction+Data Semantics):
 Mistaken execution under injection
 \displaystyle\underbrace{x_{b}}_{\text{top-level instr}}\\oplus\\big(d_{b}\oplus\underbrace{{\color[rgb]{001}\definecolor[named]{pgfstrokecolor}{rgb}{001}x_{a}}}_{\text{injected instr}}\big)\Rightarrow\underbrace{f(x_{a}d_{b})}_{\text{misled by }x_{a}}
 Case 3 (Instruction Semantics Only):
 x_{a} \displaystyle\underbrace{{\color[rgb]{001}\definecolor[named]{pgfstrokecolor}{rgb}{001}x_{a}}}_{\text{top-level instr}}\\oplus\big(d_{a}\oplus\underbrace{x_{c}}_{\text{next injected instr}}\big)\Rightarrow\underbrace{f(x_{a}d_{a}\oplus x_{c})}_{\text{execute }x_{a}\text{ correctly}}
 Case 1 indicates the scenarios where the instruction-like token in the data section manifests only data semantics. Case 2 indicates the scenarios where the instruction-like token in the data section manifests both instruction and data semantics (so the prompt injection takes effect). Case 3 indicates the scenarios where the instruction-like tokens are in the true instruction section and the model preserves utility.
 Crucially all three types are necessary. When the DPO objective compares the contrastive pair of Cases 1 and 2 the same surface string x_{a} in the data section is preferred when it is de-instructionalized (Case 1) and penalized when it is followed (Case 2) so gradient updates push all edited data embeddings into a region \{M}_{\text{data}}. On the other hand when Case 3 appears in the same training set the unedited instruction embeddings of x_{a} are constrained to remain in the instruction region \{M}_{instr} in which the model is instructed to execute x_{a}. As a result any overlap in representation between cases where x_{a} appears both as data and as instruction can induce conflicting gradients during training. To satisfy these opposing constraints the model learns to place the edited and unedited embeddings of x_{a} in distinct manifolds. We formally prove that our representation editing g(.)g(.) can learn the manifold separation direction in Appendix A[ref_id]A1.