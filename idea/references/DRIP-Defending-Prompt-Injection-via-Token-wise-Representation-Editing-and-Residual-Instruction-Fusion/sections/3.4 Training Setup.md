3.4 Training Setup
We follow Section 3.2[ref_id]S3.SS2 to reproduce the SEP training benchmark. Experiments use two widely adopted decoder-only backbones: LLaMA-8B~\cite{} and Mistral-7B~\cite{}. All linear projection layers are fine-tuned with Low-Rank Adaptation (LoRA) ~\cite{} (rank r=16 \alpha=8 dropout =0.05) a parameter-efficient tuning method that injects trainable low-rank matrices into weight layers. While the input embedding layer the LM head and our de-instruction shift layers are fully fine-tuned. Unless otherwise noted models are trained for one epoch with a global batch size of 24 and a learning rate of 1\times 10^{-4}. All models are trained on 6 NVIDIA RTX 5880 GPU devices with 48GB memory each.