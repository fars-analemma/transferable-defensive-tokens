5 Conclusion
We present DRIP a novel defense framework for mitigating prompt injection attacks in large language models. DRIP addresses key challenges in instruction-data disentanglement by reformulating the defense objective as a representation editing problem where an editing function learns to project instruction-like data tokens away from the instruction manifold. We further design a residual instruction fusion module to preserve the semantic integrity of intended instructions against adversarial overwriting. Our contrastive training paradigm built on curated examples with distinct semantics enables DRIP to learn fine-grained embedding manipulations that enhance robustness without compromising utility. Comprehensive evaluations on both heuristic and optimization-based prompt injection benchmarks demonstrate that DRIP consistently outperforms four state-of-the-art defenses reducing attack success rates by up to 66% while maintaining instruction-following utility comparable to undefended models. These results validate the effectiveness of combining semantic-level representation control with architectural separation in securing LLMs against adversarial manipulation. Looking forward we plan to extend DRIP to larger model scales multi-turn interactions and multimodal settings.