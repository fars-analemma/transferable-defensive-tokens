4.1.2 Baselines
We compare against the following training-time defenses:
 •
 Undefended. Base model without any fine-tuning.
 •
 StruQ ~\cite{}. Applies adversarial training by mixing clean and injected prompts optimized using the standard SFT objective. Role-specific delimiter tokens (e.g. [INST] [INPT] [RESP] [MARK] [COLN]) are added to the vocabulary and jointly learned.
 •
 SecAlign ~\cite{}. Extends StruQ by replacing the SFT loss with a preference-based DPO objective encouraging alignment toward injection-resistant outputs.
 •
 ISE ~\cite{}. Introduces an Instruct Segment Embedding (ISE) layer after token embeddings which adds one of four learned offsets corresponding to system instruction user instruction data and response. The ISE weights are initialized from a zero-centered Gaussian \{N}(\{0}0.01^{2}\{I}).
 •
 PFT ~\cite{}. Inserts a fixed positional ID gap (gap=512) between the instruction and data segments to enforce separation in the model’s positional encoding space.