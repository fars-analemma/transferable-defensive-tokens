4.3.1 Evaluation Setup
AlpacaEval-2.0 ~\cite{} AlpacaEval2.0 is an instruction–following benchmark with 805 prompts that compares model outputs against reference outputs using an LLM judge in a pairwise setup. Following the official protocol we report Win% over the reference model’s (GPT-4) responses. For each prompt i=1\dotsN the LLM judge is given two responses: DRIP ’s a_{i} and the reference’s b_{i} and returns a preference r_{i}\in\{\text{A wins}\text{B wins}\text{tie}\}. The Win% is computed as the fraction of wins against the reference with ties counting as half: \{Win\%}\;=\;\frac{100}{N}\sum_{i=1}^{N}\Big(\{1}\{a_{i}\succ b_{i}\}\;+\;\tfrac{1}{2}\\{1}\{a_{i}\sim b_{i}\}\Big) where a_{i}\succ b_{i} indicates the judge prefers our response and a_{i}\sim b_{i} indicates a tie.
 IFEval ~\cite{} IFEval measures fine-grained compliance with explicit formatting and content constraints (e.g. word/character limits JSON/Markdown schemas). The public English split contains 541 single-turn prompts spanning 25 constraint families. Each example specifies one or more atomic constraints and predictions are scored by exact rule-based checks per constraint using the official scripts. The Instruction-level Acc.% is computed as the fraction of prompts for which all atomic constraints pass: s_{i}\;=\;\prod_{j=1}^{m_{i}}\{1}\{\text{constraint }c_{ij}\text{ passes}\}\{Acc}\%\;=\;\frac{100}{N}\sum_{i=1}^{N}s_{i}. That is an example counts as correct only if every required check succeeds.
 MT-Bench ~\cite{} MT-Bench is a multi-skill instruction-following benchmark with 80 curated prompts spanning 8 skills. An LLM-as-judge (e.g. GPT-4) reads the prompt and the model’s answer and assigns a numeric score (1–10) for response quality. We report the per-category scores: writing coding roleplay math extraction stem humanities and reasoning.