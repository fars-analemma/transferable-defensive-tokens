4.6.2 Inference-time Defenses
A complementary line of work modifies prompts or intervenes during inference to mitigate injection attacks. Prompt restructuring methods aim to mark or isolate untrusted spans via template rearrangement ~\cite{} instruction reinforcement ~\cite{} trusted-region encoding (Spotlighting~\cite{}) or multi-encoding schemes ~\cite{}. Learned tokens such as DefensiveTokens~\cite{} can suppress adversarial content while preserving utility. Other defenses perform sanitization or authentication: PromptArmor~\cite{} removes malicious patterns via multi-stage filtering Fath~\cite{} authenticates retrieved content using hashing and Melon~\cite{} provides provable safety in agentic settings. A final category directly manipulates internal model states during inference. KV-cache pruning ~\cite{} eliminates harmful hidden states; ThinkIntervene~\cite{} injects meta-instructions to reinforce system intent; and SecInfer~\cite{} aggregates safe reasoning paths to suppress adversarial completions. While effective in narrow settings these approaches often rely on brittle heuristics or task-specific instrumentation.