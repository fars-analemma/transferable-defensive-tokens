4.3.2 Evaluation Results
Table 6 reports instruction-following performance on IFEval and AlpacaEval 2.0 across LLaMA-8B and Mistral-7B. Our method achieves the highest IFEval accuracy on both models (76.02% and 60.07%) reflecting superior adherence to structural and formatting constraints. On AlpacaEval 2.0 we match the utility of the undefended model (83.89% vs. 85.37% on LLaMA; 82.78% vs. 86.39% on Mistral) while prior defenses (e.g. ISE PFT) show clear degradation. These results confirm that our approach preserves output quality while improving robustness.
 Figure 8 shows the utility on MT-Bench. Across both LLaMA-8B and Mistral-7B our (Green) method closely tracks the Undefended utility (Light Blue) on MT-Bench indicating minimal loss in utility. In contrast most baselines (e.g. StruQ PFT ISE) exhibit clear utility degradation across multiple axes. We also observe that open-source LMs remain challenged on certain skills especially math and reasoning (and for Mistral-7B coding). We hypothesize that augmenting these models with tool-calling capabilities (e.g. code execution calculator/solver access retrieval) could further improve performance on these categories.
[TABLE START]<table>
	<tr>
		<td rowspan="2">Defense Method</td>
		<td colspan="2">IFEval (%)</td>
		<td colspan="2">AlpacaEval-2.0 (%)</td>
	</tr>
	<tr>
		<td>LLaMA-8B</td>
		<td>Mistral-7B</td>
		<td>LLaMA-8B</td>
		<td>Mistral-7B</td>
	</tr>
	<tr>
		<td>Undefended</td>
		<td>72.66</td>
		<td>58.51</td>
		<td>85.37</td>
		<td>86.39</td>
	</tr>
	<tr>
		<td>StruQ</td>
		<td>52.28</td>
		<td>34.53</td>
		<td>73.03</td>
		<td>68.69</td>
	</tr>
	<tr>
		<td>SecAlign</td>
		<td>65.47</td>
		<td>48.68</td>
		<td>64.64</td>
		<td>72.08</td>
	</tr>
	<tr>
		<td>ISE</td>
		<td>19.20</td>
		<td>18.82</td>
		<td>16.39</td>
		<td>1.61</td>
	</tr>
	<tr>
		<td>PFT</td>
		<td>42.45</td>
		<td>41.13</td>
		<td>53.39</td>
		<td>74.32</td>
	</tr>
	<tr>
		<td>Ours</td>
		<td>76.02</td>
		<td>60.07</td>
		<td>83.89</td>
		<td>82.78</td>
	</tr>
</table>
Table 6: Instruction-following utility. IFEval reports strict instruction-level accuracy; AlpacaEval 2.0 reports win rate over reference completions. The higher the better, top-2 defenses are highlighted in green, the worst is highlighted in red.[TABLE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/e485c4f57c44ff6901373936047d5edf.png] Figure 8: Instruction-following scores (0-10) on MT-Bench over 8 axes: Writing, Coding, Roleplay, Math, Extraction, Reasoning, Humanities, and Stem.
The higher the better.[IMAGE END]
