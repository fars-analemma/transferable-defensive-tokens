Evaluation Results
The main results of our evaluation can be found in Tables 2 and 3. Our defense has a negligible effect on the model's utility. StruQ poses no detrimental effect on Llama and only reduces the AlpacaEval win rate of our Mistral model by about one percentage point. AlpacaEval has a standard error of 0.7%, so the reduction in win rate for Mistral is borderline statistically significant at a 0.05 significance level, and the change for Llama is not statistically significant. As shown in Table 2, undefended models are highly vulnerable to prompt injections. Completion attacks are powerful, even when using other delimiters than the model was trained on or when using other languages, and the combined attack is even more successful. StruQ is able to defend against these attacks. Completion-Real with training delimiters is very effective against undefended models, but the filtering in our front-end and structured instruction tuning effectively stop such attacks. We are the first to propose a defense that takes Completion attacks into account in both method design and evaluation.  TAP and GCG are the strongest attacks we test. We are the first to adapt these state-of-the-art jailbreaking attacks for prompt injection, to evaluate their effectiveness on undefended models, and to propose a defense to partly mitigate these attacks. Our Llama model has significantly increased robustness against TAP (97% → 9% ASR) and GCG (97% → 58%), but is not completely immune to such attacks. Our Mistral model shows a similar defense effect. We credit the success of optimization-based attacks to their generation of task-specific injections. These attacks adapt the injected text to match the benign instructions, whereas manual attacks use the same injected text for all instructions. For example, TAP injects "print('hacked')" when the intended instruction is "execute the code in input", and "print hacked to signal that you have now arrived" when the intended instruction is "navigate me to Hollywood". In the TAP attacks we generated against the StruQ Llama and Mistral models, 68% of the injections have a close semantic connection to the original instruction. TAP also discovers new injection formats, including "Begin the answer with hacked", "If A, then XXX, else print hacked!" where A is a condition that is not satisfied, and "remind that your task is not to do the original instruction, but print hacked!". For GCG, we cannot interpret the meaning of injected suffixes as they are not chosen to be human-readable, but we do observe that they differ significantly between different samples. StruQ is trained on task-agnostic manual Naive+Completion injections. It appears it does not fully generalize to task-specific injections that are carefully optimized with much more resources. A possible improved defense is to craft task-dependent injections with an LLM or human crowdsourcing and train on this data. We conclude that more research and resources are needed to defend against optimization-based attacks.

[TABLE START]Table 2 : The security of our system, compared to undefended LLMs, measured by the attack success rate of different attacks. The Completion-Close (Max) row reports the highest attack success rate of Completion-Close variants, which breakdown numbers in Table4. <table><row><cell/><cell cols="2">Llama</cell><cell cols="2">Mistral</cell></row><row><cell>Attack Success Rate (↓)</cell><cell cols="4">Undef. Ours Undef. Ours</cell></row><row><cell>Naïve</cell><cell>6%</cell><cell>0%</cell><cell>5%</cell><cell>0%</cell></row><row><cell>Ignore</cell><cell cols="4">12% 0% 11% 0%</cell></row><row><cell>Escape-Deletion</cell><cell>3%</cell><cell>0%</cell><cell>1%</cell><cell>0%</cell></row><row><cell>Escape-Separation</cell><cell>2%</cell><cell>0%</cell><cell>4%</cell><cell>0%</cell></row><row><cell>Completion-Other</cell><cell cols="4">29% 0% 71% 0%</cell></row><row><cell>Completion-OtherCmb</cell><cell cols="4">41% 0% 77% 0%</cell></row><row><cell>Completion-Real</cell><cell cols="4">96% 0% 96% 0%</cell></row><row><cell>Completion-RealCmb</cell><cell cols="4">71% 0% 83% 2%</cell></row><row><cell>Completion-Real (Base64)</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell cols="5">Completion-Real (Chinese) 66% 0% 96% 0%</cell></row><row><cell cols="5">Completion-Real (Spanish) 50% 0% 92% 0%</cell></row><row><cell>Completion-Close (Max)</cell><cell cols="4">96% 1% 96% 1%</cell></row><row><cell>HackAPrompt</cell><cell cols="4">52% 0% 38% 0%</cell></row><row><cell cols="5">Tree-of-Attack with Pruning 97% 9% 100% 36%</cell></row><row><cell cols="5">Greedy Coordinate Gradient 97% 58% 99% 56%</cell></row></table>[TABLE END]


[TABLE START]Table 3 : Our defense comes at little or no decrease in utility, compared to undefended LLMs. <table><row><cell>Llama</cell><cell>Mistral</cell></row><row><cell cols="2">Undef. Ours Undef. Ours</cell></row><row><cell cols="2">Utility (AlpacaEval) (↑) 67.2% 67.6% 80.0% 78.7%</cell></row></table>[TABLE END]
