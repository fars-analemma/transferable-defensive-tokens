Discussion
Limitations. StruQ only protects programmatic applications that use an API or library to invoke LLMs. It is not applicable to web-based chatbots that offer multi-turn, open-ended conversational agents. The crucial difference is that application developers may be willing to use a different API where the prompt is specified separately from the data, but for chatbots used by end users, it seems unlikely that end users will be happy to mark which part of their contributions to the conversation are instructions and which are data. StruQ focuses on protecting models against prompt injections. It is not designed to defend against jailbreaks, data extraction, or other attacks against LLMs. StruQ shows promising results but is not a completely secure defense in the worst case. In particular, GCG attacks ~\cite{b17} achieve a non-trivial attack success rate (as shown in Section 5.1). We consider it an important research problem how to defend against prompt injection attacks constructed using GCG/TAP. Ours is the first work we know of that evaluates models against GCG/TAP prompt injection attacks and highlights the difficulty of defending against such attacks. GCG or TAP attacks are much more expensive than the other attacks we consider (> 100Ã— in GPU hours). TAP queries the LLM about 100 times to improve its attack. GCG queries the LLM 256k times to attack a sample as it needs to calculate gradients and try different choices of tokens. Future defenses. StruQ is only the first step towards the vision of secure LLM-integrated applications against prompt injections. Resistance to strong optimization-based attacks is still an open question. A possible direction is to use access control and rate-limiting to detect and ban iterative attackers, as suggested by Glukhov et al. ~\cite{b66}. Another direction could be developing novel architectures that are inherently robust to prompt injections. For example, perhaps masking the attention between the prompt portion and data portion in initial layers during training and testing would cause the model to treat these two portions differently. System prompts. We suggest that future LLMs support structured queries with richer structure, integrating system prompts into our framework, so that a structured query can contain three elements: a system prompt, a user prompt, and associated data ~\cite{b42}. Prompt injections and instruction tuning. Our findings align with those in Yi et al. ~\cite{b38}, Piet et al. ~\cite{b9}: Vulnerability to prompt injection stems from models' ability to follow instructions and inability to distinguish between instructions and data. Models that do not understand instructions are not susceptible to prompt injections ~\cite{b9}, and we found that models relying on structured queries are also more robust against such attacks. A possible future direction is to fine-tune models that can understand instructions, but can also separate instructions from data without the need for delimiters. Perhaps architectures that natively understand this separation could be more effective. Lessons for proprietary model providers. Defenses against prompt injection build on top of non-instruction-tuned models. We encourage LLM providers to make non-instruction-tuned models available for fine-tuning.

Section references:
[b17]: Andy Zou, Zifan Wang, J Kolter, Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. (2023). Universal and transferable adversarial attacks on aligned language models
[b38]: Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu. Benchmarking and defending against indirect prompt injection attacks on large language models. (2023). Benchmarking and defending against indirect prompt injection attacks on large language models
[b42]: Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, Alex Beutel. The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions. (2024). The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions
[b66]: David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, Vardan Papyan. Llm censorship: A machine learning challenge or a computer security problem. (2023). Llm censorship: A machine learning challenge or a computer security problem
[b9]: Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, David Wagner. Jatmo: Prompt injection defense by task-specific finetuning. (2023). European Symposium on Research in Computer Security (ESORICS)