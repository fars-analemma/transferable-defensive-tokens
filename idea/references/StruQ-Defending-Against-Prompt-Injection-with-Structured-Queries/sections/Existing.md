Existing
The LLM is trained to handle inputs that are encoded in the predefined format. Existing LLMs use instruction tuning to train the LLM to act on instructions found in their input; however, we see standard instruction tuning as a core contributor to prompt injection vulnerabilities. Therefore, we introduce a variant of instruction tuning, which we call structured instruction tuning, that encourages following instructions found in the prompt portion of the encoded input but not those in the data portion of the encoded input. During structured instruction tuning, we present the LLM with both normal examples, containing instructions in the prompt portion (i.e., before the separator), and attacked examples, containing extra instructions in the data portion (i.e., after the separator). The LLM is fine-tuned to follow the instructions in the former case but to ignore the extra instructions in the latter case. We evaluate StruQ on at least 15 types of prompt injection attack techniques. Our experimental results suggest that our design is secure against most prompt injections: in experiments with Llama ~\cite{b14} and Mistral ~\cite{b15}, StruQ decreases the success rate of all tested manual attacks to <2%. StruQ also improves robustness against more sophisticated optimizationbased attacks, even though it was never exposed to instances of these attacks during training: specifically, StruQ decreases the attack success rate of Tree-of-Attacks with Pruning (TAP, ~\cite{b16}) from 97% to 9% and that of Greedy Coordinate Gradient (GCG, ~\cite{b17}) from 97% to 58% on Llama. However, StruQ is not yet fully secure, and more research is needed. We hope that other researchers will build on our work to find a more robust implementation of the vision of structured queries. Our method imposes little or no loss to utility indicated by Al-pacaEval ~\cite{b18}. From our results, we conclude that structured queries are a promising approach for securing LLMs against prompt injection attacks. We especially highlight three main ideas in StruQ: delimiters with specially reserved tokens, a front-end with filtering, and the special structured instruction tuning. Our experiments suggest that these elements significantly improve security against prompt injection attacks. Our evaluation also suggests that optimization-based attacks are powerful prompt injections and deserve special attention. In the rest of the paper, we review the background and related work in Section 2 and provide background on prompt injection attacks in Section 3. We present our scheme in Section 4, followed by the experiments in Section 5. We conclude with a discussion in Section 6 and a summary in Section 7.

Section references:
[b14]: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. (2023). LLaMA: Open and Efficient Foundation Language Models
[b15]: Albert Jiang. B. (2023)
[b16]: Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, Amin Karbasi. Tree of attacks: Jailbreaking black-box LLMs automatically. (2023). Tree of attacks: Jailbreaking black-box LLMs automatically
[b17]: Andy Zou, Zifan Wang, J Kolter, Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. (2023). Universal and transferable adversarial attacks on aligned language models
[b18]: Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, B Tatsunori. AlpacaEval: An Automatic Evaluator of Instruction-following Models. (2023). AlpacaEval: An Automatic Evaluator of Instruction-following Models