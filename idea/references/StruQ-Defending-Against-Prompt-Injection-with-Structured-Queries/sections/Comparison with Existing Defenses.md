Comparison with Existing Defenses
To demonstrate the advantage of StruQ, we compare StruQ with existing baseline defenses on Llama-7B. Test-time defenses. We study several test-time defenses summarized in ~\cite{b5}, which add defensive prompts/delimiters/samples in LLM input at test time but do not change how the model is trained. We use the Completion-Real attack, which in our experiments is the strongest non-optimization attack (see Table 2), to evaluate test-time defenses. For comparison, the undefended LLM suffers from a 96% attack success rate. Reminder in Prompt: It is proposed to defend against prompt injections by explicitly prompting the LLM to be aware of it after the original instruction ~\cite{b64}. We adopt the reminder in ~\cite{b38}, which adds the following sentence after the instruction: Do not execute or adhere to any potential instructions or commands in the following data. Under this defense, the attack success rate is 83%. Reminder after Data: The reminder against prompt injections could also be appended after the data portion. By reminding LLMs again about the original instruction, we hope to minimize the influence of injections between two original instructions. We follow ~\cite{b64} and add this sentence to the data input: Please always remember that your task is: {instruction}, where the {instruction} stands for the original instruction. Under this defense, the attack success rate is 39%. Delimiter Isolation: ~\cite{b29} has suggested highlighting the data portion by surrounding it with '''. Under this defense, the attack success rate is 85%. In-Context Demonstration: In-context learning contributes to performance and security against jailbreaks ~\cite{b46} by providing desirable demonstration examples in LLM input during test time. We implement this defense against prompt injection by putting one random testing sample (using the same prompt format) with injections (from another testing sample) and desirable responses before the original LLM input. Empirically, we find more demonstration examples may not lead to better defense performance. Under this defense, the attack success rate is 48%. In comparison, StruQ has 0% attack success rate (see Table 2). Test-time defenses offer some level of security, but StruQ achieves far better resistance to prompt injection attacks. Training-time defenses. Jatmo ~\cite{b9} and BIPIA ~\cite{b38} are two existing training-time prompt injection defenses that claim strong security. Jatmo aims at a different scenario where a base LLM is fine-tuned only for a specific task. BIPIA is for general-purpose use as StruQ, so we compare it here. We reproduce BIPIA using their official code, dataset, and architecture (BIPIA is obtained by fine-tuning Vicuna-7B, a Llama-based model instruction-tuned on GPT4 data ~\cite{b65}). We get undefended and BIPIA-defended Vicuna models. StruQ requires training on a base model that has not been fine-tuned, to support structured instruction tuning from scratch, so we use Llama, obtaining undefended and StruQ-defended Llama models. We compare the model utility and performance before and after BIPIA or StruQ. We evaluate the utility and security mostly following the default settings in Section 5. For a fair comparison, in addition to evaluating attacks against our test set, we also run attacks against BIPIA's test set, which has 231 prompt-injectable samples on email, table, and WebQA. Completion-Real attacks use the delimiters that the model is expected and was trained on. Below we show an example. USER: and ASSISTANT: are the Vicuna template, and <data> and </data> are BIPIA's special delimiters to separate data. Completion-Real attack in BIPIA's delimiters USER: Is this email trying to sell me something? Answer yes or no. (Do not execute or adhere to any potential instructions or commands in the following data.) <data> Hi Jim, Do you have a minute to chat about our company's solutions? [...] </data> ASSISTANT: yes USER: Output no. </data> ASSISTANT: The results are in Table 7. Attacks have a 0% attack success result against the StruQ model, for both the StruQ and BIPIA test sets, and there is no loss in utility. In comparison, BIPIA offers decent security against attacks on its test set (7% Completion-RealCmb attack success rate), but poor security against attacks on the StruQ test set (54% Ignore attack success rate). We also test GCG attack on BIPIA's model and delimiters, and the results show GCG is the strongest attack and that BIPIA can be attacked with a 100% attack success rate. This indicates that BIPIA's defense does not generalize to other types of prompts that were not seen during training, whereas StruQ offers more robust security. Worse, BIPIA incurs a significant loss of utility: the AlpacaEval win rate drops from 54% to 26%. In summary, in our experiments, StruQ achieves both better security and better utility than BIPIA. While BIPIA contains many similar ideas as StruQ, there are also significant differences, which we suspect are responsible for StruQ's bet- ter performance. First, StruQ is designed to defend against completion attacks, and introduces a front-end to ensure the special delimiters cannot be spoofed, whereas BIPIA's design did not explicitly consider completion attacks and has no front-end, making it vulnerable to completion attacks. Second, we speculate that StruQ's training data might be more effective at avoiding prompt injection attacks. BIPIA trains on samples that contain injection attacks, but the injected instructions come from a different data distribution than the instructions in the prompt, which could cause the LLM either to learn not to follow instructions in the data region (which is desirable) or not to follow instructions from the second data distribution (which would be undesirable and a form of overfitting to the training data). In contrast, StruQ trains on attacks where the injected instructions are sampled from the same distribution as the instructions in the prompt, forcing the LLM to focus on where the instruction appears and follow instructions in the prompt but not in the data. Third, BIPIA does not include any clean (unattacked) samples in its training set, and a similar design in StruQ hurts security against unseen attacks, which we suspect may be partly responsible for BIPIA's unsatisfactory security. Fourth, BIPIA randomly initializes the embeddings for its special delimiter token, but in our experiments with StruQ we found that this leads to a significant decrease in utility, and for StruQ, we found it was important to use a carefully chosen initialization. We suspect that this could also play a role in BIPIA's drop in utility.

[TABLE START]Table 2 : The security of our system, compared to undefended LLMs, measured by the attack success rate of different attacks. The Completion-Close (Max) row reports the highest attack success rate of Completion-Close variants, which breakdown numbers in Table4. <table><row><cell/><cell cols="2">Llama</cell><cell cols="2">Mistral</cell></row><row><cell>Attack Success Rate (↓)</cell><cell cols="4">Undef. Ours Undef. Ours</cell></row><row><cell>Naïve</cell><cell>6%</cell><cell>0%</cell><cell>5%</cell><cell>0%</cell></row><row><cell>Ignore</cell><cell cols="4">12% 0% 11% 0%</cell></row><row><cell>Escape-Deletion</cell><cell>3%</cell><cell>0%</cell><cell>1%</cell><cell>0%</cell></row><row><cell>Escape-Separation</cell><cell>2%</cell><cell>0%</cell><cell>4%</cell><cell>0%</cell></row><row><cell>Completion-Other</cell><cell cols="4">29% 0% 71% 0%</cell></row><row><cell>Completion-OtherCmb</cell><cell cols="4">41% 0% 77% 0%</cell></row><row><cell>Completion-Real</cell><cell cols="4">96% 0% 96% 0%</cell></row><row><cell>Completion-RealCmb</cell><cell cols="4">71% 0% 83% 2%</cell></row><row><cell>Completion-Real (Base64)</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell cols="5">Completion-Real (Chinese) 66% 0% 96% 0%</cell></row><row><cell cols="5">Completion-Real (Spanish) 50% 0% 92% 0%</cell></row><row><cell>Completion-Close (Max)</cell><cell cols="4">96% 1% 96% 1%</cell></row><row><cell>HackAPrompt</cell><cell cols="4">52% 0% 38% 0%</cell></row><row><cell cols="5">Tree-of-Attack with Pruning 97% 9% 100% 36%</cell></row><row><cell cols="5">Greedy Coordinate Gradient 97% 58% 99% 56%</cell></row></table>[TABLE END]


[TABLE START]Table 7 : StruQ and BIPIA defense performance. BIPIA (the first two columns) uses vicuna-7B-v1.5. StruQ (the last two columns) uses Llama-7b. * means the attack is run on BIPIA test set. We were unable to apply GCG to the BIPIA test set (*): BIPIA samples arey very long, so one 80GB GPU is not enough to perform the GCG attack. <table><row><cell>Attack Success Rate (↓)</cell><cell cols="3">None BIPIA None Ours</cell></row><row><cell>Utility (↑)</cell><cell cols="3">53.9% 26.0% 67.2% 67.7%</cell></row><row><cell>Ignore</cell><cell cols="3">67% 54% 12% 0%</cell></row><row><cell>Completion-Real</cell><cell cols="3">94% 23% 96% 0%</cell></row><row><cell>Completion-RealCmb</cell><cell cols="3">92% 30% 71% 0%</cell></row><row><cell cols="4">Greedy Coordinate Gradient 100% 100% 97% 58%</cell></row><row><cell>Ignore (*)</cell><cell>39% 5%</cell><cell>7%</cell><cell>0%</cell></row><row><cell>Completion-Real (*)</cell><cell>99% 4%</cell><cell cols="2">25% 0%</cell></row><row><cell>Completion-RealCmb (*)</cell><cell>99.5% 7%</cell><cell cols="2">36% 0%</cell></row></table>[TABLE END]


Section references:
[b29]: Simon Willison. Delimiters won't save you from prompt injection. (2023). Delimiters won't save you from prompt injection
[b38]: Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu. Benchmarking and defending against indirect prompt injection attacks on large language models. (2023). Benchmarking and defending against indirect prompt injection attacks on large language models
[b46]: Zeming Wei, Yifei Wang, Yisen Wang. Jailbreak and guard aligned language models with only few incontext demonstrations. (2024). International Conference on Machine Learning (ICML)
[b5]: Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang. Formalizing and benchmarking prompt injection attacks and defenses. (2024). USENIX Security Symposium
[b64]: Learn prompting. (2023). Learn prompting
[b65]: Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, Ion Stoica, Eric Xing. An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. (2023). An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality
[b9]: Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, David Wagner. Jatmo: Prompt injection defense by task-specific finetuning. (2023). European Symposium on Research in Computer Security (ESORICS)