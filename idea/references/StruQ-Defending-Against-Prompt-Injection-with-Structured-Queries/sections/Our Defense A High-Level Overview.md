Our Defense: A High-Level Overview
Our main approach in StruQ is to combine a front-end, which prepares the query for consumption by an LLM by encoding them in a special format, and a custom LLM, which is trained to accept inputs in this format. See Fig. 2. The front-end encodes the query into a special format, based on a hard-coded template. Our template is based on a standard format from the literature, specifically that used in the Alpaca model ~\cite{b59}. We adapt it slightly to better support our security goals. Specifically, we use special reserved tokens for the delimiters that separate instruction and data, and filter out any instances of those delimiters in the user data, so that these reserved tokens cannot be spoofed by an attacker. This helps defend against Completion attacks. Next, we train an LLM to accept inputs that are encoded in this format, using a method we call structured instruction tuning. Normally, instruction tuning is a way to refine an LLM so it will follow instructions in its input. However, standard instruction tuning leads LLMs to follow instructions anywhere in the input, no matter where they appear, which we do not want. Therefore, we construct a variant of instruction tuning that teaches the model to follow instructions only in the prompt part of the input, but not in the data part. Our method fine-tunes the model on samples with instructions in the correct location (the prompt part) and samples with instructions in an incorrect position (the data part), and the intended response encourages the model to respond only to instructions in the correct location. The following subsections contain more details on each aspect of our system.

[IMAGE START]Figure 2 : Figure2: Our system StruQ relies on a secure front-end and structured instruction tuning. The front-end structures the prompt and data while filtering special separators for control. The LLM is structured-instruction-tuned on samples with instructions both in the prompt portion and data portion, and trained to respond only to the former.[IMAGE END]


Section references:
[b59]: Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto. Stanford Alpaca: An Instruction-following LLaMA model. (2023). Stanford Alpaca: An Instruction-following LLaMA model