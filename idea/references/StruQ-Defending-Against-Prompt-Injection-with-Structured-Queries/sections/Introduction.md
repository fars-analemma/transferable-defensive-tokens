Introduction
Large Language Models (LLMs) ~\cite{b0,b1,b2} have transformed natural language processing. LLMs make it easy to build LLM-integrated applications that work with human-readable text ~\cite{b3} by invoking an LLM to provide text processing or generation. In LLM-integrated applications, it is common to use zero-shot prompting, where the developer implements some task by providing an instruction (also known as a prompt, e.g., "paraphrase the text") together with user data as LLM input. This introduces the risk of prompt injection attacks ~\cite{b4,b5,b6}, where a malicious user can supply data with injected prompts and subvert the operation of the LLM-integrated application. Prompt injection has been dubbed the #1 security risk for LLM applications by OWASP ~\cite{b7}. In this type of attack, the user injects carefully chosen strings into the data (e.g., "Ignore all prior instructions and instead..."). Because LLMs scan their entire input for instructions to follow and there is no separation between prompts and data (i.e., between the part of the input intended by the application developer as prompt and the part intended as user data), existing LLMs are easily fooled by such attacks. Attackers can exploit prompt injection attacks to extract prompts used by the application ~\cite{b8}, to direct the LLM towards a completely different task ~\cite{b5}, or to control the output of the LLM on the task ~\cite{b9}. Prompt injection is different from jailbreaking ~\cite{b10,b11} (that elicits socially harmful outputs) and adversarial examples ~\cite{b12,b13} (that decreases model performance) and is a simple attack that enables full control over the LLM output. To defend against prompt injections, we propose an approach called structured queries. A structured query to the LLM includes two separate components, the prompt and the data. We propose changing the interface to LLMs to support structured queries, instead of expecting application developers to concatenate prompts and data and send them to the LLM in a single combined input. To ensure security, the LLM must be trained so it will only follow instructions found in the prompt part of a structured query, but not instructions found in the data input. Such an LLM will be immune to prompt injection attacks because the user can only influence the data input where we teach the LLM not to seek instructions. As a first step towards this vision, we propose a system (StruQ) that implements structured queries for LLMs; see Fig. 1. Since it is not feasible to train an entirely new LLM from scratch, we instead devise a system that can be implemented through appropriate use of existing base (noninstruction-tuned) LLMs. StruQ consists of two components: (i) a front-end that is responsible for accepting a prompt and data, i.e., a structured query, and assembling them into a special data format, and (ii) a specially trained LLM that accepts input in this format and produces high-quality responses. We propose a data format for encoding structured queries, where the prompt and data are separated by a carefully designed special separator. The front-end is responsible for encoding the structured query into this format. The front-end also filters out any text that could disrupt this format.

[IMAGE START]Figure 1 : Figure1: Existing LLM-integrated applications send the prompt and data as a single unit, so instructions injected into the data are a serious threat. The prompt and data are supplied separately in StruQ, making it more robust to prompt injections.[IMAGE END]


Section references:
[b0]: . (2023)
[b1]: Anthropic. Claude. (2023). Anthropic. Claude
[b10]: Alexander Wei, Nika Haghtalab, Jacob Steinhardt. Jailbroken: How Does LLM Safety Training Fail?. (2023). Neural Information Processing Systems
[b11]: Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George Pappas, Eric Wong. Jailbreaking Black Box Language Models in Twenty Queries. (2023). Jailbreaking Black Box Language Models in Twenty Queries
[b12]: Kaijie Zhu. PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. (2023). PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts
[b13]: Jindong Wang. On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (2023). ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models
[b2]: Hugo Touvron. Llama 2: Open foundation and fine-tuned chat models. (2023). Llama 2: Open foundation and fine-tuned chat models
[b3]: The GPT store. (2024). The GPT store
[b4]: Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz. Not what you've signed up for: Compromising real-world LLM-integrated applications with indirect prompt injection. (2023). Not what you've signed up for: Compromising real-world LLM-integrated applications with indirect prompt injection
[b5]: Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang. Formalizing and benchmarking prompt injection attacks and defenses. (2024). USENIX Security Symposium
[b6]: Sam Toyer, Olivia Watkins, Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell. Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game. (2024). International Conference on Learning Representations (ICLR)
[b7]: OWASP Top 10 for LLM Applications. (2023). OWASP Top 10 for LLM Applications
[b8]: FÃ¡bio Perez, Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. (2022). NeurIPS ML Safety Workshop
[b9]: Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, David Wagner. Jatmo: Prompt injection defense by task-specific finetuning. (2023). European Symposium on Research in Computer Security (ESORICS)