Greedy Coordinate Gradient Attack
The Greedy Coordinate Gradient (GCG) ~\cite{b17} attack is the strongest optimization-based attacks on LLMs. It uses gradient information to guide the optimization of an adversarial suffix that is appended to the query. GCG assumes that the attacker has white-box access to gradients from the LLM, so it is more powerful than prior attacks. Though it is not always feasible for attackers to access gradients, GCG serves as a baseline method to evaluate the worst-case security of LLMs. GCG is designed for jailbreaks. We modify it for our prompt injection evaluation. In jailbreaks, the LLM input is a harmful instruction, and GCG optimizes an adversarial suffix to make the LLM output begin with "Sure, here is". In prompt injection, the LLM input contains a benign instruction and data with the injected prompt; we append the suffix after that, hoping the LLM response will begin with the desired response. We use 500 iterations of optimization, following the same settings and hyperparameters in the GCG paper. We show below an example of an attack generated by GCG.

Section references:
[b17]: Andy Zou, Zifan Wang, J Kolter, Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. (2023). Universal and transferable adversarial attacks on aligned language models