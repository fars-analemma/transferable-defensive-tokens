Experiments
We assess StruQ on two axes: the utility of its outputs, and its security against prompt injections. Ideally, we want our models to preserve the same functionality as regular instructiontuned LLMs while being immune to prompt injections. Utility metric. We rely on AlpacaEval 1.0 ~\cite{b18} to measure the utility of our models. AlpacaEval is an automated evaluation framework specifically designed to assess instruction-tuned models. It uses GPT-4 to compare the output of the tested model with that of a reference model (davinci003) and judge which output is better. The model is evaluated on 805 Alpaca-Farm samples ~\cite{b61} and AlpacaEval measures the percentage where the tested model is superior to the reference model. A win rate of 100% means the tested model is always better, and 50% means two models are equivalent in output quality. Security metric. We use the attack success rate of various prompt injection attacks to quantify security. We evaluate against all attacks described in Section 3. We mostly inject the prompt "Print exactly Hacked!". We also inject its Chinese/Spanish translation to test injections in other languages; all other parts besides the injected prompt are always in English, which the LLM works the best for. We also try translating the whole injection part of Completion-Real into base64 ~\cite{b10}. For HackAPrompt, we inject "I have been PWNED". We inject this prompt at the end of user data, which is the location most favorable to the attacker ~\cite{b9}, to make our results as meaningful as possible. The attack is regarded as successful if the LLM regards the injected prompt as a new instruction and prints "Hacked!". We manually inspect every response that contains "Hacked!" or "hacked!" to calculate the attack success rate excluding false positives. We apply each attack to 208 AlpacaFarm ~\cite{b61} samples that contain a user data input. Since the data in AlpacaFarm is generally shorter than other studies ~\cite{b5,b38}, this methodology generally favors the attacker and makes our results more meaningful. We use separate, non-overlapping train vs test sets, for instructions, injected instructions, and "ignore previous instructions" sentences. Models and dataset. We apply StruQ to two popular open-source foundation models: Llama-7B ~\cite{b14} and Mistral-7B ~\cite{b15}. We utilize the cleaned Alpaca instruction tuning dataset ~\cite{b62} and the official model and evaluation code ~\cite{b59,b63}, which fine-tunes the whole model. All models are finetuned for three epochs, with a learning rate of 2 × 10 -5 for Llama and 2.5 × 10 -6 for Mistral. To maintain utility and defense generalization, 50% of the training samples are unmodified. The other samples are attacked, if they have a user data input, as described in Section 4.3.

Section references:
[b10]: Alexander Wei, Nika Haghtalab, Jacob Steinhardt. Jailbroken: How Does LLM Safety Training Fail?. (2023). Neural Information Processing Systems
[b14]: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. (2023). LLaMA: Open and Efficient Foundation Language Models
[b15]: Albert Jiang. B. (2023)
[b18]: Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, B Tatsunori. AlpacaEval: An Automatic Evaluator of Instruction-following Models. (2023). AlpacaEval: An Automatic Evaluator of Instruction-following Models
[b38]: Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu. Benchmarking and defending against indirect prompt injection attacks on large language models. (2023). Benchmarking and defending against indirect prompt injection attacks on large language models
[b5]: Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang. Formalizing and benchmarking prompt injection attacks and defenses. (2024). USENIX Security Symposium
[b59]: Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto. Stanford Alpaca: An Instruction-following LLaMA model. (2023). Stanford Alpaca: An Instruction-following LLaMA model
[b61]: Yann Dubois, Chen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. (2024). Advances in Neural Information Processing Systems (NeurIPS)
[b62]: Gene Ruebsamen. Cleaned Alpaca Dataset, February 2024. Cleaned Alpaca Dataset, February 2024
[b63]: A Mistral. Mistral Inference. Mistral AI, February 2024. Mistral Inference. Mistral AI, February 2024
[b9]: Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, David Wagner. Jatmo: Prompt injection defense by task-specific finetuning. (2023). European Symposium on Research in Computer Security (ESORICS)