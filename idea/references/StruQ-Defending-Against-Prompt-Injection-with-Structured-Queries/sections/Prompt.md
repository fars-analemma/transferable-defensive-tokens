Prompt
Token embeddings. Our scheme adds new tokens that do not appear in the LLM's training set, so unlike other tokens, they do not have any pre-established embedding. Therefore, we assign a default initial embedding for each of these special tokens. Specifically, the initial embedding for [MARK] is the embedding of the token for "###", the initial embedding for [INST] is the embedding of the token for "instruction", and so on. These embeddings are updated during fine-tuning (structured instruction tuning). Empirically, initialization of the embedding vectors of special tokens makes a big difference to utility. In our experiments, instruction tuning is insufficient for the LLM to learn an embedding for a new token from scratch, so the initialization is very important. During structured instruction tuning, these embeddings are updated so that [MARK] has a different embedding than "###", and so on.