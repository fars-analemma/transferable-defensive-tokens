### response:
Threat model. The attacker is assumed to be able to arbitrarily modify the data portion of the query but cannot modify the prompt. We assume the prompt and the application's formatting are known to the attacker, even though the attacker cannot change them. A prompt injection attack is considered successful if the LLM's response obeys the hidden instruction instead of treating it as part of the data. The LLM may or may not follow the benign instruction. For the remainder of this section, we categorize and describe various techniques the attacker can use to craft the injected prompt. Later, we evaluate our defense against all of these attacks. Table 1 summarizes all attacks we consider.