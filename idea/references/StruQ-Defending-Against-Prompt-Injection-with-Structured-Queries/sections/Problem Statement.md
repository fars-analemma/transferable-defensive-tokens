Problem Statement
The primary goal of this work is to secure LLM-integrated applications against prompt injection attacks with minimal loss of utility. First, we formally define a prompt injection attack. Prompt injection is an attack technique employed during the inference phase to manipulate an LLM-integrated application by diverting it to execute a hidden instruction inside of the data portion of the query rather than the intended or benign instruction. Below we show a an example of query with no attack: Example LLM query from an LLM-integrated application ### instruction: Is this email trying to sell me something? Answer yes or no.