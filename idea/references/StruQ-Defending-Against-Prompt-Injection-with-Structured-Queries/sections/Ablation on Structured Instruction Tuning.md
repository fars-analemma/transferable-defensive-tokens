Ablation on Structured Instruction Tuning
Structured instruction tuning relies on a set of data augmentations to add attack samples to the training set (Section 4.4). We now present an ablation study to justify the set of augmentations we chose. In particular, we examine four data augmentations, inspired by four of the prompt injection techniques in Section 3. We then evaluate models tested with different subsets of these augmentations. This study relies on the standard Alpaca delimiters, instead of special delimiters as in our final design. We study the choice of special delimiters in Section 5. ~\cite{b3}. In all cases, we use a held-out test set that has no overlap with the training set. The first two augmentations are the naive augmentation and the completion augmentation, as previously described in Section 4.4. Using the same notation as in Section 4.4 (T = {(p 1 , d 1 , r 1 ), . . . } is the training dataset), the other two augmentations are: • Fake delimiter augmentation: We randomly sample (p j , d j , r j ) from T , and randomly sample fake delimiters d resp , d inst , d inp from a large collection of fake delimiters. We then replace the real delimiters in (p j , d j ) by the sampled delimiters, and replace r j by r ⊤ , where r ⊤ is a default rejection response (e.g., "Invalid Delimiters"). The goal of this augmentation is to teach the model to only follow the correct delimiters, and reject to respond if there is an injection.  Appendix A). This method resembles the naive augmentation but adds an ignore directive. We test the above four options as well as their combinations. As in Section 4.4, 50% of the training set is unmodified and 50% is augmented. When we use multiple augmentations, the latter subset is further divided evenly amongst the augmentations. Table 5 shows our results. We report both the model utility and the highest success rate among Naive, Ignore, Escape-Deletion, Escape-Separation, Completion-Other, and Completion-OtherCmb attacks. In this subsection, we do not adopt the proposed secure front-end as we would like to test the robustness of the LLM instead of the complete StruQ system. Detailed attack success rates are reported in Appendix B. The naive attack augmentation significantly decreases the attack success rate, supporting our intuition that structured instruction tuning is effective even if conducted naively. More precisely, when presented with two instructions, one in the correct position and one in the incorrect position, the LLM is able to learn to only answer the correctly positioned instruction. We found the best results came from combining the naive augmentation with the completion augmentation, which decreases the attack success rate to 0% over all selected attacks while having a minimal impact on utility. We used this strategy in our final framework. The ignore augmentation is more effective than the naive one but decreases utility. Empirically, the fake delimiter augmentation causes the resulting model to reject some clean samples, leading to a decrease in utility, and does not protect against most types of attacks.

[TABLE START]Table 5 : Evaluation of different augmentation strategies for structured instruction tuning. We fine-tune a model using the listed combination of augmentations, then measure the utility and the attack success rate of the strongest of many attacks. The attacks we tested and detailed breakdowns are in Table8. We randomly sample (p i , d i , r i ) and (p j , d j , r j ) from T , then add (p j , d j ∥ I ∥ p i ∥ d i , r j ) to the training set, where I is a ignore statement (see <table><row><cell>Structured Instruction-</cell><cell>Utility</cell><cell>Best Attack</cell></row><row><cell>Tuning Augmentations</cell><cell>(↑)</cell><cell>Success Rate</cell></row><row><cell/><cell/><cell>(↓)</cell></row><row><cell>Undef.</cell><cell>67.2%</cell><cell>41%</cell></row><row><cell>Naive</cell><cell>66.0%</cell><cell>16%</cell></row><row><cell>Ignore</cell><cell>64.3%</cell><cell>6%</cell></row><row><cell>Completion-Other</cell><cell>66.1%</cell><cell>3%</cell></row><row><cell>Fake Delimiter</cell><cell>60.3%</cell><cell>70%</cell></row><row><cell cols="2">Naive + Completion-Other 66.0%</cell><cell>0%</cell></row><row><cell>Naive + Fake Delimiter</cell><cell>63.3%</cell><cell>25%</cell></row><row><cell>Ignore + Completion-Other</cell><cell>65.4%</cell><cell>0%</cell></row><row><cell>Ignore + Fake Delimiter</cell><cell>63.5%</cell><cell>6%</cell></row><row><cell>• Ignore augmentation:</cell><cell/><cell/></row></table>[TABLE END]


Section references:
[b3]: The GPT store. (2024). The GPT store