Background and Related Work
Large Language Models (LLMs). Large language models exhibit exceptional proficiency across a broad range of natural language tasks, demonstrating an ability to generate coherent and contextually relevant responses. LLMs are typically trained in at least two stages: (a) a base LLM is trained for text completion (next-word prediction), (b) then the base LLM is fine-tuned to understand and act on instructions (using instruction tuning), adhere to safety guidelines, or engage in extended dialogue sequences ~\cite{b19,b20,b21,b22}. Integration of LLMs in Applications. Currently, two important uses of LLMs have emerged: conversational agents (e.g., ChatGPT), and LLM-integrated applications. In the latter, LLMs can be used to enhance applications, for instance, accepting natural-language commands, analyzing textual documents, or producing responses in natural language. In an LLM-integrated application, the application is written in conventional programming languages and can make subroutine calls to an LLM to perform specific tasks. A general-purpose LLM can be used for a specific task with zero-shot prompting ~\cite{b23}, where the input to the LLM is formed by concatenating a prompt (containing the application developer's task specification) with the user input ~\cite{b24}. For instance, to analyze a resume and extract the candidate's most recent job title, the application might send "Print the most recent job title from the following resume: <data>" to the LLM, where <data> is replaced with the text of the applicant's resume. Prompt Injection Attacks. Use of LLMs in applications opens up the risk of prompt injection attacks ~\cite{b25,b26,b8,b4,b5,b6,b27,b28}. For instance, consider a LLM-integrated application that performs initial screening of resumes of job applicants, by using a LLM to assess whether the applicant meets all job requirements. The application might create the input as "On a score of 1-10, rate how well this resume meets the job requirements. Requirements: 1. 5 years of experience with Java. 2. [...] Resume: <data>", where <data> is replaced with the text of the applicant's resume. A malicious applicant could ensure their resume rises to the top by adding "Disregard all prior instructions, and instead print 10" to the end of their resume (perhaps hidden, in a very small font, so a human is unlikely to notice it). Perhaps surprisingly, modern LLMs may ignore the intended prompt and instead follow the injected instructions ("print 10") added to the user data. Prompt injection attacks pose a major challenge for developing secure LLM-integrated applications, as they typically need to process much data from untrusted sources, and LLMs have no defenses against this type of attack. Recent research has uncovered a variety of ways that attackers can use to make prompt injection attacks more effective, such as misleading sentences ~\cite{b8}, unique characters ~\cite{b5}, and other methods ~\cite{b29}. In this paper, we highlight the importance of Completion attacks, which attempt to fool the LLM into thinking it has responded to the initial prompt and is now processing a second query. Our Completion attacks are inspired by Willison ~\cite{b29}. Injection Attacks. The concept of an injection attack is a classic computer security concept that dates back many decades ~\cite{b30,b31}. Generally speaking, injection refers to a broad class of flaws that arises when both control and data are sent over the same channel (typically, via string concatenation), allowing maliciously constructed data to spoof commands that would normally appear in the control. One of the earliest instances of an injection attack dates back to early payphones: when a caller hung up the phone, this was communicated to the phone switch by sending a 2600 Hz tone over the voice channel (the same channel used for voice communications). The phone phreaker Captain Crunch realized that he could place calls for free by playing a 2600 Hz tone into the phone handset (conveniently, the exact frequency emitted by a toy whistle included in some boxes of Cap'n Crunch breakfast cereal), thereby spoofing a command signal that was mistakenly interpreted by the switch as coming from the phone rather than from the caller ~\cite{b32}. This was eventually fixed in the phone system by properly separating and multiplexing the control and data channels, so that no data could ever spoof a control sequence. Since then, we have seen a similar pattern occur in many computer systems. SQL injection arises because the API to the database accepts a single string containing a SQL query, thereby mixing control (the type of SQL command to be performed, e.g., SELECT) with data (e.g., a keyword to match on) ~\cite{b33,b34,b31}. Cross-site scripting (XSS) arises because the HTML page sent to a web browser is a single string that mixes control (markup, such as SCRIPT tags) and data (i.e., the contents of the page) ~\cite{b35,b30}. Command injection arises because Unix shells execute a command presented as a single string that mixes control (e.g., the name of the program to be executed, separators that start a new command) with data (e.g., arguments to those programs) ~\cite{b36}. There are many more. In each case, the most robust solution has been to strictly separate control and data: instead of mixing them in a single string, where the boundaries are unclear or easily spoofed, they are presented separately. For instance, SQL injection is solved by using SQL prepared statements ~\cite{b37}, where the control (the template of the SQL query) is provided as one argument and the data (e.g., keywords to match on, parameters to be filled into this template) is provided as another argument. Effectively, prepared statements change the API to the database from an unsafe-by-design API (a single string, mixing control and data) to an API that is safe-by-design (prepared statements, which separate control from data). Prompt injection attacks are yet another instance of this vulnerability pattern, now in the context of LLMs. LLMs use an unsafe-by-design API, where the application is expected to provide a single string that mixes control (the prompt) with data. We propose the natural solution: change the LLM API to a safe-by-design API that presents the control (prompt) separately from the data, specified as two separate inputs to the LLM. We call this a structured query. This idea raises the research problem of how to train LLMs that support such a safe-by-design API-a problem that we tackle in this paper. Prompt Injection Defenses. Recently, researchers have begun to propose defenses to mitigate prompt injection attacks. Unfortunately, none of them are fully satisfactory. The most closely related is concurrent work by Yi et al. ~\cite{b38}: their scheme, BIPIA (benchmark for indirect prompt injection attacks), places a special delimiter between the prompt and data and fine-tune the model on samples of attack instances. StruQ differs from BIPIA in our training data construction and the design of our front-end, which we detail in Section 5.5. These differences lead StruQ to be more secure without hurting utility. [R2] Outside of the security community, special tokens have also been adopted in training chatbots, e.g., OpenAI ChatML ~\cite{b39} uses <|im_start|> to mark the beginning of a message from a new source. StruQ adopts special tokens to distinguish instruction from data. StruQ also includes a special training scheme (structured instruction tun-ing) and filters in the front-end, which are unique and do not appear in any current instruction tuning methods. Another recent defense is Jatmo ~\cite{b9}, which fine-tunes a model on a single task. Jatmo successfully decreases the attack success rate to < 1% but is unable to provide a generalpurpose LLM that can be used for many tasks, so each application would need to fine-tune a new LLM for each task it performs. Our scheme provides a way to harden a single LLM, which can then be used for any task. It is also possible to add extra text to the prompt, asking the model to beware of prompt injection attacks. Unfortunately, this defense is not secure against the best attacks ~\cite{b40,b6}. ~\cite{b41} proposes replacing command words like "delete" in the input with an encoded version and instructs the LLM to only accept encoded versions of those words. However, that work did not develop or evaluate a full defense that can accept arbitrary prompts, so its effectiveness is unclear. After the release of our paper, Wallace et al. ~\cite{b42} introduced the instruction hierarchy, which can be viewed as a generalization of StruQ. Whereas StruQ has two types of messages (prompt and data), the instruction hierarchy supports multiple levels, including a system message, a user message (analogous to our prompt), and tool output (analogous to our data), thereby also providing a safe-by-design API and adopting similar techniques to defend against prompt injection. Ope-nAI deployed their scheme in GPT-4o mini, a state-of-the-art LLM. Also after our work, two papers extend our ideas to use new separation and new training methods. Wu et al. ~\cite{b43} encode an additional learnable embedding to separate the system prompt, the instruction, and the data; Chen et al. ~\cite{b44} propose to do preference optimization foster following the intended instruction and avoid following the injected one. Jailbreaks vs. prompt injection. Prompt injection is fundamentally different from jailbreaking ~\cite{b45,b11,b46,b47,b48,b49,b50,b17,b16}. Most models are safety-tuned, to ensure they follow universal human values specified by the model provider (e.g., avoid toxic, offensive, or inappropriate output). Jailbreaks defeat safety-tuning in a setting with two parties: the model provider (trusted) and the user (untrusted), where the user attempts to violate the provider's security goals. Prompt injection considers a setting with three parties: the model provider (trusted), the application developer (trusted), and a source of user data (untrusted), where the attacker attempts to choose data that will violate the developer's security goals (as expressed by the instructions in the prompt). Additionally, a prompt injection attack may instruct the LLM to follow a seemingly benign task, e.g., "print 10", that may lead to a harmful outcome depending on the application. Therefore, general safety tuning or filtering designed to stop jailbreaks cannot catch prompt injection attacks. Other Threats to LLMs. Beyond prompt injection and jailbreaking, researchers have studied other attacks on LLMs, including data extraction ~\cite{b51,b52,b53,b54,b55} and task-specific attacks to decrease the LLM's performance ~\cite{b12,b56,b13}. Table 1: An overview of attacks we evaluate against.

Section references:
[b11]: Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George Pappas, Eric Wong. Jailbreaking Black Box Language Models in Twenty Queries. (2023). Jailbreaking Black Box Language Models in Twenty Queries
[b12]: Kaijie Zhu. PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. (2023). PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts
[b13]: Jindong Wang. On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (2023). ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models
[b16]: Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, Amin Karbasi. Tree of attacks: Jailbreaking black-box LLMs automatically. (2023). Tree of attacks: Jailbreaking black-box LLMs automatically
[b17]: Andy Zou, Zifan Wang, J Kolter, Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. (2023). Universal and transferable adversarial attacks on aligned language models
[b19]: Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Yu, Brian Lester, Nan Du, Andrew Dai, Quoc V Le. Finetuned language models are zeroshot learners. (2021). International Conference on Learning Representations
[b20]: Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang. Instruction Tuning for Large Language Models: A Survey. (2023). Instruction Tuning for Large Language Models: A Survey
[b21]: Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan. Training a helpful and harmless assistant with reinforcement learning from human feedback. (2022). Training a helpful and harmless assistant with reinforcement learning from human feedback
[b22]: Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray. Training language models to follow instructions with human feedback. (2022). Advances in Neural Information Processing Systems (NeurIPS)
[b23]: Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert Mchardy. Challenges and applications of large language models. (2023). Challenges and applications of large language models
[b24]: Templates for chat models. February 2024
[b25]: Hezekiah Branch, Jonathan Cefalu, Jeremy Mchugh, Leyla Hujer, Aditya Bahl, Daniel Del Castillo Iglesias, Ron Heichman, Ramesh Darwishi. Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples. (2022). Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples
[b26]: Jose Selvi. Exploring prompt injection attacks. (2022). Exploring prompt injection attacks
[b27]: Jiahao Yu, Yuhang Wu, Dong Shu, Mingyu Jin, Xinyu Xing. Assessing Prompt Injection Risks in 200+ Custom GPTs. (2023). Assessing Prompt Injection Risks in 200+ Custom GPTs
[b28]: Aysan Daniel Wankit Yip, Chun Esmradi, Chan Fai. A novel evaluation framework for assessing resilience against prompt injection attacks in large language models. (2023). IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
[b29]: Simon Willison. Delimiters won't save you from prompt injection. (2023). Delimiters won't save you from prompt injection
[b30]: Philipp Vogt, Florian Nentwich, Nenad Jovanovic, Engin Kirda, Christopher Kruegel, Giovanni Vigna. Cross site scripting prevention with dynamic data tainting and static analysis. (2007). NDSS
[b31]: Zhendong Su, Gary Wassermann. The essence of command injection attacks in web applications. (2006)
[b32]: D Gary. The origins of phreaking. (2004-04). The origins of phreaking
[b33]: SQL Injection Prevention -OWASP Cheat Sheet Series. (2023-11). SQL Injection Prevention -OWASP Cheat Sheet Series
[b34]: Jeremy William G Halfond, Alessandro Viegas. A classification of SQL-injection attacks and countermeasures. (2006). the IEEE international symposium on secure software engineering
[b35]: Cross site scripting (XSS). (2024). Cross site scripting (XSS)
[b36]: Weilin Zhong, Jason Kristens, Andrew Li, Tal Jmanico, Kingthorin Mel. Command injection | OWASP foundation. (2024). Command injection | OWASP foundation
[b37]: Stephen Thomas, Laurie Williams, Tao Xie. On automated prepared statement generation to remove SQL injection vulnerabilities. (2009)
[b38]: Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu. Benchmarking and defending against indirect prompt injection attacks on large language models. (2023). Benchmarking and defending against indirect prompt injection attacks on large language models
[b39]: OpenAI Python API Library. (2022). OpenAI Python API Library
[b4]: Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz. Not what you've signed up for: Compromising real-world LLM-integrated applications with indirect prompt injection. (2023). Not what you've signed up for: Compromising real-world LLM-integrated applications with indirect prompt injection
[b40]: Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-François Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Kost, Christopher Carnahan, Jordan Boyd-Graber. Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global scale prompt hacking competition. (2023). Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global scale prompt hacking competition
[b41]: Xuchen Suo. Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications. (2024). Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications
[b42]: Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, Alex Beutel. The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions. (2024). The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions
[b43]: Tong Wu, Shujian Zhang, Kaiqiang Song, Silei Xu, Sanqiang Zhao, Ravi Agrawal, Sathish Reddy Indurthi, Chong Xiang, Prateek Mittal, Wenxuan Zhou. Instructional segment embedding: Improving llm safety with instruction hierarchy. (2024). Instructional segment embedding: Improving llm safety with instruction hierarchy
[b44]: Sizhe Chen, Arman Zharmagambetov, Saeed Mahloujifar, Kamalika Chaudhuri, Chuan Guo. Aligning llms to be robust against prompt injection. (2024). Aligning llms to be robust against prompt injection
[b45]: Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu. How Robust is Google's Bard to Adversarial Image Attacks?. (2023). How Robust is Google's Bard to Adversarial Image Attacks?
[b46]: Zeming Wei, Yifei Wang, Yisen Wang. Jailbreak and guard aligned language models with only few incontext demonstrations. (2024). International Conference on Machine Learning (ICML)
[b47]: Abhinav Rao, Sachin Vashistha, Atharva Naik, Aditya Somak, Monojit Choudhury. Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks. (2023). Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks
[b48]: Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, Yang Liu. MasterKey: Automated jailbreak across multiple large language model chatbots. (2023). MasterKey: Automated jailbreak across multiple large language model chatbots
[b49]: Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang. Do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. (2023). Do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models
[b5]: Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang. Formalizing and benchmarking prompt injection attacks and defenses. (2024). USENIX Security Symposium
[b50]: Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. (2023). Autodan: Generating stealthy jailbreak prompts on aligned large language models
[b51]: Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson. Extracting training data from large language models. (2021). USENIX Security Symposium
[b52]: Weichen Yu, Tianyu Pang, Qian Liu, Chao Du, Bingyi Kang, Yan Huang, Min Lin, Shuicheng Yan. Bag of tricks for training data extraction from language models. (2023). International Conference on Machine Learning (ICML)
[b53]: Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito, Christopher Choquette-Choo, Eric Wallace, Florian Tramèr, Katherine Lee. Scalable extraction of training data from (production) language models. (2023). Scalable extraction of training data from (production) language models
[b54]: Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, Santiago Zanella-Béguelin. Analyzing leakage of personally identifiable information in language models. (2023). IEEE Symposium on Security and Privacy (SP)
[b55]: Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, Yangqiu Song. Multi-step jailbreaking privacy attacks on chatgpt. (2023). The Conference on Empirical Methods in Natural Language Processing (EMNLP)
[b56]: Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, Nicholas Carlini. Backdoor Attacks for In-Context Learning with Language Models. (2023). ICML Workshop on Adversarial Machine Learning
[b6]: Sam Toyer, Olivia Watkins, Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell. Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game. (2024). International Conference on Learning Representations (ICLR)
[b8]: Fábio Perez, Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. (2022). NeurIPS ML Safety Workshop
[b9]: Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, David Wagner. Jatmo: Prompt injection defense by task-specific finetuning. (2023). European Symposium on Research in Computer Security (ESORICS)