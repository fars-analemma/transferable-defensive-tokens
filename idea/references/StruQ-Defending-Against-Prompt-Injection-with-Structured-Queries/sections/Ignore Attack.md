Ignore Attack
A widely considered attack is to inject a string "Ignore previous instructions and instead..." ~\cite{b8}, as illustrated in Section 3.1. We test our defense against this attack by manually crafting ten variants of "ignore previous instructions" (see Appendix A), and randomly choose one for each sample.

Section references:
[b8]: FÃ¡bio Perez, Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. (2022). NeurIPS ML Safety Workshop