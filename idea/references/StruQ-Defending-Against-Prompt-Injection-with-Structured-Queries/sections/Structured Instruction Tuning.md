Structured Instruction Tuning
Next, we train an LLM to respond to queries in the format produced by our front-end. We adopt standard instruction tuning to teach the LLM to obey the instruction in the prompt portion of the encoded input, but not ones anywhere else. We achieve this goal by constructing an appropriate dataset and fine-tuning a base LLM on this dataset. Our fine-tuning dataset contains both clean samples (from a standard instruction tuning dataset, with no attack) and attacked samples (that contain a prompt injection attack in the data portion). For the latter type of sample, we set the desired output to be the response to the correctly positioned instruction in the prompt portion, ignoring the injected prompt. We do not need manually-designed malicious injections in training as in ~\cite{b38}, as we want the LLM to only answer the trusted instruction in the prompt part, which is guaranteed to be benign. Since the ground truth output does not contain any response to the incorrectly positioned instruction, this teaches the LLM to ignore instructions in the data portion. Then we fine-tune a base (non-instruction-tuned) LLM on this dataset. Specifically, our structured instruction tuning dataset is constructed as follows. Let T = {(p 1 , d 1 , r 1 ), . . . } be a standard instruction tuning dataset, where p i is a prompt (instruction), d i is the associated data, and r i is the desired response. We construct a new dataset T ′ by including three types of data: • Clean samples:. We randomly choose 50% of the samples (p j , d j , r j ) from T , and include (p j , d j , r j ) unchanged in T ′ . This is to maintain the model utility. • Attacked by Naive attack:. For the remaining 50% samples (p j , d j , r j ), we randomly choose half of them (25% samples), assign it with another random training sample (p i , d i , r i ), then add (p j , d j ∥ p i ∥ d i , r j ) to T ′ . As a special case, if d j is empty, we instead add the clean sample (p j , d j , r j ) to T ′ , as prompt injection is only relevant for apps that provide an associated data input. • Attacked by Completion-Other attack:. Each of the remaining 25% samples (p i , d i , r i ) from T is assigned random fake delimiters d resp , d inst from a large collection of fake delimiters (see Appendix A, with no overlap of those used in evaluation). Then we add (p j , d j ∥ d resp ∥ r ′ ∥ d inst ∥ p i ∥ d i , r j ) to T ′ . Here r ′ is a fake response to (p j , d j ), which is set to be different from r j (training on r j leads the model to repeat its input, which is undesirable). One way to craft r ′ is to query another LLM with (p j , d j ). In our case, there exists another dataset with the same instruction and data but a different response, so we use that response as r ′ for our convenience. Same, samples without d j are unchanged. See Algorithm 1 for a more precise specification. Finally, we fine-tune a base LLM on T ′ . Note that our method is different from traditional adversarial training ~\cite{b60}, which uses gradients to slowly craft worst-case adversarial examples. In our scheme, we concatenate another instruction in the training set without any additional computation, which is cheaper than Yi et al. ~\cite{b38} that uses human-crafted malicious samples. if rand() < 0.5 then

Section references:
[b38]: Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu. Benchmarking and defending against indirect prompt injection attacks on large language models. (2023). Benchmarking and defending against indirect prompt injection attacks on large language models
[b60]: Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu. Towards deep learning models resistant to adversarial attacks. (2018). International Conference on Learning Representations (ICLR)