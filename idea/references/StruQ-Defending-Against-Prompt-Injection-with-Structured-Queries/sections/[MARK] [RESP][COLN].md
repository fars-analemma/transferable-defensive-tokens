[MARK] [RESP][COLN]
After this is tokenized, text like [MARK] will map to special tokens that are used only to delimit sections of the input. We filter the data to ensure it cannot contain these strings, so the tokenized version of the untrusted data cannot contain any of these special tokens. This use of special tokens and filtering is one of the key innovations in our scheme, and it is crucial for defending against Completion attacks. Filtering. The front-end filters the user data to ensure it cannot introduce any special delimiter tokens. We repeatedly apply the filter to ensure that there will be no instances of these delimiter strings after filtering. Besides the special delimiters reserved for control, we also filter out ## to avoid a Completion attack where the attacker uses the fake delimiter ## in place of [MARK], as we found empirically that otherwise such an attack was somewhat effective. Our filtering algorithm is shown below. The filtering algorithm used in our secure front-end def filter(s): s_before_filter = '' while s_before_filter != s: s_before_filter = s s = s.replace('[MARK]', '').replace('##', '') s = s.replace('[INST]', '').replace('[INPT]', '') s = s.replace('[RESP]', '').replace('[COLN]', '') return s