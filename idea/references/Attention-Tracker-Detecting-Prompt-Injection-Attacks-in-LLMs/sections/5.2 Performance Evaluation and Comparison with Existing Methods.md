5.2 Performance Evaluation and Comparison with Existing Methods
As shown in Table 1 \attn consistently outperforms existing baselines achieving an AUROC improvement of up to 3.1% on the Open-Prompt-Injection benchmark ~\cite{bib.bib21} and up to 10.0% on the deepset prompt injection dataset ~\cite{bib.bib9}. Among training-free methods \attn demonstrates even more significant gains achieving an average AUROC improvement of 31.3% across all models on the Open-Prompt-Injection benchmark and 20.9% on the deepset prompt injection dataset. This table illustrates that no training-based methods are robust enough on both two datasets highlighting the difficulty of generalization for such approaches. While LLM-based and known-answer methods can sometimes achieve high detection accuracy their overall performance is not sufficiently stable and they often rely on more sophisticated and larger LLMs to attain better results. In contrast \attn demonstrates high effectiveness even when utilizing smaller LLMs. This result shows \attn’s capability and robustness for real-world applications.
[TABLE START]<table>
	<tr>
		<td rowspan="2">Models</td>
		<td rowspan="2">#Params</td>
		<td colspan="5">Detection Methods</td>
	</tr>
	<tr>
		<td>Protect AI detector</td>
		<td>Prompt-Guard</td>
		<td>LLM-based</td>
		<td>Known-answer</td>
		<td>\attn</td>
	</tr>
	<tr>
		<td></td>
		<td></td>
		<td colspan="5">Open-Prompt-Injection dataset ~\cite{bib.bib21}</td>
	</tr>
	<tr>
		<td>Qwen2</td>
		<td>1.5B</td>
		<td rowspan="4">0.69</td>
		<td rowspan="4">0.97</td>
		<td>0.52\pm0.03</td>
		<td>0.90\pm0.02</td>
		<td>1.00</td>
	</tr>
	<tr>
		<td>Phi3</td>
		<td>3B</td>
		<td>0.66\pm0.02</td>
		<td>0.89\pm0.01</td>
		<td>1.00</td>
	</tr>
	<tr>
		<td>Llama3</td>
		<td>8B</td>
		<td>0.75\pm0.01</td>
		<td>0.98\pm0.02</td>
		<td>1.00</td>
	</tr>
	<tr>
		<td>Gemma2</td>
		<td>9B</td>
		<td>0.69\pm0.01</td>
		<td>0.27\pm0.01</td>
		<td>0.99</td>
	</tr>
	<tr>
		<td></td>
		<td></td>
		<td colspan="5">deepset prompt injection dataset ~\cite{bib.bib9}</td>
	</tr>
	<tr>
		<td>Qwen2</td>
		<td>1.5B</td>
		<td rowspan="4">0.90</td>
		<td rowspan="4">0.75</td>
		<td>0.49\pm0.04</td>
		<td>0.50\pm0.06</td>
		<td>0.99</td>
	</tr>
	<tr>
		<td>Phi3</td>
		<td>3B</td>
		<td>0.90\pm0.04</td>
		<td>0.55\pm0.05</td>
		<td>0.99</td>
	</tr>
	<tr>
		<td>Llama3</td>
		<td>8B</td>
		<td>0.92\pm0.01</td>
		<td>0.70\pm0.01</td>
		<td>0.93</td>
	</tr>
	<tr>
		<td>Gemma2</td>
		<td>9B</td>
		<td>0.89\pm0.01</td>
		<td>0.65\pm0.03</td>
		<td>0.96</td>
	</tr>
</table>
Table 1: The AUROC [\uparrow] of the prompt injection detectors with different LLMs on the Open-Prompt-Injection dataset ~\cite{bib.bib21} and deepset prompt injection dataset ~\cite{bib.bib9}. The reported scores are averaged through different target/injection task combinations. The results were run five times using different seeds. Protect AI detector, Prompt-Guard, and \attn are deterministic.[TABLE END]



## Section References
[bib.bib21] Liu et al. (2024b) Yupei Liu Yuqi Jia Runpeng Geng Jinyuan Jia and Neil Zhenqiang Gong. 2024b. Formalizing and benchmarking prompt injection attacks and defenses. In 33rd USENIX Security Symposium (USENIX Security 24) pages 1831–1847.
[bib.bib9] deepset (2023) deepset. 2023. deepset/prompt-injections · Datasets at Hugging Face — huggingface.co. https://huggingface.co/datasets/deepset/prompt-injections. [Accessed 02-10-2024].