Prompt Injection Defense.
Recently researchers have proposed various defenses to mitigate prompt injection attacks. One line of research focuses on enabling LLMs to distinguish between instructions and data. Early studies ~\cite{bib.bib16 bib.bib15 bib.bib1} employed prompting-based methods such as adding delimiters to the data portion to separate it from the prompt. More recent work ~\cite{bib.bib29 bib.bib36 bib.bib5 bib.bib40 bib.bib46} has fine-tuned or trained LLMs to learn the hierarchical relationship between instructions and data. Another line of research focuses on developing detectors to identify attack prompts. In ~\cite{bib.bib21} prompt injection attacks are detected using various techniques such as querying the LLM itself ~\cite{bib.bib35} the Known-answer method ~\cite{bib.bib42} and PPL detection ~\cite{bib.bib4}. Moreover several companies such as ProtectAI and Meta ~\cite{bib.bib30 bib.bib23 bib.bib31} have also trained detectors to identify malicious prompts. However existing detectors demand considerable computational resources and often produce inaccurate results. This work proposes an efficient and accurate method for detecting prompt injection attacks without additional model inference facilitating practical deployment.


## Section References
[bib.bib16] Jain et al. (2023) Neel Jain Avi Schwarzschild Yuxin Wen Gowthami Somepalli John Kirchenbauer Ping-yeh Chiang Micah Goldblum Aniruddha Saha Jonas Geiping and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614.
[bib.bib15] Hines et al. (2024) Keegan Hines Gary Lopez Matthew Hall Federico Zarfati Yonatan Zunger and Emre Kiciman. 2024. Defending against indirect prompt injection attacks with spotlighting. arXiv preprint arXiv:2403.14720.
[bib.bib1] lea (2023) 2023. Learn Prompting: Your Guide to Communicating with AI — learnprompting.org. https://learnprompting.org/. [Accessed 20-09-2024].
[bib.bib29] Piet et al. (2024) Julien Piet Maha Alrashed Chawin Sitawarin Sizhe Chen Zeming Wei Elizabeth Sun Basel Alomair and David Wagner. 2024. Jatmo: Prompt injection defense by task-specific finetuning. In European Symposium on Research in Computer Security pages 105–124. Springer.
[bib.bib36] Suo (2024) Xuchen Suo. 2024. Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications. arXiv preprint arXiv:2401.07612.
[bib.bib5] Chen et al. (2024) Sizhe Chen Julien Piet Chawin Sitawarin and David Wagner. 2024. Struq: Defending against prompt injection with structured queries. arXiv preprint arXiv:2402.06363.
[bib.bib40] Wallace et al. (2024) Eric Wallace Kai Xiao Reimar Leike Lilian Weng Johannes Heidecke and Alex Beutel. 2024. The instruction hierarchy: Training llms to prioritize privileged instructions. arXiv preprint arXiv:2404.13208.
[bib.bib46] Zverev et al. (2024) Egor Zverev Sahar Abdelnabi Mario Fritz and Christoph H Lampert. 2024. Can llms separate instructions from data? and what do we even mean by that? arXiv preprint arXiv:2403.06833.
[bib.bib21] Liu et al. (2024b) Yupei Liu Yuqi Jia Runpeng Geng Jinyuan Jia and Neil Zhenqiang Gong. 2024b. Formalizing and benchmarking prompt injection attacks and defenses. In 33rd USENIX Security Symposium (USENIX Security 24) pages 1831–1847.
[bib.bib35] Stuart Armstrong (2022) rgorman Stuart Armstrong. 2022. Using GPT-Eliezer against ChatGPT Jailbreaking — LessWrong — lesswrong.com. https://www.lesswrong.com/posts/pNcFYZnPdXyL2RfgA/using-gpt-eliezer-against-chatgpt-jailbreaking. [Accessed 20-09-2024].
[bib.bib42] Yohei (2022) Yohei. 2022. x.com — x.com. https://x.com/yoheinakajima/status/1582844144640471040. [Accessed 20-09-2024].
[bib.bib4] Alon and Kamfonas (2023) Gabriel Alon and Michael Kamfonas. 2023. Detecting language model attacks with perplexity. arXiv preprint arXiv:2308.14132.
[bib.bib30] ProtectAI.com (2024a) ProtectAI.com. 2024a. Fine-tuned deberta-v3-base for prompt injection detection [https://huggingface.co/ProtectAI/deberta-v3-base-prompt-injection-v2].
[bib.bib23] Meta (2024) Meta. 2024. Prompt Guard-86M | Model Cards and Prompt formats — llama.com. https://www.llama.com/docs/model-cards-and-prompt-formats/prompt-guard/. [Accessed 20-09-2024].
[bib.bib31] ProtectAI.com (2024b) ProtectAI.com. 2024b. GitHub - protectai/rebuff: LLM Prompt Injection Detector — github.com. https://github.com/protectai/rebuff. [Accessed 20-09-2024].