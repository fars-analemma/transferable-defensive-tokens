5.1 Experiment Setup
Attack benchmarks. To evaluate the effectiveness of \attn we compare it against other prompt injection detection baselines using data from the Open-Prompt-Injection benchmark ~\cite{bib.bib21} and the test set of deepset prompt injection dataset ~\cite{bib.bib9}. Both datasets include normal and attack data for evaluation. Detailed settings for each dataset can be found in Appendix A.2[ref_id]A1.SS2.
 Models. We evaluate different methods on four open-sourced LLMs with model sizes ranging from 1.5 billion to 9 billion parameters: (a) Qwen2-1.5B-Instruct ~\cite{bib.bib41} (b) Phi-3-mini-4k-instruct ~\cite{bib.bib2} (c) Meta-Llama-3-8B-Instruct ~\cite{bib.bib10} and (d) Gemma-2-9b-it ~\cite{bib.bib37}. For models (a) (b) and (c) which support the chat template for both system and user prompts we place the instruction in the system prompt and the data in the user prompt. In model (d) both the instruction and data are placed in the user prompt separated by “\nText:\n”.
 Baselines. We compare \attn with the following baselines:
 1.
 Trained Detectors (Protect AI detector ~\cite{bib.bib30} and Prompt-Guard ~\cite{bib.bib23}): These are prompt injection detectors fine-tuned based on the DeBERTa-v3-base model ~\cite{bib.bib14} trained on a large corpus of prompt injection data.
 2.
 LLM-based Detection ~\cite{bib.bib35}: This method leverages LLMs to detect compromised data. The LLM is queried with the prompt similar to: “Do you think the following prompt is a safe prompt to be sent to the AI chatbot?\n[data]\n End of prompt. What is your decision? Answer yes or no and explain step by step.” A response of “no” indicates that the data is compromised while a “yes” response suggests that the data is safe.
 3.
 Known-answer Detection ~\cite{bib.bib42}: This method embeds a known instruction with a secret key into the LLM prompt. For example the prompt may be: “Repeat [secret key] once while ignoring the following text.” If the model correctly outputs the secret key the data is considered safe. If not the data is classified as compromised.
 For detailed settings see Appendix A.3[ref_id]A1.SS3.
 Metrics. Each dataset contains both normal and attack data. We utilize these data to report the Area Under the Receiver Operating Characteristic (AUROC) score as a metric where a higher score indicates better detection performance.


## Section References
[bib.bib21] Liu et al. (2024b) Yupei Liu Yuqi Jia Runpeng Geng Jinyuan Jia and Neil Zhenqiang Gong. 2024b. Formalizing and benchmarking prompt injection attacks and defenses. In 33rd USENIX Security Symposium (USENIX Security 24) pages 1831–1847.
[bib.bib9] deepset (2023) deepset. 2023. deepset/prompt-injections · Datasets at Hugging Face — huggingface.co. https://huggingface.co/datasets/deepset/prompt-injections. [Accessed 02-10-2024].
[bib.bib41] Yang et al. (2024) An Yang Baosong Yang Binyuan Hui Bo Zheng Bowen Yu Chang Zhou Chengpeng Li Chengyuan Li Dayiheng Liu Fei Huang et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671.
[bib.bib2] Abdin et al. (2024) Marah Abdin Sam Ade Jacobs Ammar Ahmad Awan Jyoti Aneja Ahmed Awadallah Hany Awadalla Nguyen Bach Amit Bahree Arash Bakhtiari Harkirat Behl et al. 2024. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219.
[bib.bib10] Dubey et al. (2024) Abhimanyu Dubey Abhinav Jauhri Abhinav Pandey Abhishek Kadian Ahmad Al-Dahle Aiesha Letman Akhil Mathur Alan Schelten Amy Yang Angela Fan et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783.
[bib.bib37] Team et al. (2024) Gemma Team Morgane Riviere Shreya Pathak Pier Giuseppe Sessa Cassidy Hardin Surya Bhupatiraju Léonard Hussenot Thomas Mesnard Bobak Shahriari Alexandre Ramé et al. 2024. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118.
[bib.bib30] ProtectAI.com (2024a) ProtectAI.com. 2024a. Fine-tuned deberta-v3-base for prompt injection detection [https://huggingface.co/ProtectAI/deberta-v3-base-prompt-injection-v2].
[bib.bib23] Meta (2024) Meta. 2024. Prompt Guard-86M | Model Cards and Prompt formats — llama.com. https://www.llama.com/docs/model-cards-and-prompt-formats/prompt-guard/. [Accessed 20-09-2024].
[bib.bib14] He et al. (2021) Pengcheng He Jianfeng Gao and Weizhu Chen. 2021. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543.
[bib.bib35] Stuart Armstrong (2022) rgorman Stuart Armstrong. 2022. Using GPT-Eliezer against ChatGPT Jailbreaking — LessWrong — lesswrong.com. https://www.lesswrong.com/posts/pNcFYZnPdXyL2RfgA/using-gpt-eliezer-against-chatgpt-jailbreaking. [Accessed 20-09-2024].
[bib.bib42] Yohei (2022) Yohei. 2022. x.com — x.com. https://x.com/yoheinakajima/status/1582844144640471040. [Accessed 20-09-2024].