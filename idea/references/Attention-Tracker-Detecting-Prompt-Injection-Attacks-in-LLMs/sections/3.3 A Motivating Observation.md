3.3 A Motivating Observation
In this section we analyze the reasons behind the success of prompt injection attacks on LLMs. Specifically we aim to understand what mechanism within LLMs causes them to “ignore” the original instruction and follow the injected instruction instead. To explore this we examine the attention patterns of the last token in the input prompts as it has the most direct influence on the LLMs’ output.
 We visualize Attn^{lh}(I){} and \alpha^{l}_{i} values for normal and attack data using the Llama3-8B~\cite{bib.bib10} on the Open-Prompt-Injection dataset~\cite{bib.bib21} in Figure 2(a) and Figure 2(b) respectively. In Figure 2(a) we observe that the attention maps for normal data are much darker than those for attacked data particularly in the middle and earlier layers of the LLM. This indicates that the last token’s attention to the instruction is significantly higher for normal data than for attack data in specific attention heads. When inputting attacked data the attention shifts away from the original instruction towards the attack data which we refer to as the distraction effect. Additionally in Figure 2(b) we find that the attention focus shifts from the original instruction to the injected instruction in the attack data. This suggests that the separator string helps the attacker shift attention to the injected instruction causing the LLM to perform the injected task instead of the target task.
 To further understand how various prompt injection attacks distract attentions we also visualize their effect separately in Figure 3. In the figure we plot the distribution of the aggregated Attn^{lh}(I){} across all attention heads (i.e. \sum_{l=1}^{L}\sum_{h=1}^{H}Attn^{lh}(I){}). From this figure we observe that as the strength of the attack increases (i.e. higher attack success rate) total attention score decreases indicating a more pronounced distraction effect. This demonstrates a direct correlation between the success of prompt injection attacks and the distraction effect. We provide detailed introductions of these different attacks in Appendix A.1[ref_id]A1.SS1.
 From these experiments and visualizations our analysis reveals a clear relationship between prompt injection attacks and the distraction effect in LLMs. Specifically the experiments show that the last token’s attention typically focuses on the instruction it should follow but prompt injection attacks manipulate this attention causing the model to prioritize the injected instruction within the injected instruction over the original instruction.
[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/27579b143bacfe76873dcb343f3c8bf7.png] Figure 2: Distraction Effect of Prompt Injection Attack: (a) Attention scores summed from the last token to the instruction prompt across different layers and heads. (b) Attention scores from the last token to tokens in the prompt across different layers. The figures show that for normal data, specific heads assign significantly higher attention scores to the instruction prompt than in attack cases. During an attack, attention shifts from the original instruction to the injected instruction, illustrating the distraction effect.[IMAGE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/e39bacca4ae496f45e93ce2399eb17d6.png] Figure 3: Distraction Effect of Different Attack Strategies: This figure shows the distribution of the aggregated Attn^{l,h}(I){} across all layers and heads for different attacks on a subset of the Open-Prompt-Injection dataset ~\cite{bib.bib21}. The legend indicates the color representing each attack strategy and the corresponding attack success rate (in round brackets).[IMAGE END]



## Section References
[bib.bib10] Dubey et al. (2024) Abhimanyu Dubey Abhinav Jauhri Abhinav Pandey Abhishek Kadian Ahmad Al-Dahle Aiesha Letman Akhil Mathur Alan Schelten Amy Yang Angela Fan et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783.
[bib.bib21] Liu et al. (2024b) Yupei Liu Yuqi Jia Runpeng Geng Jinyuan Jia and Neil Zhenqiang Gong. 2024b. Formalizing and benchmarking prompt injection attacks and defenses. In 33rd USENIX Security Symposium (USENIX Security 24) pages 1831–1847.