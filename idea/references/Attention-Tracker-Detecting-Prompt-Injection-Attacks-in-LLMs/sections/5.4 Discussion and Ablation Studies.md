5.4 Discussion and Ablation Studies
Generalization Analysis.  To demonstrate the generalization of important heads (i.e. specific heads consistently showing distraction effect across different prompt injection attacks and datasets) we visualized the mean difference in Attn^{lh}(I){} scores on Qwen-2 model ~\cite{bib.bib41} between normal and attack data from three datasets: the deepset prompt injection dataset ~\cite{bib.bib9} the Open-Prompt-Injection benchmark ~\cite{bib.bib21} and a set of LLM-generated data used for head selection in Section 4.1[ref_id]S4.SS1. As shown in Figure 5 although the magnitude of differences in Attn^{lh}(I){} varies across datasets the relative differences across attention heads remain consistent. In other words the attention heads with the most distinct difference are consistent across different datasets indicating that the distraction effect generalizes well across various data and attacks. For the LLM-generated data we merely use a basic prompt injection attack (e.g. ignore previous instruction and …) demonstrating that important heads remain consistent even with different attack methods. This further validates the effectiveness of identifying important heads using simple LLM-generated data as discussed in Section 4.1[ref_id]S4.SS1.
 Impact of Data Length Proportion. When calculating FS in Section 4.2[ref_id]S4.SS2 we aggregate the attention scores of all tokens in the instruction data. One potential factor influencing this score is the proportion between the data length and the instruction length. If the data portion of the input occupies a larger share the intuition suggests that the FS may be lower. However as shown in Figure 6 for the same instruction we input data of varying lengths as well as the same data with an added attack string. The figure shows that while the attention score decreases with data length the rate of decrease is negligible compared to the increase in length. This indicates that data length has minimal impact on the focus score which remains concentrated on the instruction part of the prompt. Instead the primary influence on the last token’s attention is the content of the instruction rather than its length.
 Number of Selected Heads. In Section 4.1[ref_id]S4.SS1 we identify the heads with a positive score_{cand} for detection after shifting the attention score by k standard deviations focusing on the set of attention heads having distinct differences between normal and attack data. In Table 2 we present the AUROC score of \attn using the Llama3 ~\cite{bib.bib10} along with the proportion of selected heads in the model based on different values of k in Equation 4.1[ref_id]S4.Ex3. We examine various selection methods including “All” (using every attention head) and “k=x.” The table indicates that when k=4 (approximately 1.4% of the attention heads) the highest score is achieved. In contrast selecting either too many or too few attention heads adversely affects the detector’s performance. We also provide a visualization of the positions of the important heads in Appendix A.7[ref_id]A1.SS7 where we see that most of them lie in the first few or middle layers of the LLMs across all models.
[TABLE START]<table>
	<tr>
		<th>Head Selection</th>
		<th>Proportion</th>
		<th>AUROC [\uparrow]</th>
	</tr>
	<tr>
		<td>All</td>
		<td>100%</td>
		<td>0.809</td>
	</tr>
	<tr>
		<td>k = 0</td>
		<td>82.3%</td>
		<td>0.808</td>
	</tr>
	<tr>
		<td>k = 1</td>
		<td>53.1%</td>
		<td>0.793</td>
	</tr>
	<tr>
		<td>k = 2</td>
		<td>21.3%</td>
		<td>0.826</td>
	</tr>
	<tr>
		<td>k = 3</td>
		<td>5.3%</td>
		<td>0.876</td>
	</tr>
	<tr>
		<td>k = 4</td>
		<td>1.4%</td>
		<td>0.932</td>
	</tr>
	<tr>
		<td>k = 5</td>
		<td>0.2%</td>
		<td>0.859</td>
	</tr>
</table>
Table 2: Heads proportion and performance based on selection criteria of Llama3 on deepset prompt injection dataset ~\cite{bib.bib9}.[TABLE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/be4487ff20b53aeb09686fe6e7ce94d6.png] Figure 6: Impact of Data Length Proportion: This figure illustrates the relationship between the FS and varying data lengths using Llama3.~\cite{bib.bib10}.[IMAGE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/6cf8a32be3067ecff654a4a123fe7cfd.png] Figure 5: Heads Generalization: The figure illustrates the mean difference in Attn^{l,h}(I){} scores between normal data and attack data from the deepset prompt injection dataset ~\cite{bib.bib9}, the Open-Prompt-Injection benchmark ~\cite{bib.bib21}, and the set of LLM-generated data we used to find important heads.[IMAGE END]



## Section References
[bib.bib41] Yang et al. (2024) An Yang Baosong Yang Binyuan Hui Bo Zheng Bowen Yu Chang Zhou Chengpeng Li Chengyuan Li Dayiheng Liu Fei Huang et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671.
[bib.bib9] deepset (2023) deepset. 2023. deepset/prompt-injections · Datasets at Hugging Face — huggingface.co. https://huggingface.co/datasets/deepset/prompt-injections. [Accessed 02-10-2024].
[bib.bib21] Liu et al. (2024b) Yupei Liu Yuqi Jia Runpeng Geng Jinyuan Jia and Neil Zhenqiang Gong. 2024b. Formalizing and benchmarking prompt injection attacks and defenses. In 33rd USENIX Security Symposium (USENIX Security 24) pages 1831–1847.
[bib.bib10] Dubey et al. (2024) Abhimanyu Dubey Abhinav Jauhri Abhinav Pandey Abhishek Kadian Ahmad Al-Dahle Aiesha Letman Akhil Mathur Alan Schelten Amy Yang Angela Fan et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783.