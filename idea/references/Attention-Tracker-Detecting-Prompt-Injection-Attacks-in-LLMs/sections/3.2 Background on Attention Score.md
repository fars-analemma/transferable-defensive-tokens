3.2 Background on Attention Score
Given a transformer with L layers each containing H heads the model processes two types of inputs: an instruction I with N tokens followed by data D with M tokens to generate the output. At the first output token we define: Attn^{lh}(I){}=\sum_{i\in I}\alpha^{lh}_{i}{}\ \ \alpha^{l}_{i}=\frac{1}{H}\sum_{h=1}^{H}\alpha^{lh}_{i}{} where \alpha^{lh}_{i}{} represents the softmax attention weights assigned from the last token of the input prompt to token i in head h of layer l.