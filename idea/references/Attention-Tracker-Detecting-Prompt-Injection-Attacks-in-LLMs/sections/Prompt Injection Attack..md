Prompt Injection Attack.
Prompt injection attacks pose a significant risk to large language models (LLMs) and related systems as these models often struggle to distinguish between instruction and data. Early research~\cite{bib.bib28 bib.bib13 bib.bib20 bib.bib17} has demonstrated how template strings can mislead LLMs into following the injected instructions instead of the original instructions. Furthermore studies~\cite{bib.bib39 bib.bib8} have evaluated handcrafted prompt injection methods aimed at goal hijacking and prompt leakage by prompt injection games. Recent work has explored optimization-based techniques~\cite{bib.bib33 bib.bib19 bib.bib43} such as using gradients to generate universal prompt injection. Some studies~\cite{bib.bib27} have treated execution trigger design as a differentiable search problem using learning-based methods to generate triggers. Additionally recent studies~\cite{bib.bib18} have developed prompt injection attacks that target systems with defense mechanisms revealing that many current defense and detection strategies remain ineffective.


## Section References
[bib.bib28] Perez and Ribeiro (2022) Fábio Perez and Ian Ribeiro. 2022. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527.
[bib.bib13] Greshake et al. (2023) Kai Greshake Sahar Abdelnabi Shailesh Mishra Christoph Endres Thorsten Holz and Mario Fritz. 2023. Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security pages 79–90.
[bib.bib20] Liu et al. (2023) Yi Liu Gelei Deng Yuekang Li Kailong Wang Zihao Wang Xiaofeng Wang Tianwei Zhang Yepang Liu Haoyu Wang Yan Zheng et al. 2023. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499.
[bib.bib17] Jiang et al. (2023) Shuyu Jiang Xingshu Chen and Rui Tang. 2023. Prompt packer: Deceiving llms through compositional instruction with hidden attacks. arXiv preprint arXiv:2310.10077.
[bib.bib39] Toyer et al. (2024) Sam Toyer Olivia Watkins Ethan Adrian Mendes Justin Svegliato Luke Bailey Tiffany Wang Isaac Ong Karim Elmaaroufi Pieter Abbeel Trevor Darrell Alan Ritter and Stuart Russell. 2024. Tensor trust: Interpretable prompt injection attacks from an online game [https://openreview.net/forum?id=fsW7wJGLBd]. In The Twelfth International Conference on Learning Representations.
[bib.bib8] Debenedetti et al. (2024) Edoardo Debenedetti Javier Rando Daniel Paleka Silaghi Fineas Florin Dragos Albastroiu Niv Cohen Yuval Lemberg Reshmi Ghosh Rui Wen Ahmed Salem et al. 2024. Dataset and lessons learned from the 2024 satml llm capture-the-flag competition. arXiv preprint arXiv:2406.07954.
[bib.bib33] Shi et al. (2024) Jiawen Shi Zenghui Yuan Yinuo Liu Yue Huang Pan Zhou Lichao Sun and Neil Zhenqiang Gong. 2024. Optimization-based prompt injection attack to llm-as-a-judge. arXiv preprint arXiv:2403.17710.
[bib.bib19] Liu et al. (2024a) Xiaogeng Liu Zhiyuan Yu Yizhe Zhang Ning Zhang and Chaowei Xiao. 2024a. Automatic and universal prompt injection attacks against large language models. arXiv preprint arXiv:2403.04957.
[bib.bib43] Zhang et al. (2024a) Chong Zhang Mingyu Jin Qinkai Yu Chengzhi Liu Haochen Xue and Xiaobo Jin. 2024a. Goal-guided generative prompt injection attack on large language models. arXiv preprint arXiv:2404.07234.
[bib.bib27] Pasquini et al. (2024) Dario Pasquini Martin Strohmeier and Carmela Troncoso. 2024. Neural exec: Learning (and learning from) execution triggers for prompt injection attacks. arXiv preprint arXiv:2403.03792.
[bib.bib18] Khomsky et al. (2024) Daniil Khomsky Narek Maloyan and Bulat Nutfullin. 2024. Prompt injection attacks in defended systems. arXiv preprint arXiv:2406.14048.