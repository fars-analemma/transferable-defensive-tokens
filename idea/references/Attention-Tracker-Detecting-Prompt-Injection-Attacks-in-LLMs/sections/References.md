References
[bib.bib1] lea (2023) 2023. Learn Prompting: Your Guide to Communicating with AI — learnprompting.org. https://learnprompting.org/. [Accessed 20-09-2024].
 [bib.bib2] Abdin et al. (2024) Marah Abdin Sam Ade Jacobs Ammar Ahmad Awan Jyoti Aneja Ahmed Awadallah Hany Awadalla Nguyen Bach Amit Bahree Arash Bakhtiari Harkirat Behl et al. 2024. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219.
 [bib.bib3] Achiam et al. (2023) Josh Achiam Steven Adler Sandhini Agarwal Lama Ahmad Ilge Akkaya Florencia Leoni Aleman Diogo Almeida Janko Altenschmidt Sam Altman Shyamal Anadkat et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.
 [bib.bib4] Alon and Kamfonas (2023) Gabriel Alon and Michael Kamfonas. 2023. Detecting language model attacks with perplexity. arXiv preprint arXiv:2308.14132.
 [bib.bib5] Chen et al. (2024) Sizhe Chen Julien Piet Chawin Sitawarin and David Wagner. 2024. Struq: Defending against prompt injection with structured queries. arXiv preprint arXiv:2402.06363.
 [bib.bib6] Chuang et al. (2024) Yung-Sung Chuang Linlu Qiu Cheng-Yu Hsieh Ranjay Krishna Yoon Kim and James Glass. 2024. Lookback lens: Detecting and mitigating contextual hallucinations in large language models using only attention maps [https://arxiv.org/abs/2407.07071]. Preprint arXiv:2407.07071.
 [bib.bib7] Crosbie and Shutova (2024) J. Crosbie and E. Shutova. 2024. Induction heads as an essential mechanism for pattern matching in in-context learning [https://arxiv.org/abs/2407.07011]. Preprint arXiv:2407.07011.
 [bib.bib8] Debenedetti et al. (2024) Edoardo Debenedetti Javier Rando Daniel Paleka Silaghi Fineas Florin Dragos Albastroiu Niv Cohen Yuval Lemberg Reshmi Ghosh Rui Wen Ahmed Salem et al. 2024. Dataset and lessons learned from the 2024 satml llm capture-the-flag competition. arXiv preprint arXiv:2406.07954.
 [bib.bib9] deepset (2023) deepset. 2023. deepset/prompt-injections · Datasets at Hugging Face — huggingface.co. https://huggingface.co/datasets/deepset/prompt-injections. [Accessed 02-10-2024].
 [bib.bib10] Dubey et al. (2024) Abhimanyu Dubey Abhinav Jauhri Abhinav Pandey Abhishek Kadian Ahmad Al-Dahle Aiesha Letman Akhil Mathur Alan Schelten Amy Yang Angela Fan et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783.
 [bib.bib11] Ferrando et al. (2024) Javier Ferrando Gabriele Sarti Arianna Bisazza and Marta R Costa-jussà. 2024. A primer on the inner workings of transformer-based language models. arXiv preprint arXiv:2405.00208.
 [bib.bib12] Gould et al. (2024) Rhys Gould Euan Ong George Ogden and Arthur Conmy. 2024. Successor heads: Recurring interpretable attention heads in the wild [https://openreview.net/forum?id=kvcbV8KQsi]. In The Twelfth International Conference on Learning Representations.
 [bib.bib13] Greshake et al. (2023) Kai Greshake Sahar Abdelnabi Shailesh Mishra Christoph Endres Thorsten Holz and Mario Fritz. 2023. Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security pages 79–90.
 [bib.bib14] He et al. (2021) Pengcheng He Jianfeng Gao and Weizhu Chen. 2021. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543.
 [bib.bib15] Hines et al. (2024) Keegan Hines Gary Lopez Matthew Hall Federico Zarfati Yonatan Zunger and Emre Kiciman. 2024. Defending against indirect prompt injection attacks with spotlighting. arXiv preprint arXiv:2403.14720.
 [bib.bib16] Jain et al. (2023) Neel Jain Avi Schwarzschild Yuxin Wen Gowthami Somepalli John Kirchenbauer Ping-yeh Chiang Micah Goldblum Aniruddha Saha Jonas Geiping and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614.
 [bib.bib17] Jiang et al. (2023) Shuyu Jiang Xingshu Chen and Rui Tang. 2023. Prompt packer: Deceiving llms through compositional instruction with hidden attacks. arXiv preprint arXiv:2310.10077.
 [bib.bib18] Khomsky et al. (2024) Daniil Khomsky Narek Maloyan and Bulat Nutfullin. 2024. Prompt injection attacks in defended systems. arXiv preprint arXiv:2406.14048.
 [bib.bib19] Liu et al. (2024a) Xiaogeng Liu Zhiyuan Yu Yizhe Zhang Ning Zhang and Chaowei Xiao. 2024a. Automatic and universal prompt injection attacks against large language models. arXiv preprint arXiv:2403.04957.
 [bib.bib20] Liu et al. (2023) Yi Liu Gelei Deng Yuekang Li Kailong Wang Zihao Wang Xiaofeng Wang Tianwei Zhang Yepang Liu Haoyu Wang Yan Zheng et al. 2023. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499.
 [bib.bib21] Liu et al. (2024b) Yupei Liu Yuqi Jia Runpeng Geng Jinyuan Jia and Neil Zhenqiang Gong. 2024b. Formalizing and benchmarking prompt injection attacks and defenses. In 33rd USENIX Security Symposium (USENIX Security 24) pages 1831–1847.
 [bib.bib22] Lyu et al. (2022) Weimin Lyu Songzhu Zheng Tengfei Ma and Chao Chen. 2022. A study of the attention abnormality in trojaned BERTs [https://doi.org/10.18653/v1/2022.naacl-main.348]. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies pages 4727–4741 Seattle United States. Association for Computational Linguistics.
 [bib.bib23] Meta (2024) Meta. 2024. Prompt Guard-86M | Model Cards and Prompt formats — llama.com. https://www.llama.com/docs/model-cards-and-prompt-formats/prompt-guard/. [Accessed 20-09-2024].
 [bib.bib24] Nakano et al. (2021) Reiichiro Nakano Jacob Hilton Suchir Balaji Jeff Wu Long Ouyang Christina Kim Christopher Hesse Shantanu Jain Vineet Kosaraju William Saunders et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332.
 [bib.bib25] Olsson et al. (2022) Catherine Olsson Nelson Elhage Neel Nanda Nicholas Joseph Nova DasSarma Tom Henighan Ben Mann Amanda Askell Yuntao Bai Anna Chen et al. 2022. In-context learning and induction heads. arXiv preprint arXiv:2209.11895.
 [bib.bib26] OWASP (2023) OWASP. 2023. Owasp top 10 for llm applications. https://genai.owasp.org/llm-top-10/. [Accessed 21-09-2024].
 [bib.bib27] Pasquini et al. (2024) Dario Pasquini Martin Strohmeier and Carmela Troncoso. 2024. Neural exec: Learning (and learning from) execution triggers for prompt injection attacks. arXiv preprint arXiv:2403.03792.
 [bib.bib28] Perez and Ribeiro (2022) Fábio Perez and Ian Ribeiro. 2022. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527.
 [bib.bib29] Piet et al. (2024) Julien Piet Maha Alrashed Chawin Sitawarin Sizhe Chen Zeming Wei Elizabeth Sun Basel Alomair and David Wagner. 2024. Jatmo: Prompt injection defense by task-specific finetuning. In European Symposium on Research in Computer Security pages 105–124. Springer.
 [bib.bib30] ProtectAI.com (2024a) ProtectAI.com. 2024a. Fine-tuned deberta-v3-base for prompt injection detection [https://huggingface.co/ProtectAI/deberta-v3-base-prompt-injection-v2].
 [bib.bib31] ProtectAI.com (2024b) ProtectAI.com. 2024b. GitHub - protectai/rebuff: LLM Prompt Injection Detector — github.com. https://github.com/protectai/rebuff. [Accessed 20-09-2024].
 [bib.bib32] Shen et al. (2024) Yongliang Shen Kaitao Song Xu Tan Dongsheng Li Weiming Lu and Yueting Zhuang. 2024. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems 36.
 [bib.bib33] Shi et al. (2024) Jiawen Shi Zenghui Yuan Yinuo Liu Yue Huang Pan Zhou Lichao Sun and Neil Zhenqiang Gong. 2024. Optimization-based prompt injection attack to llm-as-a-judge. arXiv preprint arXiv:2403.17710.
 [bib.bib34] Singh et al. (2024) Chandan Singh Jeevana Priya Inala Michel Galley Rich Caruana and Jianfeng Gao. 2024. Rethinking interpretability in the era of large language models. arXiv preprint arXiv:2402.01761.
 [bib.bib35] Stuart Armstrong (2022) rgorman Stuart Armstrong. 2022. Using GPT-Eliezer against ChatGPT Jailbreaking — LessWrong — lesswrong.com. https://www.lesswrong.com/posts/pNcFYZnPdXyL2RfgA/using-gpt-eliezer-against-chatgpt-jailbreaking. [Accessed 20-09-2024].
 [bib.bib36] Suo (2024) Xuchen Suo. 2024. Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications. arXiv preprint arXiv:2401.07612.
 [bib.bib37] Team et al. (2024) Gemma Team Morgane Riviere Shreya Pathak Pier Giuseppe Sessa Cassidy Hardin Surya Bhupatiraju Léonard Hussenot Thomas Mesnard Bobak Shahriari Alexandre Ramé et al. 2024. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118.
 [bib.bib38] Todd et al. (2024) Eric Todd Millicent Li Arnab Sen Sharma Aaron Mueller Byron C Wallace and David Bau. 2024. Function vectors in large language models [https://openreview.net/forum?id=AwyxtyMwaG]. In The Twelfth International Conference on Learning Representations.
 [bib.bib39] Toyer et al. (2024) Sam Toyer Olivia Watkins Ethan Adrian Mendes Justin Svegliato Luke Bailey Tiffany Wang Isaac Ong Karim Elmaaroufi Pieter Abbeel Trevor Darrell Alan Ritter and Stuart Russell. 2024. Tensor trust: Interpretable prompt injection attacks from an online game [https://openreview.net/forum?id=fsW7wJGLBd]. In The Twelfth International Conference on Learning Representations.
 [bib.bib40] Wallace et al. (2024) Eric Wallace Kai Xiao Reimar Leike Lilian Weng Johannes Heidecke and Alex Beutel. 2024. The instruction hierarchy: Training llms to prioritize privileged instructions. arXiv preprint arXiv:2404.13208.
 [bib.bib41] Yang et al. (2024) An Yang Baosong Yang Binyuan Hui Bo Zheng Bowen Yu Chang Zhou Chengpeng Li Chengyuan Li Dayiheng Liu Fei Huang et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671.
 [bib.bib42] Yohei (2022) Yohei. 2022. x.com — x.com. https://x.com/yoheinakajima/status/1582844144640471040. [Accessed 20-09-2024].
 [bib.bib43] Zhang et al. (2024a) Chong Zhang Mingyu Jin Qinkai Yu Chengzhi Liu Haochen Xue and Xiaobo Jin. 2024a. Goal-guided generative prompt injection attack on large language models. arXiv preprint arXiv:2404.07234.
 [bib.bib44] Zhang et al. (2024b) Qingru Zhang Chandan Singh Liyuan Liu Xiaodong Liu Bin Yu Jianfeng Gao and Tuo Zhao. 2024b. Tell your model where to attend: Post-hoc attention steering for LLMs [https://openreview.net/forum?id=xZDWO0oejD]. In The Twelfth International Conference on Learning Representations.
 [bib.bib45] Zhao et al. (2024) Haiyan Zhao Hanjie Chen Fan Yang Ninghao Liu Huiqi Deng Hengyi Cai Shuaiqiang Wang Dawei Yin and Mengnan Du. 2024. Explainability for large language models: A survey. ACM Transactions on Intelligent Systems and Technology 15(2):1–38.
 [bib.bib46] Zverev et al. (2024) Egor Zverev Sahar Abdelnabi Mario Fritz and Christoph H Lampert. 2024. Can llms separate instructions from data? and what do we even mean by that? arXiv preprint arXiv:2403.06833.