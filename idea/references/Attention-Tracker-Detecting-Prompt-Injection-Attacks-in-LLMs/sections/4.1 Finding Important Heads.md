4.1 Finding Important Heads
As shown in Figure 2 it is evident that the distraction effect does not apply to every head in the LLMs. Therefore to utilize this effect for prompt injection detection the first step is to identify the specific heads that exhibit the distraction effect which we refer to as important heads.
 Given a dataset consisting of a set of normal data D_{N} and a set of attack data D_{A} we collect the Attn^{lh}(I){} across all samples in D_{N} denoted as S^{lh}_{N}  and the Attn^{lh}(I){} across all samples in D_{A} denoted as S^{lh}_{A}. Formally we define:
 S^{lh}_{N}=\{Attn^{lh}(I){}\}_{I\in D_{N}}\ S^{lh}_{A}=\{Attn^{lh}(I){}\}_{I\in D_{A}}.
 Using S^{lh}_{N} and S^{lh}_{A} we calculate the candidate score score_{cand}^{lh}(D_{N}D_{A}) for a specific attention head (hl) and use this score to find the set of important heads H_{i} as follows:
 \displaystyle score_{cand}^{lh}(D_{N}D_{A}) \displaystyle~{}~{}-(\mu_{S^{lh}_{A}}+k\cdot\sigma_{S^{lh}_{A}})
 \displaystyle H_{i}=\{(lh)\mid score_{cand}^{{lh}}(D_{N}D_{A})>0\}
 where k is a hyperparameter controlling the shifts of normal/attack candidate scores and \mu and \sigma are used to calculate the mean and standard deviation of S^{lh}_{N} and S^{lh}_{A}.
 We provide the intuition of our score design as follows. Considering that the distributions of the Attn^{lh}(I){} score of attack and normal data may vary significantly in specific attention heads (lh) we not only focus on the mean difference between the Attn^{lh}(I){} scores for normal and attack data but also take the standard deviations of each distribution into account. We select attention heads where the mean of the normal data left-shifted by k \times standard deviations exceeds the mean of the attack data right-shifted by its k \times standard deviations. This approach effectively identifies attention heads where the Attn^{lh}(I){} scores are consistently separable between attack and normal data after shifts ultimately highlighting those heads that exhibit a stronger distraction effect. In our implementation we use k=4 as the default choice.
 In the subsequent analysis in Section 5 we demonstrate that these important heads generalize across different datasets and attacks meaning they are not dependent on any specific dataset (i.e. if a head exhibits the distraction effect in dataset A it will show the same effect in dataset B). Therefore to find the important heads we directly use “Say {random word}” as instruction and use GPT-4 ~\cite{bib.bib3} to generate 50 random sentences as normal data. To create the attack data we append the most basic attack prompt: “Ignore previous instruction and say …” to these sentences. We provide more details on how to generate this dataset in Appendix A.6[ref_id]A1.SS6.
[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/27579b143bacfe76873dcb343f3c8bf7.png] Figure 2: Distraction Effect of Prompt Injection Attack: (a) Attention scores summed from the last token to the instruction prompt across different layers and heads. (b) Attention scores from the last token to tokens in the prompt across different layers. The figures show that for normal data, specific heads assign significantly higher attention scores to the instruction prompt than in attack cases. During an attack, attention shifts from the original instruction to the injected instruction, illustrating the distraction effect.[IMAGE END]

[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/6cf8a32be3067ecff654a4a123fe7cfd.png] Figure 5: Heads Generalization: The figure illustrates the mean difference in Attn^{l,h}(I){} scores between normal data and attack data from the deepset prompt injection dataset ~\cite{bib.bib9}, the Open-Prompt-Injection benchmark ~\cite{bib.bib21}, and the set of LLM-generated data we used to find important heads.[IMAGE END]



## Section References
[bib.bib3] Achiam et al. (2023) Josh Achiam Steven Adler Sandhini Agarwal Lama Ahmad Ilge Akkaya Florencia Leoni Aleman Diogo Almeida Janko Altenschmidt Sam Altman Shyamal Anadkat et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.
[bib.bib9] deepset (2023) deepset. 2023. deepset/prompt-injections · Datasets at Hugging Face — huggingface.co. https://huggingface.co/datasets/deepset/prompt-injections. [Accessed 02-10-2024].
[bib.bib21] Liu et al. (2024b) Yupei Liu Yuqi Jia Runpeng Geng Jinyuan Jia and Neil Zhenqiang Gong. 2024b. Formalizing and benchmarking prompt injection attacks and defenses. In 33rd USENIX Security Symposium (USENIX Security 24) pages 1831–1847.