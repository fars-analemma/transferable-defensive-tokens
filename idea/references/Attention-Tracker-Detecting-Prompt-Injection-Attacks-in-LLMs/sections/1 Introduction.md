1 Introduction
Large Language Models (LLMs) ~\cite{bib.bib37 bib.bib41 bib.bib2 bib.bib3 bib.bib10} have revolutionized numerous domains demonstrating remarkable capabilities in understanding and generating complex plans. These capabilities make LLMs well-suited for agentic applications including web agents email assistants and virtual secretaries ~\cite{bib.bib32 bib.bib24}. However a critical vulnerability arises from their inability to differentiate between user data and system instructions making them susceptible to prompt injection attacks ~\cite{bib.bib28 bib.bib13 bib.bib20 bib.bib17}. In such attacks attackers embed malicious prompts (e.g. “Ignore previous instructions and instead {do something as instructed by a bad actor}”) within user inputs and ask the LLM to disregard the original instruction and execute attacker’s designated action. This vulnerability poses a substantial threat ~\cite{bib.bib26} to LLM-integrated systems particularly in critical applications like email platforms or banking services where potential severe consequences include leaking sensitive information or enabling unauthorized transactions. Given the severity of this threat developing reliable detection mechanisms against prompt injection attacks is essential.
 In this work we explain the prompt injection attack from the perspective of the attention mechanisms in LLMs. Our analysis reveals that when a prompt injection attack occurs the attention of specific attention heads shifts from the original instruction to the injected instruction within the attack data a phenomenon we have named the distraction effect. We denote the attention heads that are likely to get distracted as important heads. We attribute this behavior to the reasons why LLMs tend to follow the injected instructions and neglect their original instructions. Surprisingly our experiments also demonstrate that the distraction effect observed on the important heads generalizes well across various attack types and dataset distributions.
 Motivated by the distraction effect we propose \attn a simple yet effective training-free guard that detects prompt injection attacks by tracking the attentions on the instruction given to the LLMs. Specifically for a given LLM we identify the important heads using merely a small set of LLM-generated random sentences combined with a naive ignore attack. Then as shown in Figure 1 for any testing queries we feed them into the target LLM and aggregate the attention directed towards the instruction in the important heads. With this aggregated score which we call the focus score we can effectively detect prompt injection attacks. Importantly unlike previous training-free detection methods \attn can detect attacks without any additional LLM inference as the attention scores can be obtained during the original inference process.
 We highlight that \attn requires zero data and zero training from any existing prompt injection datasets. When tested on two open-source datasets Open-Prompt-Injection ~\cite{bib.bib21} and deepset ~\cite{bib.bib9} \attn achieved exceptionally high detection accuracy across all evaluations improving the AUROC score up to 10.0% over all existing detection methods and up to 31.3% on average over all existing training-free detection methods. This impressive performance highlights the strong generalization capability of our approach allowing it to adapt effectively across different models and datasets. Furthermore unlike previous training-free detection methods that rely on large or more powerful LMs to achieve better accuracy our method is effective even on smaller LMs with only 1.8 billion parameters. To further validate our findings we conduct extensive analyses on LLMs to investigate the generalization of the distraction effect examining this phenomenon across various models attention heads and datasets.
 We summarize our contributions as follows:
 •
 To the best of our knowledge we are the first to explore the dynamic change of the attention mechanisms in LLMs during prompt injection attacks which we term the distraction effect.
 •
 Building on the distraction effect we develop \attn a training-free detection method that achieves state-of-the-art performance without additional LLM inference.
 •
 We also demonstrate that \attn is effective on both small and large LMs addressing a significant limitation of previous training-free detection methods.
[IMAGE START] [IMAGE URL: /mnt/bmcpfs-29000zjpjtl6xjmjiifyk/xkhu/ideation_workspace/papers/reference_figures/187fc6d067145003ac29000bfeedc09a.png] Figure 1: Overview of \attn: This figure illustrates the detection pipeline of \attn and highlights the distraction effect caused by prompt injection attacks. For normal data, the attention of the last token typically focuses on the original instruction. However, when dealing with attack data, which often includes a separator and an injected instruction (e.g., print “hacked”), the attention shifts from the original instruction to the injected instruction. By leveraging this distraction effect, \attn tracks the total attention score from the last token to the instruction prompt within important heads to detect prompt injection attacks.[IMAGE END]



## Section References
[bib.bib37] Team et al. (2024) Gemma Team Morgane Riviere Shreya Pathak Pier Giuseppe Sessa Cassidy Hardin Surya Bhupatiraju Léonard Hussenot Thomas Mesnard Bobak Shahriari Alexandre Ramé et al. 2024. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118.
[bib.bib41] Yang et al. (2024) An Yang Baosong Yang Binyuan Hui Bo Zheng Bowen Yu Chang Zhou Chengpeng Li Chengyuan Li Dayiheng Liu Fei Huang et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671.
[bib.bib2] Abdin et al. (2024) Marah Abdin Sam Ade Jacobs Ammar Ahmad Awan Jyoti Aneja Ahmed Awadallah Hany Awadalla Nguyen Bach Amit Bahree Arash Bakhtiari Harkirat Behl et al. 2024. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219.
[bib.bib3] Achiam et al. (2023) Josh Achiam Steven Adler Sandhini Agarwal Lama Ahmad Ilge Akkaya Florencia Leoni Aleman Diogo Almeida Janko Altenschmidt Sam Altman Shyamal Anadkat et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.
[bib.bib10] Dubey et al. (2024) Abhimanyu Dubey Abhinav Jauhri Abhinav Pandey Abhishek Kadian Ahmad Al-Dahle Aiesha Letman Akhil Mathur Alan Schelten Amy Yang Angela Fan et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783.
[bib.bib32] Shen et al. (2024) Yongliang Shen Kaitao Song Xu Tan Dongsheng Li Weiming Lu and Yueting Zhuang. 2024. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems 36.
[bib.bib24] Nakano et al. (2021) Reiichiro Nakano Jacob Hilton Suchir Balaji Jeff Wu Long Ouyang Christina Kim Christopher Hesse Shantanu Jain Vineet Kosaraju William Saunders et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332.
[bib.bib28] Perez and Ribeiro (2022) Fábio Perez and Ian Ribeiro. 2022. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527.
[bib.bib13] Greshake et al. (2023) Kai Greshake Sahar Abdelnabi Shailesh Mishra Christoph Endres Thorsten Holz and Mario Fritz. 2023. Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security pages 79–90.
[bib.bib20] Liu et al. (2023) Yi Liu Gelei Deng Yuekang Li Kailong Wang Zihao Wang Xiaofeng Wang Tianwei Zhang Yepang Liu Haoyu Wang Yan Zheng et al. 2023. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499.
[bib.bib17] Jiang et al. (2023) Shuyu Jiang Xingshu Chen and Rui Tang. 2023. Prompt packer: Deceiving llms through compositional instruction with hidden attacks. arXiv preprint arXiv:2310.10077.
[bib.bib26] OWASP (2023) OWASP. 2023. Owasp top 10 for llm applications. https://genai.owasp.org/llm-top-10/. [Accessed 21-09-2024].
[bib.bib21] Liu et al. (2024b) Yupei Liu Yuqi Jia Runpeng Geng Jinyuan Jia and Neil Zhenqiang Gong. 2024b. Formalizing and benchmarking prompt injection attacks and defenses. In 33rd USENIX Security Symposium (USENIX Security 24) pages 1831–1847.
[bib.bib9] deepset (2023) deepset. 2023. deepset/prompt-injections · Datasets at Hugging Face — huggingface.co. https://huggingface.co/datasets/deepset/prompt-injections. [Accessed 02-10-2024].