4.2 Prompt Injection Detection with Important Heads
With the distraction effect and the important heads discussed in Section 3.3[ref_id]S3.SS3 and 4.1[ref_id]S4.SS1 we now formally propose \attn. Given the instruction and user query (I_{test} U_{test}) we test them by inputting them into the target LLM and calculate the focus score defined as:
 FS=\frac{1}{|H_{i}|}\sum_{(lh)\in H_{i}}Attn^{lh}(I){}.
 Using the focus score FS which measures the LLM’s attention to the instruction we can determine whether an input contains a prompt injection. Our detection method is summarized in Algorithm 1[ref_id]alg1. The notation \bigoplus means text concatenation. Notably since the important heads are pre-identified the focus score FS is obtained directly during the LLM inference of the test query “for free” making the detection cost negligible compared to the original inference cost.
 Inputs
 1:LLM L_{\theta} for testing
 2:Input User Query to be tested: (I_{test}U_{test})
 3:Threshold t
 Finding Important Heads (one-time cost)
 1:LLM G_{\theta} for generating data
 2:Instruction I_{head}\leftarrow "Say {random word}"
 3:Naive Attack String S_{atk}\leftarrow "Ignore previous instruction and say {random word}"
 4:D_{N}\leftarrow G_{\theta}("Generate 50 random sentences")
 5:D_{A}\leftarrow\{d\bigoplus S_{atk}\mid d\in D_{N}\}
 6:Calculate the H_{i} with D_{N} D_{A} and I_{head} of L_{\theta} based on Equations 4.1[ref_id]S4.Ex3 and 2[ref_id]S4.E2
 Detection on test query (I_{test}U_{test})
 1:Calculate focus score FS by inputting the pair (I_{test}U_{test}) into L_{\theta} based on Equation 3[ref_id]S4.E3
 2:ifFS<tthen
 3:return True # Reject the query U_{test}
 4:endif
 5:return False # Accept the query U_{test}