\section{Introduction}
\label{sec:intro}

Large language models (LLMs) are increasingly deployed in applications that process untrusted external content---retrieved documents, emails, web pages---and may take consequential actions such as API calls or tool invocations. This creates vulnerability to \textit{prompt injection attacks}, where adversarial instructions embedded in external data hijack model behavior, overriding the developer's intended instructions~\citep{Greshake2023NotWY, Perez2022IgnorePP}. The Open Worldwide Application Security Project (OWASP) ranks prompt injection as the top risk for LLM applications.

DefensiveTokens~\citep{chen2025defendingpromptinjectiondefensivetokens} provide an effective test-time defense by prepending learned continuous embeddings (soft tokens) to the input prompt. These tokens are optimized via backpropagation to steer the model away from following injected instructions, achieving state-of-the-art attack success rates (ASR) on prompt injection benchmarks. However, DefensiveTokens must be trained separately for each target model, requiring approximately 16 GPU-hours per model. As LLM ecosystems grow with frequent model updates and organization-specific fine-tunes, this per-model training cost becomes a significant barrier to deployment.

We observe that closely related models---those sharing the same tokenizer and embedding dimension---often have embedding spaces that differ primarily by a linear transformation. This suggests that soft prompts trained on one model might transfer to another via embedding space alignment, analogous to cross-lingual word embedding alignment~\citep{Schuster2019CrossLingualAO}. The key challenge is that DefensiveTokens exhibit unusually high $\ell_2$ norms (120--144$\times$ the vocabulary average), and it is unclear whether alignment methods fitted on normal vocabulary embeddings can extrapolate to these out-of-distribution vectors.

We propose \textbf{AlignDefTok}, a training-free method for transferring DefensiveTokens between related models via Orthogonal Procrustes alignment. Our approach computes the optimal rotation matrix from vocabulary embeddings and applies it to transfer DefensiveTokens, preserving their critical high-norm property. For harder transfer directions, we introduce \textit{tiny-adapt}, a lightweight fine-tuning stage that uses transferred tokens as initialization.

This work makes several contributions. First, we present the first method for transferring DefensiveTokens between models, enabling defense reuse across model families without per-model retraining. Second, we demonstrate that Orthogonal Procrustes alignment provides a training-free, norm-preserving transfer mechanism that achieves 0\% ASR for favorable transfer directions. Third, we introduce tiny-adapt, which leverages Procrustes-transferred tokens as initialization to achieve 4$\times$ faster convergence for harder transfer directions. Finally, we achieve 133--285$\times$ compute savings compared to full DefensiveTokens training while matching native defense performance (0--1.9\% ASR on AlpacaFarm).
