[
  {
    "index": 0,
    "category": "Environment Configuration",
    "title": "Dependencies Installation and Project Structure Initialization",
    "description": "Set up the development environment for the AlignDefTok project, which transfers DefensiveTokens across closely related LLMs via Orthogonal Procrustes alignment. The project requires PyTorch with CUDA support, Hugging Face Transformers for loading Llama-3-8B-Instruct and Llama-3.1-8B-Instruct models, the PEFT library for prompt-tuning integration, SciPy for the Procrustes SVD computation, and the Meta_SecAlign evaluation framework for ASR measurement on AlpacaFarm. No API keys are needed — all evaluation uses deterministic string-match rules, not model-based judges.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 1,
    "category": "Baseline Experiment",
    "title": "Full DefensiveTokens and Prompting Baselines Evaluation on Both Models",
    "description": "Evaluate the per-model-optimized DefensiveTokens defense and prompting baselines (No Defense, Reminder, Sandwich) on both Llama-3-8B-Instruct and Llama-3.1-8B-Instruct using AlpacaFarm. The DefensiveTokens paper (Table 4) reports ASR and GCG-ASR for all methods on both models, so those ASR numbers are citable. However, RefusalRate was not reported and must be measured for all methods to ensure fair comparison — specifically to verify that the defense reduces ASR through genuine instruction-following rather than excessive refusals. Pre-optimized DefensiveToken embeddings (k=5 tokens, each 4096-d) are available from the DefensiveToken repository's `defensivetokens.json` file for both models, so no training is needed.\n\nReference: \"Defending Against Prompt Injection With a Few DefensiveTokens\" | https://github.com/Sizhe-Chen/DefensiveToken",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 2,
    "category": "Main Experiment",
    "title": "Procrustes-Transferred DefensiveTokens: Llama-3.1-8B-Instruct → Llama-3-8B-Instruct",
    "description": "Implement and evaluate the core proposed method: transfer pre-optimized DefensiveTokens from Llama-3.1-8B-Instruct (source) to Llama-3-8B-Instruct (target) using Orthogonal Procrustes alignment. The alignment computes an orthogonal matrix W* that maps the source embedding space to the target embedding space using only vocabulary embeddings (no backpropagation). The transferred tokens T_t = T_s @ W* are then prepended at inference time on the target model identically to native DefensiveTokens. This direction tests whether DefensiveTokens optimized on a newer checkpoint (3.1) can defend an older checkpoint (3) without re-optimization. The key metric is the gap-closed ratio r = (ASR_no - ASR_xfer) / (ASR_no - ASR_full), measuring what fraction of the full defense benefit is recovered by transfer.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 3,
    "category": "Optimization",
    "title": "Optimize: Procrustes-Transferred DefensiveTokens: Llama-3.1-8B-Instruct → Llama-3-8B-Instruct",
    "description": "Optimize the main experiment 'Procrustes-Transferred DefensiveTokens: Llama-3.1-8B-Instruct → Llama-3-8B-Instruct' to improve results.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 4,
    "category": "Main Experiment",
    "title": "Procrustes-Transferred DefensiveTokens: Llama-3-8B-Instruct → Llama-3.1-8B-Instruct",
    "description": "Evaluate the reverse transfer direction: transfer pre-optimized DefensiveTokens from Llama-3-8B-Instruct (source) to Llama-3.1-8B-Instruct (target) using the same Orthogonal Procrustes alignment method. This direction tests whether DefensiveTokens optimized on an older checkpoint (3) can defend a newer checkpoint (3.1). Bidirectional evaluation ensures the method is robust to transfer direction and not relying on a specific directional bias. The alignment module and transfer module implemented in the previous item are reused here — only the source/target assignment changes.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 5,
    "category": "Optimization",
    "title": "Optimize: Procrustes-Transferred DefensiveTokens: Llama-3-8B-Instruct → Llama-3.1-8B-Instruct",
    "description": "Optimize the main experiment 'Procrustes-Transferred DefensiveTokens: Llama-3-8B-Instruct → Llama-3.1-8B-Instruct' to improve results.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 6,
    "category": "Effectiveness Evaluation",
    "title": "Effectiveness Evaluation of Procrustes-Transferred DefensiveTokens",
    "description": "Evaluate whether the proposed Procrustes-based DefensiveToken transfer method is effective, based on the three success criteria defined in the research idea: (1) Transfer closes most of the security gap on AlpacaFarm (gap-closed ratio r ≥ 0.50 for partial success, r ≥ 0.85 for strong success), (2) Utility does not collapse (RefusalRate under transferred tokens remains close to the No Defense and Full DefensiveTokens baselines), (3) Compute amortization is material (transfer requires negligible computation vs. full DefensiveTokens optimization). The evaluation compares the proposed method against No Defense, Reminder, Sandwich, and Full DefensiveTokens baselines on both transfer directions (Llama-3.1→3 and Llama-3→3.1) on the AlpacaFarm benchmark.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 7,
    "category": "Analysis Experiment",
    "title": "Ablation: Direct Copy vs Procrustes Alignment",
    "description": "Ablation study comparing Direct Copy transfer (no alignment — simply use the source model's DefensiveToken embeddings on the target model without any transformation) against Procrustes-aligned transfer. This ablation isolates the contribution of embedding-space alignment: if the two models' embedding spaces differ by a meaningful rotation, Procrustes alignment should outperform direct copy. If direct copy already works well, it would suggest the embedding spaces are nearly identical and alignment is unnecessary. This analysis is run on both transfer directions using the AlpacaFarm benchmark.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 8,
    "category": "Analysis Experiment",
    "title": "Ablation: Norm Rescaling After Procrustes Transfer",
    "description": "Ablation study testing whether rescaling transferred DefensiveToken embeddings to match their original source norms improves transfer quality. Orthogonal Procrustes preserves vector norms by construction (W* is orthogonal, so ||T_t||_2 = ||T_s||_2). However, if the source and target embedding spaces have different overall scales, the absolute norms of transferred tokens relative to the target's vocabulary may differ. This ablation rescales each transferred token's L2 norm to match the original source token's L2 norm (a no-op for pure orthogonal maps, but informative if combined with non-orthogonal variants) and also tests rescaling to match the target model's natively-optimized DefensiveToken norms. This helps understand whether the high-norm property (~100× vocabulary norm) of DefensiveTokens is preserved and functional after transfer.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 9,
    "category": "Analysis Experiment",
    "title": "Ablation: Tiny-Adapt Fine-Tuning of Transferred Tokens",
    "description": "Test whether Procrustes-transferred DefensiveTokens provide a good initialization for efficient per-target fine-tuning. If the main transfer achieves only partial success (gap-closed r < 0.85), a small number of gradient steps updating only the transferred token embeddings (not model weights) may close the remaining gap at much lower cost than full from-scratch optimization. This ablation runs ≤200 gradient steps using the DefensiveTokens training objective (StruQ loss on Cleaned Alpaca) and measures how quickly ASR improves. The key comparison is: (a) tiny-adapt from Procrustes-transferred initialization vs. (b) the full DefensiveTokens optimization (1 epoch ≈ thousands of steps), and (c) tiny-adapt from random initialization as an additional control.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 10,
    "category": "Analysis Experiment",
    "title": "Embedding Geometry Diagnostics",
    "description": "Analyze the geometric properties of the source, transferred, and natively-optimized DefensiveToken embeddings in relation to the vocabulary embedding space. This analysis tests the core assumption that defense-relevant directions are approximately preserved across model variants and helps understand why transfer succeeds or fails. Key diagnostics include: norm statistics (verifying transferred tokens remain high-norm outliers ~100× vocabulary norm), cosine similarity between source and target DefensiveTokens before and after alignment, and visualization of token positions relative to vocabulary embeddings using PCA/t-SNE.",
    "status": "pending",
    "summary": ""
  }
]