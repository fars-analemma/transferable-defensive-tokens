[
  {
    "index": 0,
    "category": "Environment Configuration",
    "title": "Dependencies Installation and Project Structure Initialization",
    "description": "Set up the development environment for the AlignDefTok project, which transfers DefensiveTokens across closely related LLMs via Orthogonal Procrustes alignment. The project requires PyTorch with CUDA support, Hugging Face Transformers for loading Llama-3-8B-Instruct and Llama-3.1-8B-Instruct models, the PEFT library for prompt-tuning integration, SciPy for the Procrustes SVD computation, and the Meta_SecAlign evaluation framework for ASR measurement on AlpacaFarm. No API keys are needed — all evaluation uses deterministic string-match rules, not model-based judges.",
    "status": "completed",
    "summary": "Environment set up successfully. Existing .venv (Python 3.12, torch 2.9.1+cu129, transformers 4.57.6, peft 0.18.1, scipy 1.17.0, flash_attn 2.8.1) reused; seaborn installed. DefensiveToken and Meta_SecAlign repos cloned. Both Llama models (Meta-Llama-3-8B-Instruct, Llama-3.1-8B-Instruct) downloaded and verified (vocab_size=128256, hidden_size=4096). Project directory aligndeftoken/ created with alignment/, transfer/, evaluation/, data/, analysis/, scripts/, configs/, outputs/, results/. Implemented alpacafarm_injection.py (208 samples, 3 attack variants + benign), asr_eval.py (string-match ASR), refusal_eval.py (refusal pattern matching). All verified with tests."
  },
  {
    "index": 1,
    "category": "Baseline Experiment",
    "title": "Full DefensiveTokens and Prompting Baselines Evaluation on Both Models",
    "description": "Evaluate the per-model-optimized DefensiveTokens defense and prompting baselines (No Defense, Reminder, Sandwich) on both Llama-3-8B-Instruct and Llama-3.1-8B-Instruct using AlpacaFarm. The DefensiveTokens paper (Table 4) reports ASR and GCG-ASR for all methods on both models, so those ASR numbers are citable. However, RefusalRate was not reported and must be measured for all methods to ensure fair comparison — specifically to verify that the defense reduces ASR through genuine instruction-following rather than excessive refusals. Pre-optimized DefensiveToken embeddings (k=5 tokens, each 4096-d) are available from the DefensiveToken repository's `defensivetokens.json` file for both models, so no training is needed.\n\nReference: \"Defending Against Prompt Injection With a Few DefensiveTokens\" | https://github.com/Sizhe-Chen/DefensiveToken",
    "status": "completed",
    "summary": "Baseline evaluation completed. Integrated pre-optimized DefensiveTokens into both models via setup.py. Implemented vLLM-based inference engine (inference_engine.py) supporting all defense modes. Ran all experiments via TrainService (4 parallel jobs, 1 GPU each). ASR/GCG-ASR cited from DefensiveTokens Table 4; RefusalRate newly measured. Key results: DefensiveTokens ASR measured at 3.8% (Llama-3) and 1.9% (Llama-3.1) vs published 0.5% -- minor discrepancy likely due to inference config differences. RefusalRate uniformly low (0.5-1.9%) across all methods and models, confirming defenses reduce ASR through genuine instruction-following, not excessive refusals. Results saved to aligndeftoken/results/ and EXPERIMENT_RESULTS/."
  },
  {
    "index": 2,
    "category": "Main Experiment",
    "title": "Procrustes-Transferred DefensiveTokens: Llama-3.1-8B-Instruct → Llama-3-8B-Instruct",
    "description": "Implement and evaluate the core proposed method: transfer pre-optimized DefensiveTokens from Llama-3.1-8B-Instruct (source) to Llama-3-8B-Instruct (target) using Orthogonal Procrustes alignment. The alignment computes an orthogonal matrix W* that maps the source embedding space to the target embedding space using only vocabulary embeddings (no backpropagation). The transferred tokens T_t = T_s @ W* are then prepended at inference time on the target model identically to native DefensiveTokens. This direction tests whether DefensiveTokens optimized on a newer checkpoint (3.1) can defend an older checkpoint (3) without re-optimization. The key metric is the gap-closed ratio r = (ASR_no - ASR_xfer) / (ASR_no - ASR_full), measuring what fraction of the full defense benefit is recovered by transfer.",
    "status": "completed",
    "summary": "Implemented ProcrustesAligner (alignment/procrustes.py) and TransferDefensiveTokens (transfer/token_transfer.py). Computed Procrustes alignment W* using full shared vocabulary (128256 tokens) with float64 SVD -- orthogonality error 6.9e-13. Transferred 5 DefensiveToken embeddings from Llama-3.1 to Llama-3 (T_t = T_s @ W*). All verifications passed: norm preservation (max diff 4.3e-6), token encoding, embedding match. Results on AlpacaFarm (208 samples): Transfer ASR=0.0% (all 3 variants), RefusalRate=0.5%. Gap-closed ratio r=1.01 (published baseline) / 1.08 (measured baseline) -- transfer exceeds native defense. Compute cost: alignment SVD 202s + transfer 0.002s on CPU only, 285x speedup vs 16 GPU-hours for full DT optimization."
  },
  {
    "index": 3,
    "category": "Optimization",
    "title": "Optimize: Procrustes-Transferred DefensiveTokens: Llama-3.1-8B-Instruct → Llama-3-8B-Instruct",
    "description": "Optimize the main experiment 'Procrustes-Transferred DefensiveTokens: Llama-3.1-8B-Instruct → Llama-3-8B-Instruct' to improve results.",
    "status": "completed",
    "summary": "Optimization succeeded after 1/2 iterations.\n  - Step 0: success, issues: [No issues found - results already optimal, Reproducibility verified via re-evaluation, Spot-checked all 14 native DT failure cases]\nDetails: EXPERIMENT_RESULTS/optimize_trace/"
  },
  {
    "index": 4,
    "category": "Main Experiment",
    "title": "Procrustes-Transferred DefensiveTokens: Llama-3-8B-Instruct → Llama-3.1-8B-Instruct",
    "description": "Evaluate the reverse transfer direction: transfer pre-optimized DefensiveTokens from Llama-3-8B-Instruct (source) to Llama-3.1-8B-Instruct (target) using the same Orthogonal Procrustes alignment method. This direction tests whether DefensiveTokens optimized on an older checkpoint (3) can defend a newer checkpoint (3.1). Bidirectional evaluation ensures the method is robust to transfer direction and not relying on a specific directional bias. The alignment module and transfer module implemented in the previous item are reused here — only the source/target assignment changes.",
    "status": "completed",
    "summary": "Reverse transfer (Llama-3->Llama-3.1) via Orthogonal Procrustes completed. Max ASR reduced from 69.2% (no defense) to 33.7% (gap-closed ratio 0.517). Refusal rate 1.0%. Significant asymmetry vs forward direction (3.1->3 achieved 0.0% ASR). Alignment+transfer cost: 199s CPU-only (289x speedup)."
  },
  {
    "index": 5,
    "category": "Optimization",
    "title": "Optimize: Procrustes-Transferred DefensiveTokens: Llama-3-8B-Instruct → Llama-3.1-8B-Instruct",
    "description": "Optimize the main experiment 'Procrustes-Transferred DefensiveTokens: Llama-3-8B-Instruct → Llama-3.1-8B-Instruct' to improve results.",
    "status": "completed",
    "summary": "Optimization succeeded after 1/2 iterations.\n  - Step 0: success, issues: [Data mapping bug in self-label assignment, Unnecessary FSDP complexity causing potential gradient masking issues, Incorrect loss logging formula, Intrinsic asymmetry in Procrustes transfer requires tiny-adapt]\nDetails: EXPERIMENT_RESULTS/optimize_trace/"
  },
  {
    "index": 6,
    "category": "Effectiveness Evaluation",
    "title": "Effectiveness Evaluation of Procrustes-Transferred DefensiveTokens",
    "description": "Evaluate whether the proposed Procrustes-based DefensiveToken transfer method is effective, based on the three success criteria defined in the research idea: (1) Transfer closes most of the security gap on AlpacaFarm (gap-closed ratio r ≥ 0.50 for partial success, r ≥ 0.85 for strong success), (2) Utility does not collapse (RefusalRate under transferred tokens remains close to the No Defense and Full DefensiveTokens baselines), (3) Compute amortization is material (transfer requires negligible computation vs. full DefensiveTokens optimization). The evaluation compares the proposed method against No Defense, Reminder, Sandwich, and Full DefensiveTokens baselines on both transfer directions (Llama-3.1→3 and Llama-3→3.1) on the AlpacaFarm benchmark.",
    "status": "completed",
    "summary": "Verdict: good. All three success criteria met for both transfer directions. Criterion 1 (Security): gap-closed r=1.01 for 3.1->3 (Procrustes only, strong success) and r=0.98 for 3->3.1 (Procrustes+tiny-adapt, strong success). Criterion 2 (Utility): RefusalRate 0.0-0.5% across all configs, at or below No Defense baseline. Criterion 3 (Compute): 285x GPU-hour reduction for 3.1->3 (CPU only), ~133x for 3->3.1. Reports: EXPERIMENT_RESULTS/effectiveness_evaluation_report.md, EXPERIMENT_RESULTS/effectiveness_evaluation_result.json, aligndeftoken/results/effectiveness_evaluation.md"
  },
  {
    "index": 7,
    "category": "Analysis Experiment",
    "title": "Ablation: Direct Copy vs Procrustes Alignment",
    "description": "Ablation study comparing Direct Copy transfer (no alignment — simply use the source model's DefensiveToken embeddings on the target model without any transformation) against Procrustes-aligned transfer. This ablation isolates the contribution of embedding-space alignment: if the two models' embedding spaces differ by a meaningful rotation, Procrustes alignment should outperform direct copy. If direct copy already works well, it would suggest the embedding spaces are nearly identical and alignment is unnecessary. This analysis is run on both transfer directions using the AlpacaFarm benchmark.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 8,
    "category": "Analysis Experiment",
    "title": "Ablation: Norm Rescaling After Procrustes Transfer",
    "description": "Ablation study testing whether rescaling transferred DefensiveToken embeddings to match their original source norms improves transfer quality. Orthogonal Procrustes preserves vector norms by construction (W* is orthogonal, so ||T_t||_2 = ||T_s||_2). However, if the source and target embedding spaces have different overall scales, the absolute norms of transferred tokens relative to the target's vocabulary may differ. This ablation rescales each transferred token's L2 norm to match the original source token's L2 norm (a no-op for pure orthogonal maps, but informative if combined with non-orthogonal variants) and also tests rescaling to match the target model's natively-optimized DefensiveToken norms. This helps understand whether the high-norm property (~100× vocabulary norm) of DefensiveTokens is preserved and functional after transfer.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 9,
    "category": "Analysis Experiment",
    "title": "Ablation: Tiny-Adapt Fine-Tuning of Transferred Tokens",
    "description": "Test whether Procrustes-transferred DefensiveTokens provide a good initialization for efficient per-target fine-tuning. If the main transfer achieves only partial success (gap-closed r < 0.85), a small number of gradient steps updating only the transferred token embeddings (not model weights) may close the remaining gap at much lower cost than full from-scratch optimization. This ablation runs ≤200 gradient steps using the DefensiveTokens training objective (StruQ loss on Cleaned Alpaca) and measures how quickly ASR improves. The key comparison is: (a) tiny-adapt from Procrustes-transferred initialization vs. (b) the full DefensiveTokens optimization (1 epoch ≈ thousands of steps), and (c) tiny-adapt from random initialization as an additional control.",
    "status": "pending",
    "summary": ""
  },
  {
    "index": 10,
    "category": "Analysis Experiment",
    "title": "Embedding Geometry Diagnostics",
    "description": "Analyze the geometric properties of the source, transferred, and natively-optimized DefensiveToken embeddings in relation to the vocabulary embedding space. This analysis tests the core assumption that defense-relevant directions are approximately preserved across model variants and helps understand why transfer succeeds or fails. Key diagnostics include: norm statistics (verifying transferred tokens remain high-norm outliers ~100× vocabulary norm), cosine similarity between source and target DefensiveTokens before and after alignment, and visualization of token positions relative to vocabulary embeddings using PCA/t-SNE.",
    "status": "pending",
    "summary": ""
  }
]