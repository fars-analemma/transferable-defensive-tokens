{
  "optimize_index": 0,
  "issues": [
    {
      "title": "Data mapping bug in self-label assignment",
      "description": "The self-label indices (raw Alpaca dataset order 0..51759) were incorrectly matched to StruQ training samples by sequential index, but StruQ shuffles the data. This meant self-labels were applied to wrong instruction/input pairs. Fixed by adding orig_index tracking in build_struq_dataset() and using orig_index for lookup in DefensiveTokenTrainingDataset."
    },
    {
      "title": "Unnecessary FSDP complexity causing potential gradient masking issues",
      "description": "The original training script used multi-GPU FSDP with manual gradient masking on the embedding layer. With FSDP sharding, the gradient mask indices may not correspond correctly to the sharded parameter. Simplified to single GPU with gradient checkpointing (8B BF16 model fits in 96GB). This eliminates FSDP-related gradient masking bugs."
    },
    {
      "title": "Incorrect loss logging formula",
      "description": "Loss was logged as running_loss / 10 * gradient_accumulation_steps (operator precedence gives (loss/10)*accum instead of loss/(10*accum)). Fixed to use correct denominator. This was a cosmetic issue that did not affect training."
    },
    {
      "title": "Intrinsic asymmetry in Procrustes transfer requires tiny-adapt",
      "description": "The core issue was not a bug but an intrinsic limitation: Procrustes-transferred tokens from Llama-3 achieve 33.7% ASR on Llama-3.1 despite identical cosine similarity to native tokens (~0.08-0.11). Applied tiny-adapt: fine-tune only the 5 DT embeddings (20,480 params) with StruQ loss for 200 steps. This reduced max ASR from 33.7% to 1.9% (gap-closed ratio 0.979), matching measured full DT performance."
    }
  ],
  "experiment_result_path": "EXPERIMENT_RESULTS/optimize_trace/iteration_0/",
  "success": true,
  "summary": "Applied tiny-adapt optimization to the reverse transfer (Llama-3 -> Llama-3.1). Fixed a data mapping bug (self-label index mismatch with shuffled StruQ data), removed unnecessary FSDP complexity, and fixed a loss logging formula. The tiny-adapt method fine-tunes only the 5 transferred DefensiveToken embeddings (20,480 params) using StruQ loss for 200 steps on 1 GPU (~7 min). Results: max ASR reduced from 33.7% to 1.9% (gap-closed ratio improved from 0.517 to 0.979), with 0.0% refusal rate. The optimized model matches measured full DT performance exactly."
}
